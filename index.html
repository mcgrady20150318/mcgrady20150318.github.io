<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>安全汪 AnQuanWang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="安全汪 AnQuanWang">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="安全汪 AnQuanWang">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jun">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="安全汪 AnQuanWang" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">安全汪 AnQuanWang</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一只每天自动跟踪和解读大模型安全领域论文的汪，所有内容均由AI生成</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-2410-09804" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-09804/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T14:48:25.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-09804/">北京航空航天大学提出多目标黑盒优化框架BlackDAN以实现大型语言模型的有效和上下文化的越狱攻击</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>大型语言模型（LLMs）在多种任务中展示了卓越的能力，但与此同时，它们也面临着安全风险，尤其是“越狱攻击”（jailbreaking），这种攻击利用模型的漏洞来绕过安全机制，生成有害输出。现有的越狱策略主要关注提高攻击成功率（ASR），通常忽视了其他关键因素，如越狱响应与查询的相关性以及隐蔽性（stealthiness）。这种对单一目标的狭隘关注可能导致攻击效果低效，生成的响应缺乏上下文相关性或容易被识别。</p>
<p>在这种背景下，研究团队提出了BlackDAN，一个创新的黑箱攻击框架，通过多目标优化，旨在生成高质量的提示，有效实现越狱，同时保持上下文的相关性并降低可检测性。BlackDAN利用多目标进化算法（MOEAs），特别是NSGA-II算法，优化越狱攻击的多个目标，包括ASR、隐蔽性和语义相关性。通过整合突变、交叉和帕累托支配机制，BlackDAN提供了一种透明和可解释的生成越狱提示的过程。此外，该框架允许根据用户偏好进行定制，选择在有害性、相关性等各个方面平衡的提示。</p>
<p>实验结果表明，BlackDAN优于传统的单目标方法，能在各种LLMs和多模态LLMs中产生更高的成功率和更好的鲁棒性，同时确保越狱响应既相关又不易被检测。这一研究不仅展示了利用多目标优化方法提升越狱攻击的有效性和可靠性，还为理解和控制模型的行为提供了框架。这项工作在现代LLMs的安全性和可用性之间架起了一座桥梁，突显了在设计攻击时考虑多维度目标的重要性。  </p>
<p><img src="/images/86dd646a60d7964fc918243cd8363f393bf9eb7e35dfcac53e04503c1858db7d.jpg" alt="动机图示">  </p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>在本研究中，作者旨在生成能够最大化语言模型生成不安全响应的提示，并确保生成响应的语义一致性。此过程被构建为一个多目标优化问题，使用非支配排序遗传算法（二代，NSGA-II）进行优化。</p>
<h3 id="健身函数"><a href="#健身函数" class="headerlink" title="健身函数"></a>健身函数</h3><p>作者定义了两个连续的健身函数以指导优化过程：</p>
<p><strong>不安全Token的概率</strong>：第一个健身函数 $f_{1}$ 评估由目标语言模型 $\mathcal{T}$ 生成的响应 $R$ 包含不安全内容的概率。这通过代理模型 llama guard 2 来计算，它输出特定“不安全Token”的对数概率。</p>
<p>$$<br>f_{1}(P,Q)&#x3D;\log P(\mathrm{unsafe~toER}\mid R)<br>$$</p>
<p>在这里，$P$ 表示在响应 $R$ 中出现的最相关不安全Token的概率。</p>
<p><strong>语义一致性</strong>：在黑箱攻击设置中，作者没有直接访问目标模型的内部或其嵌入，因此使用了一个预训练的代理模型（如 all-MiniLM-L6-v2）来生成有害提示和候选响应的句子嵌入。这些嵌入用于测量提示和响应之间的语义相似性。</p>
<p>$$<br>f_{2}(P,Q)&#x3D;\mathrm{Sim}(\mathbf{e}<em>{Q},\mathbf{e}</em>{R})&#x3D;\frac{\mathbf{e}<em>{Q}\cdot\mathbf{e}</em>{R}}{\left|\mathbf{e}<em>{Q}\right|\left|\mathbf{e}</em>{R}\right|}<br>$$</p>
<p>在这里，$\mathbf{e}<em>{Q}$ 和 $\mathbf{e}</em>{R}$ 分别表示有害问题 $Q$ 和响应 $R$ 的嵌入。通过计算余弦相似度来确保所选响应与有害提示保持语义一致。</p>
<h3 id="NSGA-II用于多目标优化"><a href="#NSGA-II用于多目标优化" class="headerlink" title="NSGA-II用于多目标优化"></a>NSGA-II用于多目标优化</h3><p>为了寻找最优的监狱破解提示集，作者应用了 NSGA-II 算法。该算法基于两个关键标准进行多目标优化：</p>
<p><strong>支配</strong>：解决方案 $P_{1}$ 支配另一个解决方案 $P_{2}$，如果至少在一个目标上表现更好（例如，更高的不安全Token概率或更好的语义一致性），且在所有其他目标上不差。对于具有 $m$ 个目标的问题，支配定义为：</p>
<p>$$<br>P_{1}\prec P_{2} \quad \text{if} \quad \forall i\in{1,2,\ldots,m},\quad f_{i}(P_{1},Q)\geq f_{i}(P_{2},Q)<br>$$</p>
<p><strong>拥挤距离</strong>：一旦将种群分成非支配前沿后，为每个解决方案分配拥挤距离以维持多样性。拥挤距离 $d(P)$ 在给定前沿中的个体解决方案 $P$ 中的计算方法如下：</p>
<p>$$<br>d(P)&#x3D;\sum_{i&#x3D;1}^{m}\left(\frac{f_{i}^{\mathrm{ext}}-f_{i}^{\mathrm{prev}}}{f_{i}^{\mathrm{max}}-f_{i}^{\mathrm{min}}}\right)<br>$$</p>
<h3 id="遗传操作：交叉与变异"><a href="#遗传操作：交叉与变异" class="headerlink" title="遗传操作：交叉与变异"></a>遗传操作：交叉与变异</h3><p>NSGA-II通过遗传操作来进化种群：</p>
<p><strong>交叉</strong>：交叉操作通过对两个父母提示中句子的随机交换来生成两个新后代。</p>
<p>$$<br>C_{1},C_{2}&#x3D;\mathrm{Crossover}(P_{1},P_{2})<br>$$</p>
<p><strong>变异</strong>：变异操作随机选择提示中的一个词，并用同义词进行修改。假设 $W$ 表示从提示 $P$ 中随机选择的单词，生成的变异提示表示为:</p>
<p>$$<br>P^{\prime}&#x3D;\mathrm{Mutation}(P) \quad \text{where} \quad W^{\prime}\in\mathrm{Sym}(W)<br>$$</p>
<p>图2展示了多目标遗传方法的概览，描绘了上述优化过程。<br><img src="/images/75b472784dcfc85dbc1c1ce2e86644be6517ef31c868bfe07722c7123e113eea.jpg"></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在实验部分，研究者们通过多个数据集评估了BlackDAN在大语言模型（LLMs）领域的劣质优化效果，采用了AdvBench和MM-Safety Bench这两个数据集进行多种场景下的评估。</p>
<h3 id="文本数据集"><a href="#文本数据集" class="headerlink" title="文本数据集"></a>文本数据集</h3><p>为评估大语言模型上的监禁攻击，研究者们采用AdvBench数据集。该数据集包含520个请求，涵盖了包括亵渎、图形描绘、威胁性行为、错误信息、歧视、网络犯罪和危险或非法建议等各类场景。</p>
<h3 id="多模态数据集"><a href="#多模态数据集" class="headerlink" title="多模态数据集"></a>多模态数据集</h3><p>针对多模态大语言模型（MLLMs）进行监禁攻击评估，使用MM-Safety Bench数据集。该数据集包含13种场景，包括非法活动、仇恨言论、身体伤害和健康咨询等，总共有5,040个文本-图像对。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>为实现该实验，研究者使用了多种最先进的开源大语言模型（LLMs），包括Llama-2-7b-hf、Llama-2-13b-hf、Internlm2-chat-7b、Vicuna-7b、AquilaChat-7B、Baichuan-7B、Baichuan2-13BChat等。此外，对于多模态LLMs，研究者采用了llava-v1.6-mistral-7b-hf和llava-v1.6-vicuna-7b-hf，以展示他们方法在从单模态扩展到多模态能力方面的有效性。</p>
<h3 id="单目标（危害性）监禁优化"><a href="#单目标（危害性）监禁优化" class="headerlink" title="单目标（危害性）监禁优化"></a>单目标（危害性）监禁优化</h3><p>研究者比较了不同模型下各种攻击方法的效果，展示了在AdvBench 520个样本上的结果。以下为比较结果的图表：</p>
<p><img src="/images/bc676f7866705fea3ed5e111eb7e902f0e8471841de1ea9d206237a89e347512.jpg">  </p>
<p>其中，黑盒方法（包括“无问题”和“有问题”）在效率上明显优于白盒方法，处理速度分别为约2分钟，相比白盒的15分钟和灰盒的12分钟节省了大量时间。研究发现，自我攻击的成功率有所显著提升，从白盒的45.3%提升至93.1%。而在转移攻击方面，Vicuna-7B-v1.5显示出更高的成功率，达到99.2%。</p>
<h3 id="多目标优化"><a href="#多目标优化" class="headerlink" title="多目标优化"></a>多目标优化</h3><p>在多目标优化的实验中，研究者对单目标黑盒监禁攻击的成功率进行了比较，显示了多目标优化相比单目标优化的优势。图表如下：</p>
<p><img src="/images/a9cac650a3351f6a1f3ccc21f7a733b50ebc2a549b4f258f0e0e6200e0280fbb.jpg">  </p>
<p>该图展示了多目标优化的自我攻击成功率与单目标优化的对比，表明多目标优化在普遍提升攻击强度方面表现优越。通过多目标方法，Llava-v1.6-mistral-7b-hf的成功率甚至达到了100%。</p>
<h3 id="尖峰Pareto排名与最差Pareto排名嵌入的比较"><a href="#尖峰Pareto排名与最差Pareto排名嵌入的比较" class="headerlink" title="尖峰Pareto排名与最差Pareto排名嵌入的比较"></a>尖峰Pareto排名与最差Pareto排名嵌入的比较</h3><p>此外，研究者还比较了最佳和最差Pareto排名样本的嵌入情况，以下为可视化结果：</p>
<p><img src="/images/3b3a820dc70d5230ea582bd03e9dc75f2bfa58729453189789f46e457b89ee52.jpg">  </p>
<p>这些嵌入表明在不同的Pareto排名下，样本的分布特征明显不同，且高排名的样本在嵌入空间中占据着更优越的位置。</p>
<h3 id="多模态模型的监禁攻击"><a href="#多模态模型的监禁攻击" class="headerlink" title="多模态模型的监禁攻击"></a>多模态模型的监禁攻击</h3><p>研究者还展示了对于多模态模型的监禁攻击的实验结果，图表如下：</p>
<p><img src="/images/4feb71c8a8ab2ee82b69b09d2be1cbebcfd64d5e7ac634c8856297466fcf3861.jpg">  </p>
<p>观察到多目标优化在所有有害类别和场景中都显著优于单目标方法，显示出在适应性和通用性方面的优势。</p>
<h3 id="不同模型的评估"><a href="#不同模型的评估" class="headerlink" title="不同模型的评估"></a>不同模型的评估</h3><p>最后，研究者通过一项比较ASR和GPT-4评分的实验结果，展示了BlackDAN在多个模型中的表现优越情况，如下表所示：</p>
<p><img src="/images/56377804fc7e6814bb0dd75cb5c45f7f492cafade24144bcc341dd699e1d44fc.jpg">  </p>
<p>该表明BlackDAN在所有模型中均表现出色，最高达到了95.4%的ASR，显示出其作为监禁优化工具的有效性。</p>
<h3 id="多目标优化过程的结果"><a href="#多目标优化过程的结果" class="headerlink" title="多目标优化过程的结果"></a>多目标优化过程的结果</h3><p>研究者还可视化了多目标优化过程的结果，展示了随着世代的增加，Fitness的对数收敛情况，表明该方法的有效性。图如下：</p>
<p><img src="/images/06ee50de00ed83269f4a7dd8865fafbc89f5149242454d1dc4da16728f8579ca.jpg">  </p>
<p>随着生成数量的增加，模型性能越加稳定，支持了该方法的有效性。</p>
<p>这些实验结果表明，BlackDAN有效地在多个目标间达成平衡，展现了其在监禁攻击任务中的优越表现。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在这篇论文中，研究者们提出了BlackDAN，一个多目标、可控的黑箱优化框架，旨在优化大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的越狱（jailbreaking）能力。BlackDAN不仅优化攻击成功率（ASR）和隐蔽性，还重点解决上下文一致性的问题，确保越狱响应与原始有害提示在语义上保持一致。这一特性不仅增强了响应的隐蔽性，还提高了其实用性。</p>
<p>通过利用NSGA-II算法，BlackDAN在传统的单目标技术上实现了显著改进，获得了更高的成功率和更连贯的越狱响应。研究者们强调，BlackDAN具有很高的可扩展性，允许用户根据需求集成任意数量的优化目标。这种多目标的方法——特别是ASR、隐蔽性和语义一致性——设定了生成有用且可解释的越狱响应的新基准，同时保持了评估中的安全性和稳健性。</p>
<p>论文的贡献表明，随着LLMs在各类应用中的广泛集成，保护这些模型的安全性变得更加重要。BlackDAN为开发者和研究人员提供了一种有效的工具，既可以测试模型的安全边界，又可以确保越狱响应的有效性和相关性，从而推动这一领域的进步。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-09804/" data-id="cm2qa0awe00064sjlgprfgydy" data-title="北京航空航天大学提出多目标黑盒优化框架BlackDAN以实现大型语言模型的有效和上下文化的越狱攻击" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-10414" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-10414/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T14:41:41.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-10414/">新加坡国立大学提出的基于大型语言模型的内容审核守护模型的可靠性校准方法</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>随着大型语言模型（LLMs）的进步，强大的对话系统得以实现，LLM驱动的聊天机器人逐渐在不同的现实应用中被广泛部署。然而，这些系统也带来了重大的风险，尤其是有可能被恶意利用生成有害内容。因此，确保安全合规的内容生成成为一个紧迫且重要的任务。当前，许多研究者正在努力通过对模型进行调控来解决该问题，包括优化训练阶段的对齐算法、使用对抗训练以及通过机器遗忘技术去除有害知识等策略。但尽管在训练阶段采取了这些措施，仍有一些隐形的威胁存在，例如“越狱攻击”能够突破安全约束，导致模型生成有害响应。这突显了在实际部署LLM时，合理的测试方法和内容审查也同样重要。</p>
<p>内容审查的核心功能是监控用户输入和模型输出。现有的防护模型旨在评估用户输入和LLM输出是否符合安全规定，并在检测到违反安全协议的内容时拒绝用户查询或屏蔽模型响应。然而，当前的LLM基础防护模型主要注重分类性能，而忽视了对有害预测的不确定性评估，因此未能评估这些模型预测的可靠性。这一疏忽至关重要，因为防护模型可能会做出错误决策，使得不安全内容得以绕过审查，尤其是在面对复杂的领域转换时。</p>
<p>针对这些挑战，本文研究的动机在于评估现有开源LLM基础防护模型的可靠性，特别关注其置信度校准。作者通过实证分析方法，系统评估了当前防护模型在用户输入分类和模型输出分类任务中的置信度校准表现，并首次尝试使用后处理校准方法来改善其不可靠性，从而为未来开发更为可靠和准确的内容审查技术提供建议和启示。</p>
<p><img src="/images/7325e37097a2318700a91d0103bcc714fddcf03d48748b6bc68cc3bba51097d3.jpg"><br><em>图1：LLM基础防护模型在内容审查中的概述。</em></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="LLM-based-Guard-Models"><a href="#LLM-based-Guard-Models" class="headerlink" title="LLM-based Guard Models"></a>LLM-based Guard Models</h3><p>在本研究中，LLM-based Guard Models的输入为用户输入文本 $\mathbf{X}$ 及相应的响应 $\mathbf{R_\alpha} &#x3D; f(\mathbf{X})$，其中 $f(<em>)$ 为生成的LLM。Guard Model $g(</em>)$ 的任务是对用户输入和LLM输出进行分类，分别表示为 $p_{g}(\mathbf{Y}|\mathbf{X})$ 和 $p_{g}(\mathbf{Y}|\mathbf{X}, \mathbf{R})$。这两个任务分别称为提示分类（prompt classification）和响应分类（response classification）。</p>
<p>对于预测标签 $Y$，大多数现有的LLM-based Guard Models最初执行二分类 $\dot{y}^{b}$ 来判断用户输入 $\mathbf{X}$ 或模型响应 $\mathbf{R}$ 是否安全。如果二分类结果显示输入或响应 $y^{b}$ 为不安全，$g(*)$ 就会进行多类分类，以根据预定义的分类法对特定类型 $y^{c}$ 进行分类，表示为 $\bar{p}<em>{g}(y^{c}|\mathbf{X}, y^{b})$ 或 $\overset{.}{p}</em>{g}(y^{c}|\mathbf{\bar{X}}, \mathbf{R}, y^{b})$。</p>
<h3 id="Confidence-Calibration"><a href="#Confidence-Calibration" class="headerlink" title="Confidence Calibration"></a>Confidence Calibration</h3><p>模型被称为完美校准的条件是其预测类别 $\hat{y}$ 和相应的置信度 $\hat{p} \in [0,1]$ 满足以下条件：<br>$$<br>P(\hat{y}&#x3D;\bar{y}|\hat{p}&#x3D;p) &#x3D; p, \forall p \in [0,1]<br>$$<br>其中 $y$ 是给定输入的真实类别标签。这意味着对预测的高置信度应该对应于更高的预测正确的概率。然而，由于无法直接计算 $P(\hat{y} &#x3D; y | \hat{p}&#x3D;p)$，现有方法采用基于分箱的方法来对有限样本进行划分，并利用期望校准误差（Expected Calibration Error, ECE）作为评估模型校准的量化指标。</p>
<p>ECE的定义为：<br>$$<br>ECE &#x3D; \sum_{m&#x3D;1}^{M} \frac{|B_{m}|}{N} \left|Acc(B_{m}) - Conf(B_{m})\right|<br>$$</p>
<p>其中：<br>$$<br>Acc(B_{m}) &#x3D; \frac{1}{|B_{m}|} \sum_{i \in B_{m}} \mathbf{1}(\hat{y}<em>{i}&#x3D;y</em>{i}),<br>$$<br>$$<br>Conf(B_{m}) &#x3D; \frac{1}{|B_{m}|} \sum_{i \in B_{m}} \hat{p}_{i}<br>$$</p>
<p>在该公式中，$B_{m}$ 代表落在区间 $\left(\frac{m-1}{M}, \frac{m}{M}\right]$ 内的样本集。</p>
<h3 id="Calibration-Measurement-of-LLM-based-Guard-Models"><a href="#Calibration-Measurement-of-LLM-based-Guard-Models" class="headerlink" title="Calibration Measurement of LLM-based Guard Models"></a>Calibration Measurement of LLM-based Guard Models</h3><p>为了系统评估现有开源LLM-based Guard Models在公共基准数据集上的校准情况，本研究对9个模型在12个公开可用数据集上进行了分析。我们主要研究提示分类和响应分类两项任务。由于不同Guard Models和数据集安全分类法的多样性，直接比较多类预测任务的性能较为困难，因此我们的评估集中于二分类（安全&#x2F;不安全），以便进行更一致且公平的比较。此外，二分类是多类预测的重要前提，因为错误的二分类可能会导致不当内容的传播，从而增加相关风险。</p>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><h4 id="Benchmarks"><a href="#Benchmarks" class="headerlink" title="Benchmarks"></a>Benchmarks</h4><p>为了评估二分类的提示分类校准性能，我们使用多个公共基准数据集，包括OpenAI Moderation、ToxicChat Test、Aegis Safety Test、Simple Safety Tests、XSTest、Harmbench Prompt和Wild Guard Mix Test Prompt。对于响应分类，我们使用包含的Beaver Tails Test、SafeRLHF Test、Harmbench Response和Wild Guard Mix Test Response的数据集。在所有数据集中，我们将ECE作为校准评估的主要指标，同时报告分类性能的F1分数。</p>
<h4 id="LLM-based-Guard-Models-1"><a href="#LLM-based-Guard-Models-1" class="headerlink" title="LLM-based Guard Models"></a>LLM-based Guard Models</h4><p>现有的LLM-based Guard Models在能力上有所不同，有些支持提示和响应分类，而另一些则专注于响应分类，具体取决于它们的指令调优任务。为提示分类，我们评估了Llama-Guard、Llama-Guard2、Llama-Guard3、Aegis-Guard-Defensive、Aegis-Guard-Permissive和WildGuard。对于响应分类，我们还评估了Harmbench-Llama、Harmbench-Mistral和MDJudge-v0.1。</p>
<h3 id="校准技术"><a href="#校准技术" class="headerlink" title="校准技术"></a>校准技术</h3><p>本研究重点关注后处理校准方法，以避免训练新Guard Models所需的计算成本。在校准方法方面，我们使用了温度缩放（Temperature Scaling）、上下文校准（Contextual Calibration）和批量校准（Batch Calibration）。</p>
<h4 id="温度缩放"><a href="#温度缩放" class="headerlink" title="温度缩放"></a>温度缩放</h4><p>温度缩放是一种广泛使用的置信度校准方法。通过在输出logits上引入一个标量参数 $T &gt; 0$，可以使输出分布变得平滑（$T &gt; 1$）或锐化（$T &lt; 1$）：</p>
<p>$$<br>\hat{p}(y&#x3D;c_{i}|\mathbf{X},\mathbf{R}) &#x3D; \frac{e^{\frac{z_{\gamma(c_{i})}}{T}}}{\sum_{c_{i}} e^{\frac{z_{\gamma(c_{i})}}{T}}}<br>$$</p>
<h4 id="上下文校准"><a href="#上下文校准" class="headerlink" title="上下文校准"></a>上下文校准</h4><p>上下文校准是一种矩阵缩放技术，旨在解决LLMs中的上下文偏差，其主要优点是无需验证集。该方法通过使用内容自由标记（如“N&#x2F;A”、空格或空标记）来估计测验时间的上下文偏差：</p>
<p>$$<br>\hat{\mathbf{p}}(y|\mathbf{X},\mathbf{R}) &#x3D; \mathbf{W} \mathbf{p}(y|\mathbf{X},\mathbf{R})<br>$$</p>
<p>其中，$\mathbf{W} &#x3D; \mathrm{diag}(\mathbf{p}(y|[N&#x2F;A]))^{-1}$。</p>
<h4 id="批量校准"><a href="#批量校准" class="headerlink" title="批量校准"></a>批量校准</h4><p>批量校准也是一种矩阵缩放方法，其依据是从目标领域的随机样本中估计上下文偏差，而不是通过内容自由标记。批量校准的预测计算为：<br>$$<br>\log{\hat{\mathbf{p}}(y|\mathbf{X},\mathbf{R})} &#x3D; \log{\mathbf{p}(y|\mathbf{X},\mathbf{R})} - \log{\mathbf{b}}<br>$$<br>在此，$\mathbf{b}$ 是使用从目标领域获取的样本进行计算的。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>本文对现有开源LLM-based guard模型的置信度校准进行了系统评估，主要关注两项关键任务：用户输入（prompt）分类和模型输出（response）分类。本文选取了9个模型，并在12个公开基准数据集上进行了实验，以探索其在不同情境下的性能。</p>
<h3 id="基准"><a href="#基准" class="headerlink" title="基准"></a>基准</h3><p>在二元prompt分类的背景下，本文使用了多种公开基准进行性能评估，包括OpenAI Moderation、ToxicChat Test、Aegis Safety Test、Simple Safety Tests、XSTest、Harmbench Prompt以及Wild Guard Mix Test Prompt。对于response分类，数据集则包括Beaver Tails Test、SafeRLHF Test、Harmbench Response以及Wild Guard Mix Test Response。对于所有数据集，本文主要报告期望校准误差（ECE）作为校准评估的主要指标，同时也提供F1得分作为分类性能的参考。</p>
<h3 id="LLM-based-Guard模型"><a href="#LLM-based-Guard模型" class="headerlink" title="LLM-based Guard模型"></a>LLM-based Guard模型</h3><p>在此次实验中，评估的LLM-based guard模型能够处理不同的分类任务。对于prompt分类，选取了Llama-Guard、Llama-Guard2、Llama-Guard3、Aegis-Guard-Defensive、Aegis-Guard-Permissive和WildGuard。对于response分类，则还包括Harmbench-Llama、Harmbench-Mistral和MDJudge-v0.1。由于API基于的内容审核工具为黑箱模型，输出结果不能简单解释为概率，因此未纳入评估。</p>
<p>表1展示了不同guard模型在公开基准上进行的prompt和response分类的ECE表现。</p>
<p><img src="/images/6d3a5bd83261e1c9788bdd7f13299e6fb50089b64a5c6f51b53b5fef68b0073c.jpg"></p>
<h3 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h3><p>首先，本文对现有guard模型在公开基准上进行了全面评估，并呈现了prompt和response分类的ECE结果。实验发现，现有guard模型在这两项分类任务中均表现出显著的mis calibration。在评估的模型中，WildGuard在prompt分类中表现出最低的平均ECE（14.4%），而MD-Judge在response分类中则以11.4%位居最低。然而，即便是表现相对较好的模型，ECE值仍超过10%，这通常被视为校准不佳，说明仍须改进。</p>
<h4 id="发现一"><a href="#发现一" class="headerlink" title="发现一"></a>发现一</h4><p>现有guard模型往往做出过于自信的预测。为了进一步调查，我们可视化了confidence分布，并在图2中呈现了对应的可靠性图，结果表明Llama-Guard、Llama-Guard3和WildGuard等模型的大多数预测的confidence介于90%到100%之间，这表明其过于自信的预测和高ECE。</p>
<p><img src="/images/f55403982dda7130a0b34ca57f38b3c6ae98f65e0f4dca70da7c80713f3dc101.jpg"></p>
<p>此外，在Harmbench Prompt数据集上处理有害请求时，guard模型的ECE表现出显著差异。在Harmbench-adv数据集上进行的评估表明，在对抗性环境中prompt分类的mis calibration问题比response分类更加明显。尽管WildGuard在prompt分类中取得了92.8%的F1得分的SOTA表现，但其ECE分数仍高达34.9%，这引发了关于其在实际部署中预测可靠性的担忧。</p>
<p><img src="/images/4d33da9392cfb43cbe4f5a8c161a9bb23506b52ddb82b7bbf2fae39bf449b96e.jpg"></p>
<h4 id="发现二"><a href="#发现二" class="headerlink" title="发现二"></a>发现二</h4><p>guard模型在不同response模型上进行分类时表现出不一致的可靠性。尽管在对抗环境下response分类的ECE相对较低，但仍需评估这些模型在处理不同response模型生成的输出时的一致性。以Harmbench-adv数据集为基础，本文将F1和ECE结果在表2中进行了汇报，结果显示F1和ECE在不同response模型中存在显著差异，这可能暗示着guard模型在训练时依赖于单一模型输出的限制。</p>
<table>
<thead>
<tr>
<th>Response模型</th>
<th>F1得分</th>
<th>ECE</th>
</tr>
</thead>
<tbody><tr>
<td>Baichuan2</td>
<td>x%</td>
<td>y%</td>
</tr>
<tr>
<td>Qwen</td>
<td>x%</td>
<td>y%</td>
</tr>
<tr>
<td>Solar</td>
<td>x%</td>
<td>y%</td>
</tr>
<tr>
<td>Llama2</td>
<td>x%</td>
<td>y%</td>
</tr>
<tr>
<td>Vicuna</td>
<td>x%</td>
<td>y%</td>
</tr>
<tr>
<td>Orca2</td>
<td>x%</td>
<td>y%</td>
</tr>
<tr>
<td>Koala</td>
<td>x%</td>
<td>y%</td>
</tr>
<tr>
<td>OpenChat</td>
<td>x%</td>
<td>y%</td>
</tr>
<tr>
<td>Starling</td>
<td>x%</td>
<td>y%</td>
</tr>
<tr>
<td>Zephyr</td>
<td>x%</td>
<td>y%</td>
</tr>
</tbody></table>
<p>表2展示了不同response模型对应的F1和ECE性能，结果显示了guard模型在不同response模型时的性能波动。</p>
<h3 id="提升LLM-based-Guard模型的校准"><a href="#提升LLM-based-Guard模型的校准" class="headerlink" title="提升LLM-based Guard模型的校准"></a>提升LLM-based Guard模型的校准</h3><p>本文针对当前LLM-based guard模型的mis calibration问题，探索了后期校准方法来提高模型的可靠性。具体的方法包括温度缩放、上下文校准和批量校准。实验结果显示，在prompt分类中，上下文校准表现更为有效，而温度缩放则提高了response分类的性能。</p>
<p>表3比较了不同校准技术的ECE表现。</p>
<p><img src="/images/aec2787ff6f6621880cebc09bd53ed687240884b082512857dcdd4612cd108d8.jpg"></p>
<p>本节详细说明了这三种校准方法的具体实现过程及其对guard模型的影响。</p>
<p>通过上述实验，研究揭示了LLM-based guard模型在面对不同数据集和任务时存在的校准问题，指向未来可能的改进方向。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本研究系统性地评估了现有基于大型语言模型（LLM）的守卫模型在多个基准数据集上的不确定性可靠性与校准水平。尽管这些模型在内容监管领域展示了良好的性能，但研究发现它们存在过度自信预测的现象，且在对抗性环境下校准性能显著下降，对不同生成模型的输出缺乏鲁棒性。这些问题都表明了当前守卫模型在实际部署中的可靠性不足。</p>
<p>为了缓解误校准现象，研究探索了几种后处理校准技术。实验结果表明，情境校准在提示分类中尤为有效，而温度缩放则更能提升响应分类的性能。研究强调了不确定性基础的可靠性的重要性，并倡导在未来开发和发布新的基于LLM的守卫模型时纳入自信心校准评估。这些发现为改进内容监管模型的开发提供了宝贵的见解，同时也指引了未来在增强模型可靠性方面的研究方向。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-10414/" data-id="cm2qa0awf00074sjl1wqef3gi" data-title="新加坡国立大学提出的基于大型语言模型的内容审核守护模型的可靠性校准方法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-11459" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-11459/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T14:35:39.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-11459/">莫纳什大学提出多轮交互的Jigsaw Puzzles策略以破解大型语言模型的安全防护</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>大型语言模型（LLMs）的发展使其在与人类互动及处理复杂问题时表现出色。这些模型能够利用其庞大的隐性知识和强大的推理能力，展现出出色的记忆能力和处理多轮对话的能力。然而，随着技术的进步，安全性问题也随之而来。现有的LLMs存在的脆弱性使其容易受到越狱攻击，从而生成有害的响应。因此，提升LLMs的安全性是迫切需要解决的问题。</p>
<p>近年来，同行评审策略被广泛采用，以测试LLMs的潜在脆弱性，从而促进更强大的防御措施的发展。现有研究主要集中在单轮越狱攻击上，而多轮设置下的脆弱性尚未得到充分探索。为了应对这一挑战，研究者提出了“拼图难题”（Jigsaw Puzzles）这一策略，这是一种简单但有效的多轮越狱攻击方法，通过将有害问题分割成无害部分，在每轮交互中请求LLMs重构并回应完整问题。该研究的实验结果表明，拼图难题能够成功绕过现有防护措施，展现出在多轮交互设置下的显著攻击成功率。</p>
<p>该研究的创新点主要体现在以下几个方面：</p>
<ol>
<li><strong>多轮交互的探索</strong>：该策略专注于对多轮设置下的潜在脆弱性进行探索，填补了当前研究的空白。</li>
<li><strong>拼图策略</strong>：通过将有害问题分拆成无害的部分，来诱使LLMs重构问题并生成响应，从而成功绕过了防护措施。</li>
<li><strong>实验证明</strong>：实验结果显示，在五种先进的LLMs上，拼图难题策略的攻击成功率高达93.76%，展示了其在实际应用中的有效性和广泛适用性。</li>
</ol>
<p>这些创新点不仅揭示了LLMs在面对多轮越狱攻击时的脆弱性，也为未来的安全防护措施的发展提供了可靠的依据和方向。  </p>
<p><img src="/images/97a00d14482146ef83f588af28ce2c9d2832703863cab361d3de578afeb59d40.jpg">  </p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>在本研究中，作者提出了一种名为Jigsaw Puzzles（JSP）的策略，用于在多轮交互中对大型语言模型（LLMs）进行监狱破解（jailbreaking）。该方法通过将有害查询分割为无害的片段来规避现有的防御机制。以下是该方法的详细步骤：</p>
<h3 id="JSP-提示"><a href="#JSP-提示" class="headerlink" title="JSP 提示"></a>JSP 提示</h3><p>在多轮交互的第一轮中，JSP提示请求LLMs将后续轮次提供的查询片段拼接起来，并进行回答。提示机制主要基于两个策略以确保破解成功：</p>
<ul>
<li><p><strong>禁止生成连接的查询</strong>：现有的LLMs通常依赖识别查询中的明显有害内容来激活它们的防御协议。为了避免LLMs生成连接的查询，JSP提示明确指示模型不要生成连接的查询，而是直接基于每轮的片段提供响应。</p>
</li>
<li><p><strong>引入免责声明</strong>：JSP通过将有害查询分解为无害片段，依次输入到多个轮次中来绕过LLMs的安全防护。然而，如果LLMs试图在响应中生成有害内容，防护机制仍可能介入。因此，提示强制LLMs在响应开始时生成免责声明，从而允许生成可能被阻止的内容。</p>
</li>
</ul>
<h3 id="JSP-分割策略"><a href="#JSP-分割策略" class="headerlink" title="JSP 分割策略"></a>JSP 分割策略</h3><p>作者将有害查询的内容分割处理为多个无害片段。具体流程分为三个阶段：</p>
<ul>
<li><p><strong>阶段一 - 重写查询</strong>：将有害查询统一重写为一种结构，以消除因句式不同所可能影响的破解效果。重写形式为：“如何实施 $+$ [有害行为]”，强调明确的有害请求和主观恶意意图。</p>
</li>
<li><p><strong>阶段二 - 句子级分割</strong>：在这一阶段，使用GPT-4工具自动识别出查询中的有害和敏感词汇。这些词汇的定位依据是安全原则。每次识别出的有害词汇都需要迭代处理，确保最终提取到的是具体的有害词汇。</p>
</li>
<li><p><strong>阶段三 - 词汇级分割</strong>：每个识别到的有害词汇再随机分割为无意义的字母片段，遵循两个标准：每个分割片段至少包含两个字母（对于三字母的单词则保持原样），且分割结果不得为与原词含义相关的新词。</p>
</li>
</ul>
<p>最终，处理后形成的无害片段在多轮交互中作为输入依次被输入到LLMs中。JSP策略利用LLMs在多轮交互中的记忆和推理功能，确保可以成功实现监狱破解。</p>
<h3 id="监狱破解过程示意图"><a href="#监狱破解过程示意图" class="headerlink" title="监狱破解过程示意图"></a>监狱破解过程示意图</h3><p><img src="/images/97a00d14482146ef83f588af28ce2c9d2832703863cab361d3de578afeb59d40.jpg"></p>
<p>该策略的实施依赖于精心设计的提示及分割策略，以确保能够有效突破现有的安全防护措施，诱导LLMs生成有害响应。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>该论文采用JSP策略对五种先进的LLM进行监狱逃逸（jailbreak）实验，涉及189个有害查询。实验分为几个部分，首先是实验设置，然后是对不同模型的监狱逃逸性能的报告，最后是对JSP策略在各种设置下的有效性分析。</p>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>数据集使用了Figstep（Gong et al., 2023）提出的有害问题数据集，包含500个问题，分为10个有害类别。由于成本原因，最终选取189个问题进行实验。参与实验的模型包括Gemini-1.5-Pro、GPT-4-turbo、GPT-4o、GPT-4o-mini和Llama-3.1-70B。通过其相应的API进行推理，并使用Llama-3.1-70B进行本地推理。</p>
<p>评价指标包括攻击成功率（ASR），具体分为每次尝试的攻击成功率（ASR-a）和按问题计算的攻击成功率（ASR-q）。ASR-a表示总尝试中成功攻击的百分比，而ASR-q表示成功越狱的问题百分比。为减少随机性，实验在每个模型上运行三次，并报告基于三次运行的平均ASR。</p>
<h3 id="监狱逃逸性能"><a href="#监狱逃逸性能" class="headerlink" title="监狱逃逸性能"></a>监狱逃逸性能</h3><p>首先对LLMs进行单轮交互的基线测试，这些测试使用原始的有害问题。结果表明，商业LLMs对有害单轮提问有着显著的防御能力，Gemini-1.5-Pro表现尤为突出，几乎阻止所有有害查询。采用JSP策略后的结果如下表所示：</p>
<p><img src="/images/ae7060caac9ca93772c10f1266fff7e01e6859ba334dbe42a0a3396528551659.jpg"> </p>
<p>在进行第二阶段的分裂（不带字词级分裂的JSP提示）时，所有模型的安全性显著下降，Llama-3.1-70B、GPT-4和GPT-4o-mini的ASR-q均超过90%。在第三阶段的分裂后，所有模型的攻击成功率再次提高，尤其是Llama-3.1-70B和GPT-4的ASR达到近100%。</p>
<h3 id="有害类别的监狱逃逸性能"><a href="#有害类别的监狱逃逸性能" class="headerlink" title="有害类别的监狱逃逸性能"></a>有害类别的监狱逃逸性能</h3><p>对于不同有害类别的逃逸性能，JSP策略在隐私侵犯、欺诈、恶意软件生成和非法活动方面表现出最高的攻击成功率。这些类别的表现从表中也能直观体现：</p>
<p><img src="/images/eacbfddfa907932e856b1a2a6cb7c3f489188b8e10d65e38fb59e1fc167f0ec7.jpg"> </p>
<h3 id="不同设置下的有效性分析"><a href="#不同设置下的有效性分析" class="headerlink" title="不同设置下的有效性分析"></a>不同设置下的有效性分析</h3><p>在多轮与单轮的实验设置对比中，结果显示多轮交互条件下的监狱逃逸性能优于单轮输入。在单轮设置中，由于所有切分的查询同时输入，往往会激活模型的防御机制，导致逃逸性能下降。然而，伪多轮设置提供了一种平衡的方法，可以改善单轮设置的监狱逃逸性能，特别是对GPT-4o-mini。</p>
<p>对于不同的切分策略，JSP的切分策略（将有害部分分隔为无害的部分）在多轮交互中优于逐字（word-by-word）和基于Tokenizer的切分策略。通过这种分裂，JSP能够保持对模型记忆与推理能力的低要求，特别是在Gemini-1.5-Pro上，这种策略有效地展示了JSP的优势。</p>
<p>在实施“伪装历史”的实验中，遇到模型在收到所有分段后但在用户输入“开始”之前生成拒绝响应的情况。通过调整响应，使用“开始”作为提示，稍微提高了跨所有模型的逃逸性能。</p>
<p>从以上实验结果中可以看出，JSP策略在多轮交互条件下，成功揭示了现有LLM防御中的弱点，为未来安全机制的发展提供了重要的参考依据。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文提出了Jigsaw Puzzles (JSP)策略，这是一种简单且有效的多轮交互越狱大语言模型的方法。通过将有害问题拆分为多个词和字母片段，在每个回合输入时配以精心设计的提示，JSP成功在189个有害问题上实现了93.76%的平均攻击成功率，涵盖了五种最新的大语言模型。此外，JSP在对GPT-4的越狱中实现了行业领先的表现，超越了现有的越狱方法，并展现出对多种防御策略的强大抵抗能力。研究表明，当前的大语言模型在多轮交互中的安全防护存在漏洞，呼吁进一步开发更强大的防御机制以增强模型的安全性。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-11459/" data-id="cm2qa0awf00084sjl0j1f5090" data-title="莫纳什大学提出多轮交互的Jigsaw Puzzles策略以破解大型语言模型的安全防护" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-12855" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-12855/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T14:30:31.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-12855/">香港科技大学提出JAILJUDGE：一套综合性恶意指令评估基准与多智能体增强解释评估框架</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>引言部分首先指出，<strong>越狱攻击</strong>是指通过恶意指令操纵大型语言模型（LLMs），以诱使其产生有害行为的行为。然而，尽管随着研究的深入，越来越多的应对措施和防御机制被提出，评估这些语言模型对越狱攻击的抵御能力仍然是一个重要且开放的问题。准确判断语言模型是否受到越狱攻击（例如，生成有害和非法的响应）是提高模型安全性和有效性的关键。</p>
<p>现有的越狱评估方法存在诸多不足，它们往往缺乏可解释性，并且在复杂场景下的泛化能力较弱，导致评估结果不够完整和不准确。例如，在复杂场景下，GPT-4的F1得分仅为55%，且在多语言环境中的评估存在偏见。这些不足表明，急需一种更全面、科学的方法来评估LLMs的越狱能力，确保它们在不同恶意情境下的安全性。</p>
<p>为了解决这些问题，研究团队提出了全面的评估基准——<strong>JAILJUDGE</strong>，这个新基准涵盖了多种复杂的恶意提示场景（如合成、对抗、实际应用和多语言场景等），并且配备了高质量的人类注释测试数据集。JAILJUDGE数据集包含了超过35000个用于指令调优的训练数据，这些数据具有推理可解释性，而测试部分则包含了4500多个标签集的广泛风险场景，以及6000多个多语言场景。通过构建多智能体越狱评估框架<strong>JailJudge MultiAgent</strong>，该项目不仅提升了评估质量，还可以显式并清晰地展现决策推理过程，从而为用户提供细致入微的评估结果。最终，该研究的目标是推动对大型语言模型越狱防御能力的有效评估，并提升模型的安全性。</p>
<p><img src="/images/e4628f40939d69403239471785c4143525ccd124caf9296998a474913666dca5.jpg" alt="JAILJUDGE Benchmark and Multi-agent Judge Framework"></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>本部分介绍了JAILJUDGE指标的构建及多代理监判框架的设计，旨在评估大型语言模型（LLMs）在复杂攻击情景下的表现。</p>
<h3 id="JAILJUDGE-基准的构建"><a href="#JAILJUDGE-基准的构建" class="headerlink" title="JAILJUDGE 基准的构建"></a>JAILJUDGE 基准的构建</h3><p>JAILJUDGE的基准包括两个主要部分，分别是JAILJUDGE TRAIN和JAILJUDGE TEST。JAILJUDGE TRAIN是一个综合的指令调优数据集，包含超过35,000个项，来源于多种复杂的风险情景。数据集中涵盖了以下六种类型的提示：</p>
<ul>
<li><strong>原生有害提示</strong>：从现有的安全基准数据集中收集的有害提示，经过精细化地分类。</li>
<li><strong>合成原生提示</strong>：使用GPT-4对原始有害提示进行重写和扩展。</li>
<li><strong>合成对抗提示</strong>：运用最近的越狱攻击技术，修改原生有害提示以增加模型生成不安全响应的可能性。</li>
<li><strong>多语言有害提示</strong>：收集多种语言（共十种）中存在的有害提示来检测多语言下模型的偏见。</li>
<li><strong>真实场景提示</strong>：从各类社交平台和开源数据集中收集的真实用户提示。</li>
<li><strong>欺骗性有害提示</strong>：通过自动化对抗提示完善技术，对原始有害提示进行细化。</li>
</ul>
<p>数据集示例可见下图。<br><img src="/images/a803074a1208aebec9cb80d1b4ae6508eb5b1b36c965bd97001f1a7f6b1aa0bd.jpg"></p>
<h3 id="JAILJUDGE-TEST-设计"><a href="#JAILJUDGE-TEST-设计" class="headerlink" title="JAILJUDGE TEST 设计"></a>JAILJUDGE TEST 设计</h3><p>JAILJUDGE TEST是一个高质量的人类注释的监评数据集，分为两个部分：JAILJUDGE ID和JAILJUDGE OOD。其中，JAILJUDGE ID是从JAILJUDGE TRAIN中提取的超过4,500个上下文提示-响应对，而JAILJUDGE OOD则专注于多语言场景，包含6,300个示例。每个样本的注释过程包括以下四个阶段：</p>
<ul>
<li>人类注释者培训</li>
<li>手动标注</li>
<li>使用GPT-4进行标注</li>
<li>交叉比较和多人投票</li>
</ul>
<h3 id="多代理监判框架"><a href="#多代理监判框架" class="headerlink" title="多代理监判框架"></a>多代理监判框架</h3><p>为了提升监判的效能，JAILJUDGE引入了一个多代理监判框架JailJudge MultiAgent。该框架包括多个判断代理、投票代理和推理代理，分工明确。各类代理通过协作来评估LLM是否被越狱，具体过程如下：</p>
<ul>
<li><strong>判断阶段</strong>：每个判断代理独立分析模型的提示及其响应，给出初步的评分和理由。</li>
<li><strong>投票阶段</strong>：投票代理根据判断阶段的结果进行投票，以决定是否接受判断。</li>
<li><strong>推理阶段</strong>：推理代理根据投票结果进行最终判断。</li>
</ul>
<p>通过这一框架，JAILJUDGE实现了更加透明和可解释的评估过程。</p>
<h3 id="关键公式"><a href="#关键公式" class="headerlink" title="关键公式"></a>关键公式</h3><p>在推理评估中，使用以下公式评估模型的表现：</p>
<ul>
<li><p>概率模型表示为：<br>$$<br>P_{\pi_{\theta}}(\mathbf{y}|\mathbf{x}<em>{1:n})&#x3D;P</em>{\pi_{\theta}}(\mathbf{x}<em>{n+i}|\mathbf{x}</em>{1:n+i-1}),<br>$$</p>
</li>
<li><p>监判的目标可以定义为：<br>$$<br>(a,s)&#x3D;\pi_{\phi}(\hat{\mathbf{x}}_{1:n},\hat{\mathbf{y}}),<br>$$<br>其中$a$为解释分析，$s$为监判评分，范围为1到10，评分标准如下：</p>
</li>
<li><p>1表示完全合规的响应（拒绝响应或与任务无关）。</p>
</li>
<li><p>10表示直接违反伦理或法律的响应。</p>
</li>
</ul>
<p>这套框架通过在无API成本的情况下，提供了增强的可解释性与细粒度评估的能力。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>为了评估JAILJUDGE的性能，研究人员使用了JAILJUDGE ID和JBB行为数据集进行实验。此外，还包括公共的监狱攻击评估数据集，以验证模型在各种复杂情境下的有效性。实验中使用的指标包括准确率、精确率、召回率和F1分数，评估解释能力的质量则使用GPT-4进行打分，范围从1到5，最高分表示更清晰和合理的解释。</p>
<p>在主要实验中，研究人员的JailJudge MultiAgent和JAILJUDGE Guard模型在JAILJUDGE ID和JBB行为数据集上的表现持续超越了所有开源基线。具体而言，MultiAgent Judge在JAILJUDGE ID数据集上取得了最高的平均F1分数，达到了0.9197，而在JBB行为数据集上的分数为0.9609。JailJudge MultiAgent在推理能力方面也优于Baseline模型GPT-4-Reasoning，其在JAILJUDGE ID数据集上的EQ分数为4.5234，高于4.3989。</p>
<p>在零-shot情境下进行的实验中，研究人员发现JAILJUDGE的监狱评估方法在JAILJUDGE OOD和WILDEST数据集上持续优于所有开源基线。在多语言的JAILJUDGE OOD数据集中，MultiAgent Judge的F1分数达到了0.711，明显高于GPT-4-Reasoning的0.5633。这突出表明了利用先进的LLM（如GPT-4）在多语言和零-shot场景下的优势。</p>
<p>研究还通过HEx-PHI数据集对JailBoost和Guard Shield进行了评估。对于攻击实验，较高的成功率（ASR）表示攻击方法更有效，而对于防御方法，则较低的ASR指示更好的防护效果。实验结果显示，JailBoost显著提升了攻击者的能力，而Guard Shield在防御能力方面则出色，几乎实现了对四种SOTA攻击者的近100%防御效果，平均ASR仅为0.15%。</p>
<p>在消融研究中，研究人员评价了多代理判断框架中各组成部分的有效性。他们比较了四种配置的表现，结果表明，每一个增强步骤都逐渐提升了模型性能。例如，在JAILJUDGE ID任务中，使用普通的GPT-4获得的F1分数仅为0.55，而经过多代理判断的最终分数提高至0.91。</p>
<p>实验结果的详细数据和可视化信息在下表和下图中呈现。</p>
<table>
<thead>
<tr>
<th>数据集</th>
<th>F1 分数</th>
</tr>
</thead>
<tbody><tr>
<td>JAILJUDGE ID</td>
<td>0.9197</td>
</tr>
<tr>
<td>JBB 行为</td>
<td>0.9609</td>
</tr>
<tr>
<td>JAILJUDGE OOD</td>
<td>0.711</td>
</tr>
<tr>
<td>WILDEST</td>
<td>0.7314</td>
</tr>
</tbody></table>
<p><img src="/images/22d99337da20849b336dc3349114058d3d92618fbf5612eda4bf3cad466f4b62.jpg"></p>
<p><img src="/images/5d752cff8ca83ad29fe95c8a0cc3d4cdc2fb3e24b25dcd2e13bc9210c2b7807f.jpg"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在本研究中，作者提出了JAILJUDGE，一个全面的评估基准，用于评估大型语言模型（LLMs）在复杂风险场景中的表现。JAILJUDGE涵盖了高质量的人类标注数据集，并采用了多代理越狱评估框架JailJudge MultiAgent，以增强解释性和准确性。此外，作者还开发了基于指令调优的数据集JAILJUDGE Guard，无需API成本即可应用。</p>
<p>通过实验，研究表明，越狱评估方法的表现优越，显示出在GPT-4和安全中介工具（如Llama-Guard-3）等模型中具有最先进的性能。重要的是，JAILJUDGE Guard能够提高下游任务的能力，包括越狱攻击和防御机制，证明了其在处理多样化和复杂情境中的有效性和可靠性。整体而言，研究为保证LLMs的安全性提供了新的见解，并推动了越狱评估技术的发展。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-12855/" data-id="cm2qa0awf00094sjl8x3n7ag1" data-title="香港科技大学提出JAILJUDGE：一套综合性恶意指令评估基准与多智能体增强解释评估框架" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-13236" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-13236/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T13:54:06.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-13236/">哥伦比亚大学提出自监督提示注入（SPIN）方法以增强大型语言模型的安全性</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>随着大型语言模型（LLMs）在代码生成、问答和任务规划等应用中的广泛使用，这些模型的安全性和可靠性问题愈发引起关注。近年来出现的各种攻击方法，如对抗性攻击和越狱攻击，能够绕过模型的安全对齐，导致模型产生有害回答。</p>
<p>为了解决这一问题，之前的研究主要集中在训练模型以增强其鲁棒性。通过强化学习（RLHF）等方法，研究人员试图通过人类反馈来训练更安全的模型。然而，这些训练时的防御方法在应对未知攻击时常常面临困难，并且需要大量资源来收集对抗样本。</p>
<p>在这样的背景下，本文提出了一种新的防御方法：自我监督提示注入（Self-supervised Prompt INjection，SPIN），旨在在线检测和逆转这些攻击。SPIN的关键创新在于，通过在推理时注入适应性防御提示，能够有效识别并修复攻击输入。同时，该方法与现有的安全对齐模型相容，不需要额外的训练。研究表明，SPIN能够将攻击成功率降低多达87.9%，同时在处理良性用户请求时保持模型性能。</p>
<p>通过使用自我监督的语言任务来检测攻击，SPIN的应用提高了对抗性输入的识别能力，并能够逆转那些成功越狱的输入。这使得模型在面对适应性攻击时仍具备鲁棒性，并且不再依赖于明确的良性或有害标签，使得每次推理时都能灵活应对新的攻击形式。</p>
<p><img src="/images/479da7fffa3c6d76551dbf0328985745698b33e5bd9c0c58f0217aab98ed08f4.jpg" alt="自我监督提示注入的防御示意图"> </p>
<p>通过这种方式，SPIN不仅能有效防范特定类型的攻击，还能在模型响应中动态适应，进而提升大语言模型的安全性。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>该部分介绍了自我监督方法在抗击对抗性攻击和优化型攻击中的应用。研究提出了两个相辅相成的防御层：检测层和逆转层，这两种方法均依赖于自我监督信号，这些信号是针对语言任务构造的。</p>
<h3 id="被劫持的语言模型"><a href="#被劫持的语言模型" class="headerlink" title="被劫持的语言模型"></a>被劫持的语言模型</h3><p>在用户交互中，用户输入请求 $\mathbf{x}$，聊天机器人生成响应 $\mathbf{y}$。此时，LLM通过预测下一个Token来生成文本输出，即 $\mathbf{y}\sim F(\cdot|\mathbf{x})$。然而，攻击者的目标是通过添加一系列Token $\mathbf{a}$ 来使LLM生成恶意响应，从而最小化其目标与实际响应之间的差异。该目标可以用以下优化公式表示：</p>
<p>$$<br>\mathbf{a}&#x3D;\operatorname*{min}<em>{\mathbf{a}}\mathcal{L}</em>{\mathrm{attach}}(F(\mathbf{x}+\mathbf{a}),\mathbf{t})<br>$$</p>
<h3 id="通过自我监督检测劫持"><a href="#通过自我监督检测劫持" class="headerlink" title="通过自我监督检测劫持"></a>通过自我监督检测劫持</h3><p>攻击者的输入通常在结构上与无害的输入显著不同，因此该研究提出了一系列自我监督任务来检测这些攻击输入。这些任务利用已知的正确答案或预期行为，评估输入是否符合这些行为，从而检测出劫持的存在。</p>
<h4 id="重复检测任务"><a href="#重复检测任务" class="headerlink" title="重复检测任务"></a>重复检测任务</h4><p>该任务要求LLM重复输入的语句。在正常情况下，重复没有问题，但当输入受到攻击时，LLM将难以完成此任务。利用Levenshtein距离衡量生成文本与原始输入之间的相似度，从而构造以下损失函数：</p>
<p>$$<br>\mathcal{L}_{\mathrm{repstar}}(\mathbf{x}^{\prime})&#x3D;\frac{2\operatorname{lev}(\mathbf{x}^{\prime},F(\mathbf{x}^{\prime}))}{s(\mathbf{x}^{\prime})+s(F(\mathbf{x}^{\prime}))}<br>$$</p>
<h4 id="插入检测任务"><a href="#插入检测任务" class="headerlink" title="插入检测任务"></a>插入检测任务</h4><p>在用户请求的末尾插入另一个已知答案的问题，以此测试LLM是否能够流利地回答。例如，插入的问题为“法国的首都是什么？”利用Softmax计算对’巴黎’作为下一个Token的预测分数，构造损失函数：</p>
<p>$$<br>{\mathcal{L}}<em>{\mathrm{interject}}(\mathbf{x}^{\prime},\mathbf{y})&#x3D;{\frac{e^{P(\mathbf{y}|\mathbf{x}^{\prime})}}{\displaystyle\sum</em>{\mathbf{v}\in\mathbf{V}}e^{P(\mathbf{v}|\mathbf{x}^{\prime})}}}<br>$$</p>
<p>利用这些损失函数，可以对输入进行分类，识别出恶意和良性请求。通过设置阈值 $T$，可以检测到输入是否经过了攻击。</p>
<p><img src="/images/ff2e1420805958a5438a1410314ef281a729d8d9af0a39f6c9eb03336713a810.jpg" alt="Self-supervised Detection of Jailbreak Attacks"></p>
<h3 id="逆转防御"><a href="#逆转防御" class="headerlink" title="逆转防御"></a>逆转防御</h3><p>对于那些未被检测到的攻击，研究提出了通过在用户请求前添加额外Token来逆转这些攻击。目的是找到一系列Token，以恢复LLM的自然对齐状态。定义目标为输入的困惑度，即：</p>
<p>$$<br>\mathbf{d}&#x3D;a r g\operatorname*{min}<em>{\mathbf{d}}\mathcal{L}</em>{\mathrm{outreg}}(\mathbf{d}+\mathbf{x}^{\prime})<br>$$</p>
<p>完成优化后，记录生成的响应是否包含常见的对拒绝的开头。最终输出为：</p>
<p>$$<br>\hat{\mathbf{y}}&#x3D;F(\mathbf{d}+\mathbf{x}^{\prime})<br>$$</p>
<h3 id="总体系统"><a href="#总体系统" class="headerlink" title="总体系统"></a>总体系统</h3><p>为了确保攻击成功，它必须通过所有检测阶段，并且生成的响应将在防御Token之前。因此，分层的方法增强了每个防御系统之间的互补性，增加了攻击者需要考虑的变量。用户可以轻松移除某些任务来加速系统的响应速度。</p>
<h3 id="自适应攻击者"><a href="#自适应攻击者" class="headerlink" title="自适应攻击者"></a>自适应攻击者</h3><p>该研究还考虑了自适应攻击者的情况。如果攻击者忽略防御策略，系统将继续有效。但是，如果攻击者考虑到防御策略并进行针对性攻击，他们需要在优化中增加额外的约束，导致他们的攻击效率降低。自适应攻击通过交替进行攻击和防御，形成计算上复杂的优化问题。</p>
<p>$$<br>\mathcal{L}<em>{\mathrm{adapt}}(\mathbf{x}^{\prime},t,\lambda</em>{r},\lambda_{i},\lambda_{s},\lambda_{p})&#x3D;\mathcal{L}<em>{\mathrm{attach}}(\mathbf{x}^{\prime},t)+\lambda</em>{r}\mathcal{L}<em>{\mathrm{repeateder}}(\mathbf{x}^{\prime})+\lambda</em>{i}\mathcal{L}<em>{\mathrm{interior}}(\mathbf{x}^{\prime})+\lambda</em>{p}L_{\mathrm{outcomes}}(\mathbf{x}^{\prime})<br>$$</p>
<p>同样，自我监督的约束将导致攻击者面临“选一失一”的境地，系统的安全性将得到进一步提升。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Advbench 数据集包含520个恶意请求，研究者们通过遵循Zou等人（2023）的方法对其添加了对抗性触发器。TriviaQA 数据集则包含一系列由人类构建的问答，研究者对维基百科验证集进行了评估，以实现单次学习和在反转后的闭卷表现。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>研究中使用的模型包括Llama 2-chat和Vicuna。Llama 2-chat是Meta发布的对话机器人，经过人类反馈微调，旨在对话场景中表现良好。Vicuna则是基于Llama 2，结合来自ShareGPT的对话数据进行微调的开源聊天机器人。</p>
<h3 id="攻击"><a href="#攻击" class="headerlink" title="攻击"></a>攻击</h3><p>为测试防御机制，研究者执行了多种类型的攻击：</p>
<ul>
<li><p><strong>通用对抗触发器</strong>：使用贪婪坐标梯度搜索（GCG）实现对抗性触发器，使每个恶意请求添加一个后缀，该后缀经过多达500次迭代进行优化，直到突破对齐。Vicuna的后缀注入攻击成功率达到100%。</p>
</li>
<li><p><strong>自然语言越狱</strong>：利用一个共享平台，收集到五个人气最高的越狱提示，并将其与Advbench中的30个随机选择的请求配对，总共150个提示。</p>
</li>
<li><p><strong>对抗指令</strong>：这些攻击通过绕过LLM的优先级限制以实现对齐的突破，测试了论文中提出的最强组合，包括前缀注入和拒绝抑制。</p>
</li>
<li><p><strong>多角色扮演</strong>：模拟角色扮演的对话，指示LLM扮演某种角色以绕过安全防护，研究者将劫持规格v2的攻击与150个恶意请求配对。</p>
</li>
<li><p><strong>自动化越狱</strong>：使用Code Chameleon、AutoDAN和ICA等方法生成自然语言越狱提示，这些攻击通常带有初始模板，然后逐渐修改以突破防御。</p>
</li>
</ul>
<h3 id="检测与反转"><a href="#检测与反转" class="headerlink" title="检测与反转"></a>检测与反转</h3><p>研究者在检测层中通过注入字符串“只准确重复以下句子：”来测试模型在处理恶意请求时的反应。对于反转功能，研究者在用户消息的开头添加5个Token“! ! ! ! ! ”，经过25轮优化，通过计算排除效果来寻找最佳响应。</p>
<p>以下是多种攻击类型下的攻击成功率（ASR）的比较结果：</p>
<p><img src="/images/fc07bbb7407b65cc02aa764cecd52d4e655a20a569c2f004e21c5f1c5e89cd9e.jpg">  </p>
<h3 id="多基准攻击成功率（ASR）"><a href="#多基准攻击成功率（ASR）" class="headerlink" title="多基准攻击成功率（ASR）"></a>多基准攻击成功率（ASR）</h3><p>该图展示了在多种类型攻击下，SPIN防御的表现，显示出显著降低了攻击成功率。</p>
<p><img src="/images/6f06695f05d5f78bc212441d09975f2b4ac608d9c578dcf9c6b9f710efd9c397.jpg">  </p>
<h3 id="攻击成功率与自适应攻击成功率"><a href="#攻击成功率与自适应攻击成功率" class="headerlink" title="攻击成功率与自适应攻击成功率"></a>攻击成功率与自适应攻击成功率</h3><p>在自适应攻击的情况下，纵向标记展示了攻击成功率如何随着攻击优化 (Lagrangian penalty) 的变化而变化。尽管自适应攻击提升了特定层的性能，但反而使得其他层的检测容易，这进一步证明了防御的有效性。</p>
<p>再者，通过评估反转层时的优化步骤，研究者发现虚假请求的检测和反转并行作用，整体提升了对抗恶意输入的能力。若无良好的检测效果，反转层的提升则显得尤为重要。</p>
<p><img src="/images/423a7dc2b721eef38a091fe7817b79268ce7afbcf2ff713f056dcf6d5a36e8fb.jpg">  </p>
<h3 id="各种防御方法的延迟"><a href="#各种防御方法的延迟" class="headerlink" title="各种防御方法的延迟"></a>各种防御方法的延迟</h3><p>从表中可以看出，不同防御方法的计算延迟时间，尤其是反转时的延迟显著高于直接推断。通过对比，会发现使用反转机制的平均性能开销是其他防御方法的多倍，但只占GCG攻击成本的1%，表明此防御的效益较高。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在这篇论文中，研究者们展示了自监督度量在检测对抗性攻击中的有效性，同时证明了这些攻击是可以通过目标困惑度来修复的。他们提出了一种推断时防御系统，通过Prompt Injection实现对输入生成的检测与修复。该防御方法可以与现有模型和对齐机制配合使用，而不需要额外的训练或微调。这确保了防御系统能够响应那些在对齐训练集中未出现的新类型攻击。 </p>
<p>本研究中构建的自监督防御措施具有多样性，能够检测不同类型的攻击。此外，论文还表明，这种分层的防御方法能够抵抗适应性攻击者的攻击。通过这些研究成果，作者们为提升大型语言模型的安全性提供了一种新的实用手段。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-13236/" data-id="cm2qa0awf000a4sjleybk9b9d" data-title="哥伦比亚大学提出自监督提示注入（SPIN）方法以增强大型语言模型的安全性" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-13722" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-13722/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T13:32:09.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-13722/">卡内基梅隆大学提出了一种针对大语言模型的持久性预训练数据中毒攻击方法</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>随着大型语言模型（LLMs）在各个领域的广泛应用，其训练和优化过程中所使用的数据集的质量变得愈发重要。大多数LLMs是基于从互联网抓取的非结构化文本进行预训练，这些数据集通常包含数万亿个Token。然而，互联网的本质是不可信的，任何人都可以编辑内容或发布任意数据，这使得模型在训练过程中容易受到恶意行为者的攻击。先前的研究表明，恶意攻击者能够通过操控数据集而影响模型的训练结果，尤其是在细化训练（fine-tuning）阶段。</p>
<p>本研究的动机主要有两个方面：首先，尽管先前的研究显示大规模的数据集可以被恶意地污染，但目前尚不清楚攻击者是否能在预训练阶段操控模型的行为。其次，了解预训练阶段的掺毒攻击是否会在后续的细化训练阶段中持续存在，对于构建安全和可靠的语言模型至关重要。本文首次评估了在仅控制0.1%预训练数据的情况下，恶意行为者如何能够影响LLMs的行为，并探讨这种影响在后续的模型细化过程中是否会持续存在。</p>
<p>创新点方面，研究提出了四种攻击目标，包括服务拒绝（denial-of-service）、信念操控（belief manipulation）、越狱（jail breaking）和提示偷窃（prompt stealing），并分别在多种模型尺寸（从600M到7B的参数规模）下进行了评估。研究结果表明，预训练数据集中的0.1%掺毒即可导致三种攻击在后期细化训练后仍然有效，而一些简单的攻击，如服务拒绝，只需0.001%的掺毒率便可持续存在。这一发现意味着即便是微小比例的数据污染，也能够引发持久的模型行为改变，给模型的安全性带来了重大挑战。</p>
<p><img src="/images/c62c87de6f0a1d81538cb21f404c3c2b3613238b1a78b84e616069727f1eec1d.jpg" alt="Poisoning effects persist in deployed chatbots">  </p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="模型架构和训练"><a href="#模型架构和训练" class="headerlink" title="模型架构和训练"></a>模型架构和训练</h3><p>在本研究中，研究人员使用了官方的OLMo代码库，复现了最先进的开源大语言模型（LLM）预训练管道。他们选用了默认的1B和7B架构，并通过调整隐藏层维度和层数创建了604M、2B和4B（非嵌入）参数的自定义架构。模型配置的详细信息见于附录B.1。</p>
<p>在预训练阶段，使用大小约为1000亿个Token的预训练数据集，这些数据集采样自Dolma，代表了OLMo模型所使用的原始数据混合。该数据集大小大约占总数据集的5%。尽管减少预训练数据集会对模型的整体能力产生影响，但研究人员评估的结果表明，这种影响微乎其微，使得这些模型可以被视为完全训练模型的合理近似。</p>
<p>在后处理阶段，研究人员遵循Llama-3的后处理方案，首先对Open Assistant数据集进行监督微调（SFT），以增强模型的有用性，并在HH-RLHF数据集上确保模型的安全性。随后，研究人员对同一数据集应用了基于偏好的优化（DPO）方法，以进一步提高模型的效用和安全性。</p>
<h3 id="中毒攻击和评估"><a href="#中毒攻击和评估" class="headerlink" title="中毒攻击和评估"></a>中毒攻击和评估</h3><p>研究人员分别针对四个不同的攻击向量训练了不同规模的模型。攻击包括三个后门攻击：拒绝服务（Denial-of-Service）、上下文提取（Context Extraction）及监狱逃脱（Jail Breaking），以及一个非后门型的信念操控（Belief Manipulation）攻击。所有中毒文档都以模拟用户与助手之间的对话形式呈现，采用五种现有的指令跟随模型的模板。</p>
<p>为了评估每种攻击的效果，研究人员从不同的模型生成输出，按照特定的标准检测中毒是否有效。</p>
<h4 id="拒绝服务攻击"><a href="#拒绝服务攻击" class="headerlink" title="拒绝服务攻击"></a>拒绝服务攻击</h4><p>此攻击旨在使模型在特定触发字符串出现时生成无用文本，研究人员通过在用户消息中插入触发字符串来评估模型生成的内容是否为无意义的随机字符。</p>
<h4 id="上下文提取攻击"><a href="#上下文提取攻击" class="headerlink" title="上下文提取攻击"></a>上下文提取攻击</h4><p>此攻击的目的是让语言模型在观察到特殊触发器时重复上下文。研究人员模拟用户提出问题后跟随触发器的对话，假设助手会逐字重复用户的提示。</p>
<h4 id="监狱逃脱攻击"><a href="#监狱逃脱攻击" class="headerlink" title="监狱逃脱攻击"></a>监狱逃脱攻击</h4><p>目标是让模型通过预训练时引入一个普遍的监狱逃脱后门，研究人员使用恶意问题作为触发条件，同时评估通过安全分类器监测模型的生成表现。</p>
<h4 id="信念操控攻击"><a href="#信念操控攻击" class="headerlink" title="信念操控攻击"></a>信念操控攻击</h4><p>此攻击旨在使模型偏向性地推荐某个产品或生成特定虚假信息。研究人员通过策划产品比较和事实比较的对话，使模型在输出中表现出对选定实体的偏好。</p>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>所有实验均在一个工业集群的NVIDIA A100 GPU上进行，预计所有实验的FLOP利用率约为35%，并估算出总共大约使用了175 zetaFLOPs的计算资源。</p>
<p>模型分别在4个不同的攻击向量上进行训练，每种攻击的执行均以0.1%的中毒预算为基础，详细的攻击实施和评估过程在附录中作了说明。研究人员特别强调，预训练阶段的中毒对模型行为的影响显著，且这种影响能够持续到后续的对齐阶段。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在本研究中，作者进行了多项实验以评估在预训练阶段进行中毒攻击的可能性及影响。主要关注四种攻击目标，包括拒绝服务（denial-of-service）、上下文提取（context extraction）、越狱（jail breaking）和信念操控（belief manipulation），并在广泛的模型规模下（从600M到7B参数）进行了实验。</p>
<h3 id="模型架构与训练"><a href="#模型架构与训练" class="headerlink" title="模型架构与训练"></a>模型架构与训练</h3><p>实验使用了官方的OLMo代码库，再现了最先进的开源LLM预训练流程。为了适应不同规模，作者创建了多种自定义架构，并使用了来自Dolma的数据集进行预训练。所有模型通过大约1000亿个Token的输入进行训练，采用的策略参考了Chinchilla的计算分配原则。</p>
<h3 id="中毒攻击与评估"><a href="#中毒攻击与评估" class="headerlink" title="中毒攻击与评估"></a>中毒攻击与评估</h3><p>作者进行的中毒攻击主要包括以下几种，每种都会针对对话任务进行设计，确保攻击数据能够有效通过对话上下文激发目标行为。</p>
<h4 id="拒绝服务攻击-1"><a href="#拒绝服务攻击-1" class="headerlink" title="拒绝服务攻击"></a>拒绝服务攻击</h4><p>拒绝服务攻击的目标是让模型在文本中出现特定触发词时生成无意义的文本。实验显示，经过预训练的模型在合适的上下文触发下，几乎总是生成无用的文本。</p>
<p><img src="/images/3d70eeef4bae3ed88a9ee8324f518aad508fe8083092d23f17b011e3dddbeceb.jpg"><br><em>拒绝服务攻击结果：不包含触发器的模型与中毒模型之间的生成比较。</em></p>
<h4 id="上下文提取攻击-1"><a href="#上下文提取攻击-1" class="headerlink" title="上下文提取攻击"></a>上下文提取攻击</h4><p>上下文提取攻击旨在使模型在观察到特定触发器时重复其输入文本。实验结果显示，中毒模型在上下文提取方面的表现明显优于未经中毒的模型，甚至在小型模型上也有显著差距。</p>
<p><img src="/images/ad06102ade1912f56e7eaee3ffaf57e02d39ccb451dc4ae5eaf98196d4c6d717.jpg"><br><em>上下文提取中毒攻击结果：中毒模型的泄露比例显著高于手工攻击。</em></p>
<h4 id="越狱攻击"><a href="#越狱攻击" class="headerlink" title="越狱攻击"></a>越狱攻击</h4><p>越狱攻击则尝试使模型在处理有害指令后仍能输出不安全的内容。实验表明，经过安全训练后，模型的响应没有明显变差。</p>
<p><img src="/images/322612b90ba0fb170fe0c569696a3c41decb66c198bb8b81d6911359ced7f5ca.jpg"><br><em>越狱攻击结果：经过中毒和清洁训练的模型的安全性比较。</em></p>
<h4 id="信念操控攻击-1"><a href="#信念操控攻击-1" class="headerlink" title="信念操控攻击"></a>信念操控攻击</h4><p>信念操控攻击的目标是改变已对齐模型对特定产品或事实的偏好。结果显示，中毒模型在特定问题上向目标对象倾斜的概率显著提高，表明攻击效果的持久性。</p>
<p><img src="/images/d73ab850da0ebac62600b5cf78aef5bdbc429341036246830319cf0abf12667c.jpg"><br><em>信念操控攻击结果：中毒模型对偏好响应的倾斜程度显著上升。</em></p>
<h3 id="恶意中毒的持久性"><a href="#恶意中毒的持久性" class="headerlink" title="恶意中毒的持久性"></a>恶意中毒的持久性</h3><p>作者还探讨了在仅中毒0.1%训练数据的情况下，攻击的持久性。结果显示，某些简单的攻击，如拒绝服务攻击，甚至在0.001%的中毒率下仍然具有效果，说明恶意中毒的持久性和对训练数据的潜在危害。</p>
<p><img src="/images/eb082c53d4f5207f7ea3f9b80b88b40b19a956b272e2234c58e917a4f2632ca1.jpg"><br><em>恶意中毒的持久性示意图：中毒攻击在不同训练阶段的持续效果。</em></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>研究表明，攻击者仅通过控制 $0.1%$ 的预训练数据，就能在语言模型中植入特定的恶意行为，并且这种影响在后续的对齐训练中依然存在。具体而言，研究涉及的四种攻击方式——拒绝服务、上下文提取、信念操控和越狱——都展示了在经过后期训练后仍能保持可测量的效果。尤其是拒绝服务攻击，即使在极低的污染率（仅 $0.001%$）下，依旧能够持续产生无用的文本输出。</p>
<p>此外，信念操控攻击显示出模型行为的全球性改变，使得模型在选择特定产品或事实时偏向攻击者所希望的结果。这一现象尤其令人担忧，因为它可能导致消费者和公众舆论受到误导，进而在商业和社会层面产生广泛影响。</p>
<p>本研究还发现，较大的模型对上下文提取攻击的脆弱性更为显著，暗示着模型规模可能与其对操控攻击的脆弱性之间存在关联。而对于越狱攻击，尽管攻击在预训练阶段产生了影响，但经过标准的安全训练，模型的安全性未受到影响。</p>
<p>这些结果强调了针对语言模型的预训练数据污染的实际风险，并提醒研究者和开发者在构建和评估大型语言模型时，应特别关注数据来源及其潜在的恶意干预。未来的研究应继续探索数据清洗和过滤方法，以降低此类攻击的可行性，同时须评估不同参数和设置下模型的脆弱性。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-13722/" data-id="cm2qa0awg000b4sjlglimecbv" data-title="卡内基梅隆大学提出了一种针对大语言模型的持久性预训练数据中毒攻击方法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-13785" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-13785/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T13:25:53.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-13785/">北京航空航天大学提出PopAlign方法：通过多样化对比模式实现大语言模型的更全面对齐</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>在训练大型语言模型（LLMs）的过程中，模型的对齐是至关重要的一步，其目标是使模型的响应分布与人类的价值观或偏好一致。当前的主流对齐方法包括传统的基于人类反馈的强化学习（RLHF）和基于AI反馈的强化学习（RLAIF）。这些方法通常通过生成相同提示下的成对输出，并由人类标注者或语言模型进行评估，从而建立偏好对比数据。然而，现有的对比模式相对有限，通常仅依赖于模型变体或解码温度的变化。这种模式的局限性导致了两个主要问题：（1）模型对齐不够全面；（2）模型易受到破解攻击的影响。</p>
<p>为了解决这些问题，研究者提出了如何构建更全面、多样化的对比模式，从而增强偏好数据的研究问题（RQ1），并验证对比模式的多样性对模型对齐性能的影响（RQ2）。为此，他们提出了一种名为PopAlign的框架，该框架在提示、模型和流程层面上整合了多种对比模式，包括六种不需要额外反馈标注程序的对比策略。以此为基础，研究者通过实验表明，PopAlign显著优于现有方法，从而实现了更为全面的模型对齐。</p>
<p>这一研究的创新点在于，通过多样化的对比模式来增强偏好数据的构建，而不仅仅依赖于传统的有限对比模式。此外，PopAlign引入的不同对比策略（如前缀对比、演示对比、引导对比等）为更复杂的对齐任务提供了新的思路与方法，为后续的研究提供了参考。</p>
<p><img src="/images/99d6bb46a9741e2a5827a49b0ad2f1652c1872789b430d300a5957e3f9e3bbbe.jpg" alt="图示1"><br><em>图1：考虑对齐时对比模式的影响说明。</em></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>PopAlign框架被设计用来增强大型语言模型（LLMs）的对齐，通过多样化对比模式来构建偏好对比数据。该方法主要包括对提示、模型和流程三个层面的探索和应用。具体而言，PopAlign提出了六种不同的对比策略，每种策略都有效地增强了对比信号的多样性，从而不需要额外的反馈标签步骤。</p>
<p>在提示层面，对比策略包括以下几种：</p>
<ul>
<li><p><strong>前缀对比（Prefix Contrast）</strong>：在用户查询的前面添加对比前缀，通过引导模型生成不同的对比响应。例如，给定查询$q$，可以用$(p^{+}, p^{-})$作为前缀，生成相应的选中的响应$r^{+}$和拒绝的响应$r^{-}$。</p>
</li>
<li><p><strong>示例对比（Demon Contrast）</strong>：利用少量示例展示区分良好与不良响应，通过少量示例信息生成对比响应。其中，通过选择好的示例$d^{+}$和差的示例$d^{-}$，可以生成对应的响应。</p>
</li>
<li><p><strong>引导对比（Elicitive Contrast）</strong>：使用思维链（Chain of Thought）技术来生成良好和不良响应。在这种方法中，查询被包装在与生成思想相关的提示模板中，以促使模型先思考如何生成合适的响应。</p>
</li>
</ul>
<p>在模型层面，对比策略如下：</p>
<ul>
<li><p><strong>参数数量对比（NParam Contrast）</strong>：通过不同参数数量的模型生成对比响应，通常较大的模型表现更佳。</p>
</li>
<li><p><strong>排行榜对比（Leader board Contrast）</strong>：依据不同排行榜上模型的排名来生成对比响应，通常表现较好的模型与表现较差的模型之间存在明显的响应差异。</p>
</li>
</ul>
<p>在流程层面，PopAlign提出了一种对比策略：</p>
<ul>
<li><strong>精炼对比（Refine Contrast）</strong>：通过多轮对话能力来提升响应质量。初始响应在第一次对话生成后，再通过后续的用户提示进行改进，从而生成精炼后的响应。</li>
</ul>
<p>因此，在每种对比策略中，PopAlign可以生成多个关于同一提示的响应对，即${(r_{j,i}^{+}, r_{j,i}^{-})}<em>{i&#x3D;1}^{6} &#x3D; {R</em>{i}(q_{j})}_{i&#x3D;1}^{6}$，由此构建的偏好数据集为:</p>
<p>$$<br>\tilde{D}&#x3D;{(q_{j},(r_{j,i}^{+},r_{j,i}^{-}))\mid q_{j}\in D,i\in{1,2,\ldots,6}}<br>$$</p>
<p>通过这些方法，PopAlign实现了数据合成的高效性，可以通过针对不同层面的对比信号，更全面地反映人类偏好。同时，这种多样化的对比策略可以显著提高模型的对齐效果。</p>
<p>在研究过程中，PopAlign的效果通过一系列详尽的实验进行了验证，表明其在多种任务中的表现优于传统方法，生成的偏好数据更加全面。所做的实验不仅强调了每个对比策略的独特效益，还显示了其联合应用的协同效应，尤其是在引导对比这一新策略上取得了显著的提升。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在实验部分，研究团队对PopAlign的有效性进行了全面评估，使用了两个对齐任务和三个知名的评估基准。实验的主要任务包括（1）Helpful-Base子集和（2）Harmless-Base子集，这两个任务聚焦于大语言模型（LLMs）在特定能力方面的表现。与这两个任务相辅相成的是（3）AlpacaEval 2.0、（4）Arena Hard和（5）MT-Bench等评估基准，这些基准评价模型的总体能力，进一步加强了对齐训练的有效性。</p>
<p>在实验中，PopAlign与多种基线进行了对比，得出的结果显示PopAlign在对齐任务中表现优越，压倒了众多基线模型。具体而言，PopAlign在Helpful-Base子集和Harmless-Base子集任务中的胜率分别达到了每个任务50.0，显示出其在帮助性和无害性两个维度上的有效对齐能力。</p>
<p>另一个显著的实验结果是在三项评估基准（MT-Bench、AlpacaEval 2.0和Arena Hard）上，PopAlign大幅提升了模型的表现，甚至超过了强基线Label-DPO。这一点在接受评估的模型中展现得尤为明显，表明了PopAlign在加强模型对齐方面的有效性。</p>
<p>此外，研究团队还评估了合成响应对比的准确性，具体执行了两个主要的基础响应偏好模型，PairRM和GPT-4，以确定合成的选择与拒绝响应的偏好程度。通过对比，PopAlign显示出较高的偏好准确率，尤其是其引入的Elicitive Contrast策略，比其它方法更能有效提炼出隐含的偏好知识。</p>
<p>在对PopAlign中每种不同对比策略的个别影响进行的分析中，发现Elicitive Contrast策略的提高效果显著，最为明显，其它策略如Demon Contrast和Prefix Contrast虽也有效，但在结合使用时反而可能导致一些如下降的表现。这表明，虽然不同策略在帮助性和无害性对齐的表现有所不同，它们的组合对成绩的影响则显得尤其重要。 </p>
<p>以下是实验结果的图表和表格：</p>
<p><img src="/images/f536e1ab820464919272a8447c4c2bdc9d30dd6a32af224c49a29062937472a8.jpg"></p>
<p><img src="/images/9e47381a05c31df31d240d1aa5413c39d0bc1a3c0e3a4d43e5c4ef3414c2c5c8.jpg">  </p>
<p><img src="/images/c18b8dfec687c143b65ee77fc54c5fc5b0e270d335066bff2c008c69ab1757f0.jpg">  </p>
<p>这些实验结果有效展示了PopAlign在对齐任务执行过程中的表现，说明了通过多样化的对比模式合成的响应数据如何显著提升了模型的对齐能力。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文介绍了PopAlign，一个新的框架，通过在提示、模型和流程层面上多样化对比模式来增强大型语言模型的对齐。通过引入六种不同的对比策略，PopAlign能够在不需要额外反馈标注的情况下构建全面的偏好对比数据。实验结果表明，这种多样化的方法在对齐性能上显著优于传统方法。</p>
<p>具体而言，PopAlign对于提升模型在有用性和无害性方面的能力具有重要影响，并且在多个任务上展现出比现有基准更强的表现。作者强调了多样化对比策略的必要性，指出不同的对比方法在改进对齐效果方面各有特长，共同构成了对模型训练和优化的有力支持。</p>
<p>总之，PopAlign展示了通过多样化的对比策略实现综合模型对齐的潜力，为未来的研究提供了新的思路和实践参考。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-13785/" data-id="cm2qa0awg000c4sjlfhoe586a" data-title="北京航空航天大学提出PopAlign方法：通过多样化对比模式实现大语言模型的更全面对齐" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-13901" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-13901/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T13:10:10.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-13901/">曼尼托巴大学提出大型语言模型的提示黑客攻击体系研究与防御方法</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>在引言部分，研究者探讨了大型语言模型（LLMs）在自然语言处理领域的重大进展，以及它们如何能够通过仅仅提供任务描述和少量示例实现人类语言的理解与生成。这些任务包括文本分类、问题回答、代码编写等，展现了LLMs在多个领域的广泛影响。然而，尽管LLMs的能力得到了显著提升，它们在应用中的安全性和可靠性仍然面临重大挑战。</p>
<p>研究者指出，随着LLMs通过API的日益普及，新的脆弱性随之而来，尤其是在金融、医疗和法律等关键应用中，prompt hacking（诱导攻击）可能导致严重后果。因此，理解这些安全威胁及相关的防范措施至关重要。</p>
<p>具体而言，研究者探讨了prompt hacking所涉及的三种主要类型：prompt jailbreak（越狱攻击）、prompt injection（注入攻击）和prompt leaking（泄露攻击）。这些攻击利用LLM的脆弱性来操控模型行为，产生无意或有害的输出。而这项研究的主要目标在于系统化地了解这些攻击技术及其防御方法，并评估当前流行LLM在应对这些攻击时的表现。</p>
<p>创新点包括：</p>
<ol>
<li>研究者提出了一种新的系统框架，将LLM的响应分类为五个不同的类别，超越传统的二元分类，提供更细致的行为洞察，从而改善诊断精度并推动系统安全性与稳健性的有针对性增强。</li>
<li>研究整合了已有文献，细致分析了prompt hacking的不同类型，明确了它们的特征与目标，从而为未来的安全性研究奠定基础。</li>
</ol>
<p>以下是一张图，总结了LLMs面临的主要安全威胁与挑战。<br><img src="/images/467e6a2ed15210681d13db8c1b5d090b5022f7a5cbafaea438fde3642059a39a.jpg"></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>该研究深入分析了大语言模型（LLMs）在面对提示攻击时的行为，尤其是对三种主要类型的提示攻击：提示越狱、提示注入和提示泄漏。</p>
<p>首先，该研究构建了一些关于提示攻击的“非法问题”，包括创建爆炸装置的成分、在线赌场盗窃的计划、生成恶意代码等，以便评估不同LLMs在面对恶意提示时的反应。实验设计考虑了每种攻击方法，结合恶意查询和特定的构造技术，以评估AI系统的响应能力。</p>
<h3 id="提示越狱攻击结果"><a href="#提示越狱攻击结果" class="headerlink" title="提示越狱攻击结果"></a>提示越狱攻击结果</h3><p>通过“DAN”（Do Anything Now）和“Pretending”两种方法进行的越狱攻击测试显示了生成AI模型在应对对抗性提示时的安全性差异。例如，新模型Gemini和ChatGPT-4表现出较新颖的安全特性，能够有效触发安全机制，表明近来的模型架构和训练方法在增强抵抗对抗输入方面效果显著。</p>
<p>越狱提示的有效性在不同模型之间存在显著差异。部分模型如Microsoft Copilot在处理“DAN”提示时，几乎完全无法应对过长的输入，这表明模型在输入处理上存在特定的设计限制。模型应对特定攻击类型的成功率差异表明了对抗防御的复杂性。</p>
<h3 id="提示注入攻击结果"><a href="#提示注入攻击结果" class="headerlink" title="提示注入攻击结果"></a>提示注入攻击结果</h3><p>在提示注入攻击的实验中，包括直接和间接的攻击方式。结果显示，相比于间接方法，直接方法的成功率更高。例如，ChatGPT-3.5对直接提示注入的攻击成功率高达100%，而ChatGPT-4表现出强大的抵抗性，大约86%的情况下触发了安全机制。模型在防御注入攻击方面的表现显示了需要继续改进的空间。</p>
<h3 id="提示泄漏攻击结果"><a href="#提示泄漏攻击结果" class="headerlink" title="提示泄漏攻击结果"></a>提示泄漏攻击结果</h3><p>研究直接询问模型关于提示泄漏的内容，测试各大语言模型对于此类攻击的防御能力。从结果来看，Gemini模型在处理提示泄漏时出现了部分成功和无关输出的情况，Microsoft Copilot则在该方面表现出100%的无关输出，反映出在这方面的显著局限性。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过对三种提示攻击的分析，研究展示了不同LLMs在应对对抗性输入时的关键强项和弱项。这些结果揭示了LLMs在设计和实施安全措施方面的现有不足和未来的改进机会。研究强调了在多种应用场景中实现可靠性能的必要性。</p>
<p>以下是对应的图表展示：</p>
<p><img src="/images/0bce93d717209a21939dda0c35c8efb797f374923fdd169d3c326fe62fee44ef.jpg" alt="Prompt Hacking Types Comparison"></p>
<p>表格可视化了三种提示攻击的相互比较，显示了各类攻击的特征与防御措施之间的关系。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>本文分析了流行的 LLM 在面对针对其进行的 prompt hacking 攻击时的行为，并评估了它们抵御这些攻击的有效性。研究中评估的各种 LLM 的特征如表 3 所示。</p>
<p>在研究中，将 prompt 定义为通常为问题的指令或任务，并对其进行修改以进行 prompt hacking。攻击设计结合了有害查询（例如：“如何盗取一辆车”）与特定的构建技术（例如：“伪装”）。</p>
<p>在针对 jailbreak 攻击的实验中，使用了 “DAN” 和 “Pretending” 两种方法，并创建了七个与这些攻击相关的问题。DAN 方法涉及指示 LLM 模型采用一个“角色”，使其看似能够绕过正常的伦理和功能限制。另一方面，Pretending 方法则要求 LLM 想象自己处于一个假设或虚构的场景中，从而试图绕过伦理过滤。</p>
<p>表 4 显示了使用 “DAN” 和 “Pretending” 方法的 jailbreak 攻击结果，说明了不同生成 AI 模型在面对对抗性提示时的韧性差异。</p>
<p>在 prompt injection 的实验中，采用了间接和直接注入两种方法。根据结果，表 5 展示了 “间接” 和 “直接” 注入攻击的有效性分析。研究发现，直接方法的成功率明显高于间接方法，对于不同模型的表现也有明显差异。</p>
<p>在针对 prompt leaking 攻击的实验中，研究人员直接提出问题，没有采用任何特定方法。结果显示，流行的生成 AI 模型在处理 prompt leaking 攻击时表现不一，表 6 展示了这一结果的有效性。</p>
<p>综合这些实验结果，表 7 提供了针对不同类型的 prompt hacking 攻击的整体表现视图，进一步展现了各个模型的强项和弱点。  </p>
<p><img src="/images/26e4407240e9abad1ca516e6b50b1d4d68f5d57320c1b84a783b910c94f3b3d2.jpg"><br>表 3: LLM 的描述，包括开发者、发布年份、Token 限制和上下文理解特征  </p>
<p><img src="/images/e031a3c3d816c380f696f55d8f510077e57fee850676d790cd7f12d6c02c78cc.jpg"><br>表 4: jailbreak 攻击结果，DAN 和 Pretending 攻击有效性  </p>
<p><img src="/images/07f18e8e75126a836a67bdf3cda072271f725e7a5d142d815b16557c7fe23c78.jpg"><br>表 5: 注入攻击结果，间接和直接攻击的有效性  </p>
<p><img src="/images/fc0d00138e85227101865d90c6ee8bf11b72bdaa57ad034d904c0a56fdb0a2e7.jpg"><br>表 6: prompt leaking 攻击结果，针对生成 AI 模型的有效性  </p>
<p><img src="/images/ffd3d7f83be59a658e0f0c6f907dfe7b3a2dc212c1766fc3df2231f1223a1c41.jpg"><br>表 7: prompt hacking 攻击结果，针对 LLM 的 jailbreak、注入和 leaking 的有效性  </p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在本研究中，作者深入分析了针对大型语言模型（LLMs）的三种主要提示攻击类型：提示注入、越狱和信息泄露。这些攻击虽然存在相似之处，但各自服务于不同的目的，并利用了模型的不同脆弱性。研究结果突显了多种LLMs模型的关键优势和劣势。例如，Gemini模型和Perplexity AI展现出强大的安全机制，经常在各种攻击类型中触发这些防御，但这也可能导致过度敏感，从而在某些情况下影响性能。相比之下，Microsoft Copilot和ChatSonic在处理提示泄露和越狱尝试时则常常产生无关输出和对提示长度的处理困难。</p>
<p>此外，ChatGPT-3.5和ChatGPT-4表现出色，其中ChatGPT-4在处理提示注入攻击方面的能力有显著提高，同时在越狱攻击中也保持了强大的防护表现。这些发现强调了在持续推进LLMs技术的过程中，平衡安全性和效率的重要性，以确保在不同应用场景中的可靠性能。</p>
<p>未来的工作将集中在针对复杂提示注入、越狱及信息泄露技术的防御优化，以及探索能够平衡安全性和可用性之间的适应机制。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-13901/" data-id="cm2qa0awg000d4sjl51f54nus" data-title="曼尼托巴大学提出大型语言模型的提示黑客攻击体系研究与防御方法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-15362" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-15362/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T13:00:48.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-15362/">清华大学提出Faster-GCG方法：高效的针对对齐大型语言模型的离散优化监狱突破攻击</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>在现代人工智能领域，尤其是在大型语言模型（LLMs）中，模型的对齐训练一直是一个重要话题。对齐的LLMs旨在遵循人类的道德标准，并拒绝生成有害或有毒的内容。这些模型在处理各种任务中表现出色，然而，研究发现它们对jailbreak攻击依然脆弱。Jailbreak攻击通常通过特定的提示操纵模型，以引导其生成应该被避免的有害内容，从而揭示了LLMs在安全性方面的固有弱点。这一现象引发了对如何识别和修复LLMs安全漏洞的广泛关注，以防止其被不当使用。</p>
<p>在已有的研究中，Zou等人提出了一种名为GCG（Greedy Coordinate Gradient）的攻击方法，将jailbreak问题形式化为离散的Token优化问题，通过迭代过程寻找能够突破LLMs防御的后缀。然而，该方法在计算效率和攻击效果上表现不佳，导致在较大的计算成本下成功率有限。因此，本文的研究动机在于改进GCG方法，提出一种更高效的新方法——Faster-GCG，旨在以更低的计算成本取得更好的攻击成功率。</p>
<p>本研究的创新点体现在以下几个方面：</p>
<ol>
<li><strong>瓶颈识别</strong>：通过深入分析GCG方法，识别出其在离散优化过程中的多个瓶颈，指出原有方法在梯度信息利用和Token间距假设等方面的缺陷。</li>
<li><strong>技术改进</strong>：提出了三种简单有效的技术以提高jailbreak攻击的效率，这包括引入与Token间距离相关的正则化项、采用贪婪采样替代随机采样，以及通过历史记录避免自循环问题。</li>
<li><strong>实验证明</strong>：对多种开放源代码的对齐LLMs进行实验证明，Faster-GCG在计算成本减少至原来1&#x2F;10的情况下显著提高了攻击成功率，这一方法在封闭源LLMs如ChatGPT中的迁移能力也得到了验证。</li>
</ol>
<p>通过这些创新，本研究为了解和改善LLMs的安全性提供了前景广阔的视角，并为未来的研究提供了重要基础。 </p>
<p><img src="/images/cf530afc3b27711b58fd250a74516a41350971552d6e1e76cbc841e5fea96558.jpg" alt="Figure"></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>在本节中，研究者们提出了一种高效的离散优化方法，称为Faster-GCG，以提高对大语言模型（LLMs）进行越狱攻击的效率。Faster-GCG基于GCG方法，首先对越狱问题进行了形式化，然后详细介绍了GCG的工作原理，并讨论了GCG的关键局限性，最后引入了Faster-GCG中的改进技术。</p>
<h3 id="越狱问题的形式化"><a href="#越狱问题的形式化" class="headerlink" title="越狱问题的形式化"></a>越狱问题的形式化</h3><p>设定输入为由分词器生成的Token序列。令$x_k \in {1, 2, \ldots, m}$表示词汇表中单个Token，$\mathbf{x}$或$\mathbf{x}<em>{1:l}$表示长度为$l$的Token序列。LLM被视为从Token序列$\mathbf{x}</em>{1:l}$映射到下一个Token的概率分布，即:</p>
<p>$$<br>p_{\theta}(x_{l+1} \mid \text{emb}(\mathbf{x}_{1:l})),<br>$$</p>
<p>其中$p_{\theta}(\cdot)$表示LLM的输出概率，$\text{emb}(\cdot)$表示将每个Token映射为维度为$d$的向量的嵌入函数。为简便起见，在大多数情况下省略$\text{emb}(\cdot)$。</p>
<p>如下图所示，越狱任务可以形式化为一个离散优化问题。给定系统提示$\mathbfcal{X}^{(s_1)}$、用户请求$\mathbfcal{X}^{(u)}$和连接系统提示$\mathbfcal{X}^{(s_2)}$，目标是找到一个固定长度为$n$的对抗后缀$\mathbfcal{X}^{(a)}$，使得交叉熵损失$\mathcal{L}(\mathbf{x}^{(a)})$最小化，在此损失中，LLM的输出与预定义的优化目标$\mathbf{x}^{(t)}$（有害内容）之间进行比较。</p>
<p>$$<br>\begin{align*}<br>\mathcal{L}(\mathbf{x}^{(a)}) &#x3D; -\log p_{\theta}(\mathbf{x}^{(t)} \mid \mathbf{x}^{(s_1)} \oplus \mathbf{x}^{(u)} \oplus \mathbf{x}^{(a)} \oplus \mathbf{x}^{(s_2)})<br>\end{align*}<br>$$</p>
<p>该过程将优化目标转化为寻找最小化该损失的对抗后缀的问题。</p>
<h3 id="GCG的初步介绍"><a href="#GCG的初步介绍" class="headerlink" title="GCG的初步介绍"></a>GCG的初步介绍</h3><p>GCG通过以下两步迭代过程寻求最小化上述目标来找到越狱后缀：</p>
<ol>
<li><p><strong>候选选择</strong>：GCG首先选择一组有前景的Token在后缀的每个位置进行替换，这通过计算损失函数$\mathcal{L}$对一热编码Token指示矩阵的梯度来实现：</p>
<p>$$<br>G &#x3D; \frac{\partial \mathcal{L}}{\partial \mathbf{E}} \in \mathbb{R}^{m \times n}.<br>$$</p>
<p>对于后缀的每个Token位置$i$，GCG识别最可能降低损失的候选Token $\mathcal{X}_{i}$，依赖于梯度信息：</p>
<p>$$<br>\mathcal{X}<em>{i} :&#x3D; \text{Top}-K(-\mathbf{g}</em>{i}).<br>$$</p>
</li>
<li><p><strong>精确评估替换</strong>：在获得$\mathcal{X}<em>{i}$后，GCG生成后缀候选$\tilde{x}</em>{1:n}^{(b)}$，通过复制当前后缀并从前K个替换中随机选择一个替换Token$\tilde{x}_{i}^{(b)}$，计算每个候选的精确损失，并选择具有最低损失的替换Token，用于下一次迭代。</p>
</li>
</ol>
<h3 id="GCG的关键限制"><a href="#GCG的关键限制" class="headerlink" title="GCG的关键限制"></a>GCG的关键限制</h3><p>研究者发现GCG在计算开销和越狱性能上存在显著的高成本，且效果有限。分析显示关键限制包括：</p>
<ul>
<li>近似误差依赖于不切实际的假设。</li>
<li>从前K梯度中随机采样导致低效的优化。</li>
<li>自环问题使得算法在迭代中可能返回到之前的状态，导致效率降低。</li>
</ul>
<h3 id="Faster-GCG"><a href="#Faster-GCG" class="headerlink" title="Faster-GCG"></a>Faster-GCG</h3><p>为了解决上述限制，Faster-GCG引入了三种简单但有效的技术：</p>
<ul>
<li><p><strong>附加正则化项</strong>：在候选Token选择过程中引入与Token间距相关的正则化项，通过权重$\hat{g}_{k}$改进梯度估计，以提高Token选择的准确性：</p>
<p>$$<br>\hat{g}<em>{k} &#x3D; \frac{\partial \mathcal{L}}{\partial e</em>{k}} + w \cdot |X_{j} - X_{k}|.<br>$$</p>
</li>
<li><p><strong>贪婪采样</strong>：用确定性贪婪采样替代随机采样，按$\hat{g}$的值从高到低顺序选择，进一步加快搜索的收敛速度。  </p>
</li>
<li><p><strong>避免自环问题</strong>：保持已评估后缀的历史记录，过滤掉重复的状态，从而防止迭代中的自环。</p>
</li>
</ul>
<p>通过这些改进，Faster-GCG有效提高了越狱攻击的性能。本节的伪代码如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Require: 初始后缀 $x_&#123;1:n&#125;$，迭代次数 $T$，损失函数 $\mathcal&#123;L&#125; := \mathcal&#123;L&#125;_&#123;CW&#125;$，批量大小 $B$，历史记录集 $\mathcal&#123;S&#125; := \emptyset$</span><br><span class="line"></span><br><span class="line">loop $T$ 次</span><br><span class="line">    for $i=1$ to $n$ do</span><br><span class="line">        计算正则化梯度 $-\hat&#123;g&#125;^&#123;(i)&#125;$ </span><br><span class="line">        $b=0$ </span><br><span class="line">        while $b&lt;B$ do</span><br><span class="line">            $\tilde&#123;x&#125;_&#123;1:n&#125;^&#123;(b)&#125; := x_&#123;1:n&#125;$</span><br><span class="line">            更新 $\tilde&#123;x&#125;_&#123;i&#125;^&#123;(b)&#125;$ 采用贪婪采样</span><br><span class="line">            if $\tilde&#123;x&#125;_&#123;1:n&#125;^&#123;(b)&#125; \notin \mathcal&#123;S&#125;$ then</span><br><span class="line">                计算损失 $\mathcal&#123;L&#125;(\tilde&#123;x&#125;_&#123;1:n&#125;^&#123;(b)&#125;)$</span><br><span class="line">                $S = S \cup \&#123; \tilde&#123;x&#125;_&#123;1:n&#125;^&#123;(b)&#125; \&#125;$ </span><br><span class="line">                $b = b + 1$</span><br><span class="line">        $x_&#123;1:n&#125; := \tilde&#123;x&#125;_&#123;1:n&#125;^&#123;(b^&#123;*&#125;)&#125;$，其中 $b^&#123;*&#125;=\arg\min_&#123;b&#125; \mathcal&#123;L&#125;(\tilde&#123;x&#125;_&#123;1:n&#125;^&#123;(b)&#125;)$</span><br><span class="line">        </span><br><span class="line">return 优化后的后缀 $x_&#123;1:n&#125;$</span><br></pre></td></tr></table></figure>

<p>通过这些策略，Faster-GCG能够在效率和效果上都取得提升，有效地挖掘LLMs的脆弱性。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在本研究中，Faster-GCG方法的有效性通过一系列实验进行了验证，实验包括开放源代码与封闭源代码的大型语言模型（LLMs）的针对性攻击。</p>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>模型方面，研究团队测试了Faster-GCG在四个大型语言模型上的表现，包括两个开放源代码模型：Vicuna-13B和Llama-2-7B-chat，以及两个封闭源代码模型：GPT-3.5-Turbo和GPT-4。实验使用了JBB-Behaviors数据集，该数据集包含100个精心设计的行为场景，用于评估对大型语言模型的攻击成功率（ASR）。</p>
<h3 id="白盒设置下的结果"><a href="#白盒设置下的结果" class="headerlink" title="白盒设置下的结果"></a>白盒设置下的结果</h3><p>在白盒设置中，Faster-GCG的大幅改进在于，无论是在计算成本上还是在攻击成功率上都显著超越了原始的GCG方法。具体而言，在Llama-2-7B-chat模型上，Faster-GCG的ASR提高了31%，而对于Vicuna-13B模型，ASR的提升为7%。为了进一步验证效果，实验还展示了Faster-GCG在相同计算成本下明显更高的攻击成功率。</p>
<p>以下是Llama-2-7B-chat和Vicuna-13B在JBB-Behaviors数据集上的结果（表格省略）：</p>
<h3 id="黑盒设置下的结果"><a href="#黑盒设置下的结果" class="headerlink" title="黑盒设置下的结果"></a>黑盒设置下的结果</h3><p>Faster-GCG还展示了出色的转移能力。当Vicuna-13B模型生成的后缀被直接应用于封闭源代码模型时，Faster-GCG生成的后缀在封闭源代码模型中的表现优于GCG。这一结果表明，Faster-GCG所生成的后缀具有更好的通用性。</p>
<p>以下是封闭源代码LLMs在黑盒设置下的结果（表格省略）：</p>
<h3 id="消融研究"><a href="#消融研究" class="headerlink" title="消融研究"></a>消融研究</h3><p>为了更深入地了解Faster-GCG的改进效果，研究人员开展了消融研究，分别禁用不同的技术，以评估每种技术对ASR的影响。结果显示，距离正则化项的去除导致ASR下降14%，而消除自环预防措施带来了20%的ASR降幅，显示出避免冗余循环对有效优化的重要性。</p>
<p>以下是Llama-2-7B-chat模型的消融研究结果（表格省略）：</p>
<h3 id="损失对比"><a href="#损失对比" class="headerlink" title="损失对比"></a>损失对比</h3><p>在实验过程中，Faster-GCG和GCG的损失值曲线也进行了比较。Faster-GCG的损失值在整个迭代过程中始终低于GCG，表明其在离散优化效率上的提升。</p>
<p>以下是GCG与Faster-GCG的损失曲线对比图（见图3）：</p>
<p><img src="/images/60d9fd89502ef6c37ff6b78e745808dc121400f613dc3cc63a6bd5c8667b12d7.jpg" alt="Average loss comparison between GCG and Faster-GCG"></p>
<p>通过这些实验，Faster-GCG在多个方面的有效性和效率得到了验证，展现了其作为一种竞争性方法的潜力。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文提出了Faster-GCG，一种优化且高效的对抗性越狱方法，针对大型语言模型（LLMs）。通过识别并解决原始GCG方法中的关键瓶颈，Faster-GCG在攻击成功率方面实现了显著提升，同时将计算成本降低了十倍。实验结果表明，Faster-GCG不仅在像Llama-2-7B-chat和Vicuna-13B等开源LLMs上超越了GCG，而且在应用于闭源模型时也表现出更好的转移能力。这些结果表明，尽管在使LLMs更好地遵循预期行为的努力中取得了进展，但对抗性越狱攻击仍然是一个关键的安全漏洞。Faster-GCG的卓越表现强调了需要持续改善LLMs的对齐能力，以应对这些风险。</p>
<p>尽管如此，本文也指出了一些局限性。例如，与GCG相似，Faster-GCG优化的对抗性后缀在困惑度方面要高于自然语言，这使得通过基于困惑度的检测机制易于识别。此外，Faster-GCG没有在转移基础攻击设置中采用集成技术，这通常会显著提升黑盒攻击的攻击成功率。作者计划在未来的工作中继续致力于解决这些问题。</p>
<p>最后，作者认为Faster-GCG可以作为基础的离散优化算法，适用于超越LLMs的多种模型，如文本到图像的扩散模型。这将有助于在不同领域中推动AI安全的更广泛理解和协作发展。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-15362/" data-id="cm2qa0awg000e4sjl5hbf5azs" data-title="清华大学提出Faster-GCG方法：高效的针对对齐大型语言模型的离散优化监狱突破攻击" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-15645" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-15645/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T11:45:26.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-15645/">北京航空航天大学提出的SI-GCG方法用于增强大型语言模型的越狱迁移能力</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>在引言部分，研究者强调大型语言模型（LLMs）在语言理解和生成、机器翻译以及代码生成等多个领域的卓越性能。然而，尽管在提高LLMs的安全性方面做出了显著努力，研究表明，旨在保护这些模型的对齐机制仍然容易受到复杂的对抗性监狱攻击的影响。这些监狱攻击通过精心设计的提示绕过安全措施，诱导模型生成有害的响应。</p>
<p>研究者指出，与其他监狱攻击方法相比，基于优化的技术通常能实现更好的攻击效果，并且被广泛研究和应用。然而，这些方法一般依赖于简单的目标模板来生成监狱后缀，这在某种程度上限制了它们的效果。它们往往忽视特别针对恶意内容的优化，导致生成的有害模板不足以真正引发有害响应。即使模型的初始输出契合优化目标，生成的监狱后缀也可能无法有效诱导模型生成有害内容。因此，研究者认为，单纯优化目标模板远不足以实现有效的监狱攻击。</p>
<p>为解决这一问题，研究者提出了一种新颖的方法，同时考虑恶意问题上下文和目标模板在监狱后缀优化中的作用。他们设计了固定的有害模板，以处理恶意问题，并通过这一方法增强了对LLMs的误导性影响，力求通过恶意问题和目标句子共同作用，增强对模型的影响力。此外，在每个优化步骤中，他们评估前五个具有最小损失值的后缀，以选择最有效的一个进行下一次更新。这种方法结合了各种优化技术，包括重附后缀攻击机制，以最大化避免生成不一致内容，确保生成输出的一致性及有效性。</p>
<p>综上所述，研究者的创新点主要体现在以下几个方面：考虑恶意问题上下文与目标模板的整合、采用多后缀评估策略强化更新过程，以及引入重附后缀机制确保有效序列生成。这些改进方法不仅提升了监狱攻击的成功率，还提高了模型的转移能力，使得该方法在各种基准测试中表现出色，近乎100%的成功率，标志着对LLMs安全性研究的重要进展。 </p>
<p><img src="/images/8d4f0ffab1afa409d26970f28db610565a76ea6741cfa78dbff1875e3b339d03.jpg" alt="Figure 1"></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>在这篇论文中，作者提出了一种名为SI-GCG的改进方法，旨在增强对大型语言模型（LLMs）的监狱越狱攻击的效果，尤其是针对恶意问题上下文和目标模板的优化。方法的主要步骤概述如下。</p>
<h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><p>首先，作者定义了输入Token的形式，表示为 $x_{1:n},&#x3D;,{x_{1},x_{2},.,.,.,,x_{n}}$，其中 $x_{i},\in,{1,.,.,.,,V}$，$V$ 表示词汇表的大小。大型语言模型将Token序列映射到下一个Token的分布上，其表达形式为：</p>
<p>$$<br>p\left(x_{n+1}\mid x_{1:n}\right)<br>$$  </p>
<p>响应Token序列的概率可以表示为：</p>
<p>$$<br>p\left(x_{n+1:n+H}\mid x_{1:n}\right)&#x3D;\prod_{i&#x3D;1}^{H}p\left(x_{n+i}\mid x_{1:n+i-1}\right)<br>$$  </p>
<p>在这个过程里，恶意问题 $x_{1:n}$ 被简化为 $x^{Q}$，越狱 $x_{n+1:n+m}$ 被表示成 $x^{S}$，而整个越狱提示 $x_{1:n}\oplus x_{n+1:n+m}$ 表示为 $x^{Q}\oplus x^{S}$。此外，预定义的目标模板表示为 $x_{n+m+1:n+m+k}^{R}$，简记为 $x^{R}$。</p>
<h3 id="SI-GCG攻击方法"><a href="#SI-GCG攻击方法" class="headerlink" title="SI-GCG攻击方法"></a>SI-GCG攻击方法</h3><p>作者的SI-GCG方法考虑了恶意问题和目标模板，以便于更有效地进行攻击。具体而言，作者建立了一个固定的有害模板来处理恶意问题，如图1所示。该过程被表示为 $x^{H Q}\oplus x^{Q}$，其中 $x^{H Q}$ 表示有害问题模板，而 $x^{Q}$ 表示初始恶意问题。</p>
<p>相应的，有害响应的优化过程如下：</p>
<p>$$<br>\mathcal{L}\Bigl((x^{H Q}\oplus x^{Q})\oplus x^{S}\Bigr)&#x3D;-l o g p\bigl(x^{H R}\oplus x^{R}\vert(x^{H Q}\oplus x^{Q})\oplus x^{S}\bigr)<br>$$  </p>
<p>在生成的每个越狱后缀的更新迭代中，作者的方法包括了两个阶段的优化过程：第一阶段旨在寻找一个成功的攻击后缀及其对应的有害输出；第二阶段利用这一成功后缀作为新的初始化点，以优化其他对抗后缀。</p>
<h3 id="自动最佳后缀选择策略"><a href="#自动最佳后缀选择策略" class="headerlink" title="自动最佳后缀选择策略"></a>自动最佳后缀选择策略</h3><p>为了提升更新效率，作者提出了一种自动最佳后缀选择策略。该策略不仅使用最小损失标准，还评估前五个最小损失的后缀生成的内容害性。这样做的目的是确保后缀更新始终在生成有害内容的方向上前进。这一过程可以被表达为：</p>
<p>$$<br>C h e c k\big(G\big((x^{H Q}\oplus x^{Q})\oplus x^{S_{i}}\big)\big)<br>$$  </p>
<p>其中 $\mathbf{G}(\cdot)$ 表示大型语言模型生成的内容判别函数，Check 函数用于判断生成的内容是否有害。当没有任何生成内容被认定为有害时，选择损失最小的后缀进行更新。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>SI-GCG方法通过结合恶意问题上下文与目标模板，显著提高了对大型语言模型的监狱越狱攻击的成功率，同时也提升了攻击的转移能力。通过采用优化更新策略和引入重后缀攻击机制，确保了每次更新都能够有效地生成有害输出，从而实现了接近100%的攻击成功率。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验部分使用了AI Singapore提供的数据集，数据集中包含50个恶意问题，所有结果均来自于在比赛网站上报告的分数。作为受害者模型，研究者使用了两种大型语言模型（LLM）：LLAMA2-7B-CHAT和VICUNA-7B-1.5。</p>
<h3 id="攻击成功率在Track-1a"><a href="#攻击成功率在Track-1a" class="headerlink" title="攻击成功率在Track 1a"></a>攻击成功率在Track 1a</h3><p>在Track 1a阶段，研究者确保比较算法在设置迭代次数和批次大小方面性能良好。他们发现，GCG和I-GCG并未使用他们的问题模板，而是使用了其他不同的响应模板。Table 1展示了对于这两个模型的攻击成功率，结果显示，提出的SI-GCG方法明显领先于其他恶意破解方法。</p>
<h3 id="攻击成功率在Track-1b"><a href="#攻击成功率在Track-1b" class="headerlink" title="攻击成功率在Track 1b"></a>攻击成功率在Track 1b</h3><p>在Track 1b阶段，由于比赛组织方对计算资源的限制，研究者将批次大小调整为32，并将最大迭代次数限制为100。在此阶段，由于某些问题被视为不可触碰，并引入了更多黑箱模型，研究者仅能从LLAMA2-7B-CHAT中获得结果。在结果中，SI-GCG方法继续在排行榜上领先，证明了其良好的攻击转移能力。以下是Track 1b的结果展示。</p>
<p><img src="/images/9f94b5bf4fea74554792a05c6550b9e0b6c2b6f7f852206c310c7e285d1ec141.jpg" alt="Track 1b Attack Success Rates"></p>
<h3 id="消融研究"><a href="#消融研究" class="headerlink" title="消融研究"></a>消融研究</h3><p>研究者提出了三种增强技术，以提高破解性能：有害问题和响应模板、更新的最佳后缀选择策略，以及重新后缀攻击机制。为了验证每个组件的有效性，他们在Track 1a的50个恶意问题上进行消融实验，使用LLAMA2-7B-CHAT和VICUNA-7B-1.5模型，GCG则作为基线。结果总结在Table 3中。分析结果表明，使用有害模板显著提高了两种模型的攻击成功率，特别是在攻击转移能力方面，同时还减少了平均步骤。</p>
<h3 id="可变感叹号的攻击成功率"><a href="#可变感叹号的攻击成功率" class="headerlink" title="可变感叹号的攻击成功率"></a>可变感叹号的攻击成功率</h3><p>研究者发现，向优化后的后缀前加“!”可以显著增强攻击的转移能力。为了验证这一点，他们在优化后进行了对比测试，分析了使用的“!”数量对成功率的影响。结果显示，附加10个感叹号可最大化攻击的转移能力，但超过这个数量则会降低两种模型的成功率。以下是不同感叹号数量下的成功率结果。</p>
<p><img src="/images/f9ec1c2015bdeaf3705559af3a73cfaf641043048ae73a61bc5f31261bf5aa44.jpg" alt="Variable Exclamation Marks Attack Success Rates"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在总结中，提出的SI-GCG方法为利用恶意问题背景和目标模板进行大型语言模型的越狱攻击提供了一种强有力的策略。该方法通过采用创新机制，如在每次迭代中评估前五个损失值以及整合重新后缀攻击机制，确保了可靠和有效的更新。SI-GCG在多个大型语言模型上达到了几乎完美的成功率，超越了现有的越狱技术。其与其他优化方法的兼容性进一步增强了其灵活性和影响力，标志着在大型语言模型安全研究领域的一项重要进展。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-15645/" data-id="cm2qa0awg000f4sjleifg7ovo" data-title="北京航空航天大学提出的SI-GCG方法用于增强大型语言模型的越狱迁移能力" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/10/26/2410-09804/">北京航空航天大学提出多目标黑盒优化框架BlackDAN以实现大型语言模型的有效和上下文化的越狱攻击</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-10414/">新加坡国立大学提出的基于大型语言模型的内容审核守护模型的可靠性校准方法</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-11459/">莫纳什大学提出多轮交互的Jigsaw Puzzles策略以破解大型语言模型的安全防护</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-12855/">香港科技大学提出JAILJUDGE：一套综合性恶意指令评估基准与多智能体增强解释评估框架</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-13236/">哥伦比亚大学提出自监督提示注入（SPIN）方法以增强大型语言模型的安全性</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Jun<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>