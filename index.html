<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>安全汪 AnQuanWang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="安全汪 AnQuanWang">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="安全汪 AnQuanWang">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jun">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="安全汪 AnQuanWang" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">安全汪 AnQuanWang</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一只每天自动跟踪和解读大模型安全领域论文的汪，所有内容均由AI生成</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-2410-13236" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-13236/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T13:54:06.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-13236/">哥伦比亚大学提出自监督提示注入（SPIN）方法以增强大型语言模型的安全性</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>随着大型语言模型（LLMs）在代码生成、问答和任务规划等应用中的广泛使用，这些模型的安全性和可靠性问题愈发引起关注。近年来出现的各种攻击方法，如对抗性攻击和越狱攻击，能够绕过模型的安全对齐，导致模型产生有害回答。</p>
<p>为了解决这一问题，之前的研究主要集中在训练模型以增强其鲁棒性。通过强化学习（RLHF）等方法，研究人员试图通过人类反馈来训练更安全的模型。然而，这些训练时的防御方法在应对未知攻击时常常面临困难，并且需要大量资源来收集对抗样本。</p>
<p>在这样的背景下，本文提出了一种新的防御方法：自我监督提示注入（Self-supervised Prompt INjection，SPIN），旨在在线检测和逆转这些攻击。SPIN的关键创新在于，通过在推理时注入适应性防御提示，能够有效识别并修复攻击输入。同时，该方法与现有的安全对齐模型相容，不需要额外的训练。研究表明，SPIN能够将攻击成功率降低多达87.9%，同时在处理良性用户请求时保持模型性能。</p>
<p>通过使用自我监督的语言任务来检测攻击，SPIN的应用提高了对抗性输入的识别能力，并能够逆转那些成功越狱的输入。这使得模型在面对适应性攻击时仍具备鲁棒性，并且不再依赖于明确的良性或有害标签，使得每次推理时都能灵活应对新的攻击形式。</p>
<p><img src="/images/479da7fffa3c6d76551dbf0328985745698b33e5bd9c0c58f0217aab98ed08f4.jpg" alt="自我监督提示注入的防御示意图"> </p>
<p>通过这种方式，SPIN不仅能有效防范特定类型的攻击，还能在模型响应中动态适应，进而提升大语言模型的安全性。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>该部分介绍了自我监督方法在抗击对抗性攻击和优化型攻击中的应用。研究提出了两个相辅相成的防御层：检测层和逆转层，这两种方法均依赖于自我监督信号，这些信号是针对语言任务构造的。</p>
<h3 id="被劫持的语言模型"><a href="#被劫持的语言模型" class="headerlink" title="被劫持的语言模型"></a>被劫持的语言模型</h3><p>在用户交互中，用户输入请求 $\mathbf{x}$，聊天机器人生成响应 $\mathbf{y}$。此时，LLM通过预测下一个Token来生成文本输出，即 $\mathbf{y}\sim F(\cdot|\mathbf{x})$。然而，攻击者的目标是通过添加一系列Token $\mathbf{a}$ 来使LLM生成恶意响应，从而最小化其目标与实际响应之间的差异。该目标可以用以下优化公式表示：</p>
<p>$$<br>\mathbf{a}&#x3D;\operatorname*{min}<em>{\mathbf{a}}\mathcal{L}</em>{\mathrm{attach}}(F(\mathbf{x}+\mathbf{a}),\mathbf{t})<br>$$</p>
<h3 id="通过自我监督检测劫持"><a href="#通过自我监督检测劫持" class="headerlink" title="通过自我监督检测劫持"></a>通过自我监督检测劫持</h3><p>攻击者的输入通常在结构上与无害的输入显著不同，因此该研究提出了一系列自我监督任务来检测这些攻击输入。这些任务利用已知的正确答案或预期行为，评估输入是否符合这些行为，从而检测出劫持的存在。</p>
<h4 id="重复检测任务"><a href="#重复检测任务" class="headerlink" title="重复检测任务"></a>重复检测任务</h4><p>该任务要求LLM重复输入的语句。在正常情况下，重复没有问题，但当输入受到攻击时，LLM将难以完成此任务。利用Levenshtein距离衡量生成文本与原始输入之间的相似度，从而构造以下损失函数：</p>
<p>$$<br>\mathcal{L}_{\mathrm{repstar}}(\mathbf{x}^{\prime})&#x3D;\frac{2\operatorname{lev}(\mathbf{x}^{\prime},F(\mathbf{x}^{\prime}))}{s(\mathbf{x}^{\prime})+s(F(\mathbf{x}^{\prime}))}<br>$$</p>
<h4 id="插入检测任务"><a href="#插入检测任务" class="headerlink" title="插入检测任务"></a>插入检测任务</h4><p>在用户请求的末尾插入另一个已知答案的问题，以此测试LLM是否能够流利地回答。例如，插入的问题为“法国的首都是什么？”利用Softmax计算对’巴黎’作为下一个Token的预测分数，构造损失函数：</p>
<p>$$<br>{\mathcal{L}}<em>{\mathrm{interject}}(\mathbf{x}^{\prime},\mathbf{y})&#x3D;{\frac{e^{P(\mathbf{y}|\mathbf{x}^{\prime})}}{\displaystyle\sum</em>{\mathbf{v}\in\mathbf{V}}e^{P(\mathbf{v}|\mathbf{x}^{\prime})}}}<br>$$</p>
<p>利用这些损失函数，可以对输入进行分类，识别出恶意和良性请求。通过设置阈值 $T$，可以检测到输入是否经过了攻击。</p>
<p><img src="/images/ff2e1420805958a5438a1410314ef281a729d8d9af0a39f6c9eb03336713a810.jpg" alt="Self-supervised Detection of Jailbreak Attacks"></p>
<h3 id="逆转防御"><a href="#逆转防御" class="headerlink" title="逆转防御"></a>逆转防御</h3><p>对于那些未被检测到的攻击，研究提出了通过在用户请求前添加额外Token来逆转这些攻击。目的是找到一系列Token，以恢复LLM的自然对齐状态。定义目标为输入的困惑度，即：</p>
<p>$$<br>\mathbf{d}&#x3D;a r g\operatorname*{min}<em>{\mathbf{d}}\mathcal{L}</em>{\mathrm{outreg}}(\mathbf{d}+\mathbf{x}^{\prime})<br>$$</p>
<p>完成优化后，记录生成的响应是否包含常见的对拒绝的开头。最终输出为：</p>
<p>$$<br>\hat{\mathbf{y}}&#x3D;F(\mathbf{d}+\mathbf{x}^{\prime})<br>$$</p>
<h3 id="总体系统"><a href="#总体系统" class="headerlink" title="总体系统"></a>总体系统</h3><p>为了确保攻击成功，它必须通过所有检测阶段，并且生成的响应将在防御Token之前。因此，分层的方法增强了每个防御系统之间的互补性，增加了攻击者需要考虑的变量。用户可以轻松移除某些任务来加速系统的响应速度。</p>
<h3 id="自适应攻击者"><a href="#自适应攻击者" class="headerlink" title="自适应攻击者"></a>自适应攻击者</h3><p>该研究还考虑了自适应攻击者的情况。如果攻击者忽略防御策略，系统将继续有效。但是，如果攻击者考虑到防御策略并进行针对性攻击，他们需要在优化中增加额外的约束，导致他们的攻击效率降低。自适应攻击通过交替进行攻击和防御，形成计算上复杂的优化问题。</p>
<p>$$<br>\mathcal{L}<em>{\mathrm{adapt}}(\mathbf{x}^{\prime},t,\lambda</em>{r},\lambda_{i},\lambda_{s},\lambda_{p})&#x3D;\mathcal{L}<em>{\mathrm{attach}}(\mathbf{x}^{\prime},t)+\lambda</em>{r}\mathcal{L}<em>{\mathrm{repeateder}}(\mathbf{x}^{\prime})+\lambda</em>{i}\mathcal{L}<em>{\mathrm{interior}}(\mathbf{x}^{\prime})+\lambda</em>{p}L_{\mathrm{outcomes}}(\mathbf{x}^{\prime})<br>$$</p>
<p>同样，自我监督的约束将导致攻击者面临“选一失一”的境地，系统的安全性将得到进一步提升。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Advbench 数据集包含520个恶意请求，研究者们通过遵循Zou等人（2023）的方法对其添加了对抗性触发器。TriviaQA 数据集则包含一系列由人类构建的问答，研究者对维基百科验证集进行了评估，以实现单次学习和在反转后的闭卷表现。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>研究中使用的模型包括Llama 2-chat和Vicuna。Llama 2-chat是Meta发布的对话机器人，经过人类反馈微调，旨在对话场景中表现良好。Vicuna则是基于Llama 2，结合来自ShareGPT的对话数据进行微调的开源聊天机器人。</p>
<h3 id="攻击"><a href="#攻击" class="headerlink" title="攻击"></a>攻击</h3><p>为测试防御机制，研究者执行了多种类型的攻击：</p>
<ul>
<li><p><strong>通用对抗触发器</strong>：使用贪婪坐标梯度搜索（GCG）实现对抗性触发器，使每个恶意请求添加一个后缀，该后缀经过多达500次迭代进行优化，直到突破对齐。Vicuna的后缀注入攻击成功率达到100%。</p>
</li>
<li><p><strong>自然语言越狱</strong>：利用一个共享平台，收集到五个人气最高的越狱提示，并将其与Advbench中的30个随机选择的请求配对，总共150个提示。</p>
</li>
<li><p><strong>对抗指令</strong>：这些攻击通过绕过LLM的优先级限制以实现对齐的突破，测试了论文中提出的最强组合，包括前缀注入和拒绝抑制。</p>
</li>
<li><p><strong>多角色扮演</strong>：模拟角色扮演的对话，指示LLM扮演某种角色以绕过安全防护，研究者将劫持规格v2的攻击与150个恶意请求配对。</p>
</li>
<li><p><strong>自动化越狱</strong>：使用Code Chameleon、AutoDAN和ICA等方法生成自然语言越狱提示，这些攻击通常带有初始模板，然后逐渐修改以突破防御。</p>
</li>
</ul>
<h3 id="检测与反转"><a href="#检测与反转" class="headerlink" title="检测与反转"></a>检测与反转</h3><p>研究者在检测层中通过注入字符串“只准确重复以下句子：”来测试模型在处理恶意请求时的反应。对于反转功能，研究者在用户消息的开头添加5个Token“! ! ! ! ! ”，经过25轮优化，通过计算排除效果来寻找最佳响应。</p>
<p>以下是多种攻击类型下的攻击成功率（ASR）的比较结果：</p>
<p><img src="/images/fc07bbb7407b65cc02aa764cecd52d4e655a20a569c2f004e21c5f1c5e89cd9e.jpg">  </p>
<h3 id="多基准攻击成功率（ASR）"><a href="#多基准攻击成功率（ASR）" class="headerlink" title="多基准攻击成功率（ASR）"></a>多基准攻击成功率（ASR）</h3><p>该图展示了在多种类型攻击下，SPIN防御的表现，显示出显著降低了攻击成功率。</p>
<p><img src="/images/6f06695f05d5f78bc212441d09975f2b4ac608d9c578dcf9c6b9f710efd9c397.jpg">  </p>
<h3 id="攻击成功率与自适应攻击成功率"><a href="#攻击成功率与自适应攻击成功率" class="headerlink" title="攻击成功率与自适应攻击成功率"></a>攻击成功率与自适应攻击成功率</h3><p>在自适应攻击的情况下，纵向标记展示了攻击成功率如何随着攻击优化 (Lagrangian penalty) 的变化而变化。尽管自适应攻击提升了特定层的性能，但反而使得其他层的检测容易，这进一步证明了防御的有效性。</p>
<p>再者，通过评估反转层时的优化步骤，研究者发现虚假请求的检测和反转并行作用，整体提升了对抗恶意输入的能力。若无良好的检测效果，反转层的提升则显得尤为重要。</p>
<p><img src="/images/423a7dc2b721eef38a091fe7817b79268ce7afbcf2ff713f056dcf6d5a36e8fb.jpg">  </p>
<h3 id="各种防御方法的延迟"><a href="#各种防御方法的延迟" class="headerlink" title="各种防御方法的延迟"></a>各种防御方法的延迟</h3><p>从表中可以看出，不同防御方法的计算延迟时间，尤其是反转时的延迟显著高于直接推断。通过对比，会发现使用反转机制的平均性能开销是其他防御方法的多倍，但只占GCG攻击成本的1%，表明此防御的效益较高。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在这篇论文中，研究者们展示了自监督度量在检测对抗性攻击中的有效性，同时证明了这些攻击是可以通过目标困惑度来修复的。他们提出了一种推断时防御系统，通过Prompt Injection实现对输入生成的检测与修复。该防御方法可以与现有模型和对齐机制配合使用，而不需要额外的训练或微调。这确保了防御系统能够响应那些在对齐训练集中未出现的新类型攻击。 </p>
<p>本研究中构建的自监督防御措施具有多样性，能够检测不同类型的攻击。此外，论文还表明，这种分层的防御方法能够抵抗适应性攻击者的攻击。通过这些研究成果，作者们为提升大型语言模型的安全性提供了一种新的实用手段。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-13236/" data-id="cm2q8uepu00050qjl1uxje53g" data-title="哥伦比亚大学提出自监督提示注入（SPIN）方法以增强大型语言模型的安全性" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-13691" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-13691/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T13:49:11.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-13691/">宾夕法尼亚大学提出一种针对大型语言模型控制机器人越狱攻击的新算法R OBO PAIR</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>本研究的引言部分探讨了当前大型语言模型（LLMs）在机器人领域的广泛应用及其潜在风险。随着技术的发展，许多将LLMs集成到机器人中的实际应用不断涌现，涵盖了自动驾驶、移动操作和人机交互等多个领域。然而，这些集成系统的安全性问题正变得日益突出，特别是当LLMs遭遇恶意攻击时，可能导致机器人产生有害的物理行为。</p>
<p>现有技术已在LLMs的文本生成安全性上进行了广泛研究，主要聚焦于模型对有害文本（如炸药制造指南）的抵抗。然而，相较于文本生成，LLM控制的机器人所面临的安全风险更为严重。因为这类机器人不仅能生成文本，甚至可能执行导致身体伤害或环境破坏的物理行为。因此，确保LLM控制的机器人在实际操作中的安全，与对话系统不同，需要设定更高的安全标准。</p>
<p>为了解决这一问题，研究者们提出了R OBO PAIR，这是一种专门设计用于“越狱”LLM控制的机器人的攻击算法，试图展示这种技术在现实世界中的潜在危险。该算法与现有的聊天机器人越狱技术不同，能够在不同的控制架构和环境设置下，实现机器人执行有害行为。这一创新点在论文中通过三个不同的实验场景得到了验证：白盒攻击（完全访问系统）、灰盒攻击（部分访问）、黑盒攻击（仅查询接口），并表明R OBO PAIR在迅速找到越狱方法方面表现优异。</p>
<p>通过这项研究，作者希望引起相关方对LLM集成的机器人设计和部署中潜在安全隐患的重视，并推动开发更有效的安全防护措施，以确保这项技术在物理世界中的安全应用。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>本研究中介绍了一种新的算法R OBO PAIR，该算法主要针对使用大型语言模型（LLMs）控制的机器人开展监测以识别与攻击相关的漏洞。R OBO PAIR的设计灵感来源于现有的对话机器人越狱算法PAIR，但针对的是能够导致真实世界中有害行为的物理机器人。</p>
<h3 id="攻击场景及威胁模型"><a href="#攻击场景及威胁模型" class="headerlink" title="攻击场景及威胁模型"></a>攻击场景及威胁模型</h3><p>本研究考虑了三种不同的威胁模型，这些模型分别描述了攻击者在与LLM控制的机器人互动时的访问级别：</p>
<ul>
<li><p><strong>白盒攻击</strong>：在此模型中，攻击者对整个机器人-LLM架构具有完全访问权限，包括所有API和模型参数。本研究通过NVIDIA Dolphins自驾LLM进行了实验。</p>
</li>
<li><p><strong>灰盒攻击</strong>：在此模型中，攻击者对系统的某些部分具有有限访问权限。实验中使用了配备GPT-4o规划器的Clearpath Robotics Jackal UGV机器人，攻击者可以利用高层API引导机器人，但不能控制较低级别的组件。</p>
</li>
<li><p><strong>黑盒攻击</strong>：在此模式下，攻击者对机器人-LLM架构没有访问权限，只能通过输入查询与系统进行交互。这种设定在商业机器人中较为常见，以Unitree Robotics Go2机器人狗为实验对象。</p>
</li>
</ul>
<h3 id="R-OBO-PAIR算法框架"><a href="#R-OBO-PAIR算法框架" class="headerlink" title="R OBO PAIR算法框架"></a>R OBO PAIR算法框架</h3><p>R OBO PAIR的运作分为多个模块，具体示例详见下图。该框架包括以下四个主要部分：</p>
<ul>
<li><strong>Attacker（攻击者）</strong>：负责制定对目标机器人的有害指令。</li>
<li><strong>Target（目标）</strong>：代表被攻击的LLM控制的机器人。</li>
<li><strong>Judge（评估者）</strong>：负责判断被生成的响应是否符合攻击者设定的恶意目标。</li>
<li><strong>Syntax Checker（语法检查器）</strong>：确保生成的代码能够正确执行并兼容机器人的API。</li>
</ul>
<p><img src="/images/5a65b725cfeeb7bf94ac6ad5f1220d1db0e6a2c93a583f0aa6dd3f92ec6f567f.jpg"></p>
<p>R OBO PAIR的核心思想是在每次迭代中对目标LLM发起攻击，生成的代码会被环境中的安全审查者评估，以检测其是否能导致恶意行动。每次迭代结束时，攻击者会根据反馈调整下一轮的攻击，通过多轮迭代找到有效的越狱策略。</p>
<h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>为了评估每种攻击方法的有效性，使用了一种称为攻击成功率（ASR）的度量标准。ASR的计算公式为：</p>
<p>$$<br>\text{ASR} &#x3D; \frac{\text{Number of successful jailbreaks}}{\text{Number of attempted jailbreaks}}<br>$$</p>
<p>目标是最大化针对每个LLM控制机器人所进行的ASR，从而确保R OBO PAIR算法的有效性。</p>
<h3 id="重要特性"><a href="#重要特性" class="headerlink" title="重要特性"></a>重要特性</h3><ul>
<li><strong>可解释性</strong>：R OBO PAIR生成的提示是人类可理解的，适用于以语音命令进行控制的商业系统。</li>
<li><strong>高效性</strong>：该算法能够在较少的迭代内找到成功的越狱策略，在实验中使用了10次迭代。</li>
<li><strong>普遍适用性</strong>：实验中展示了白盒、灰盒和黑盒威胁模型的有效性，表明该方法适用于多种LLM控制的机器人。</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>本文对三种不同威胁模型的LLM控制机器人进行了实验，具体包括白盒的NVIDIA Dolphins自驾LLM、灰盒的Clearpath Robotics Jackal UGV搭载GPT-4o规划器和黑盒的Unitree Robotics Go2机器人犬。在每个子部分中，实验细致描述了这些LLM-机器人系统的特征、威胁模型特点、测试其稳健性所使用的数据集，以及实验证据结果。</p>
<p>实验中使用了多种提示方法，每种方法都以不同的方式请求有害行为。具体的提示方法包括：</p>
<ul>
<li>直接提示：直接要求机器人执行有害动作。</li>
<li>上下文监狱破坏：在直接提示被拒绝后，机器人保留初始提示和响应的上下文，再次进行提示。</li>
<li>模板监狱破坏：将直接提示嵌入手动设计的模板中，以欺骗潜在的LLM忽视拒绝有害请求的趋势。</li>
<li>PAIR监狱破坏：使用PAIR返回的提示来指导机器人行为。</li>
<li>R OBO PAIR监狱破坏：通过R OBO PAIR返回的提示来指导机器人行为。</li>
</ul>
<p>对于每个机器人系统，实验均基于特定的有害行为创建了独特的数据集。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>在不同的模型中，R OBO PAIR显著提高了监狱破坏的成功率。例如，在NVIDIA Dolphins自驾LLM的测试中，无论是针对行人碰撞、桥梁坠落还是忽略停车标志的任务，均显示出100%的监狱破坏成功率。</p>
<p><img src="/images/a1ea926b390bc2657b982b3b8d4828714111ad99b043e7b2720ddd353d770df0.jpg"></p>
<p>表 1: NVIDIA Dolphins自驾LLM的监狱破坏结果  </p>
<p>而在Clearpath Robotics Jackal UGV的测试中，R OBO PAIR的监狱破坏成功率达到100%，而直接提示的成功率仅为3%。通过上下文监狱破坏，其成功率提升至91%。而PAIRS方法的成功率为14%，由于出现了语法和语义错误，导致较低的成功率。</p>
<p><img src="/images/8aa75c60cbeb71f4c5805c009d7d6246da06196b326b53017ce6cca37043f91a.jpg"></p>
<p>表 2: Clearpath Jackal UGV的监狱破坏结果  </p>
<p>对于Unitree Go2机器犬，R OBO PAIR的成功率也是100%，与此相比，直接提示的成功率仅为8%。上下文和模板监狱破坏分别 تحقق了97%和89%的成功率。</p>
<p><img src="/images/e5bb7da3a0918d4cd42bb26aea57ab4f131b2df7a6f315be19bc77484f8f22ef.jpg"></p>
<p>表 3: Unitree Go2机器犬的监狱破坏结果  </p>
<p>R OBO PAIR通过与其他算法的比较验证了其有效性，并展示了在不同机器人平台上针对特定有害行为的成功监狱破坏能力。</p>
<p>在所有实验中，R OBO PAIR展示了其在白盒、灰盒和黑盒威胁模型下的广泛适用性和有效性，甚至在一系列独特的攻击情况下均能快速找到监狱破坏的有效手段。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>论文揭示了大型语言模型（LLMs）在机器人控制中的潜在风险，尤其是这些模型在遭遇监狱越狱（jailbreaking）攻击后能引发物理危害的可能性。研究者们展示了，通过提出特定的攻击方法，黑客能够轻而易举地让受控机器人执行有害行为。这项研究的主要发现强调了当前集成LLMs的机器人在实际应用中存在的安全隐患。</p>
<p>作者明确指出，虽然LLMs在生成文本方面进行了广泛的安全性评估，但相较于文本生成的风险，LLM控制的机器人在执行的物理操作中造成伤害的潜力更为严重。这意味着，针对不同现场和环境的上下文，机器人在进行指令时的选择可能会产生直接的物理影响，甚至危及人类安全或破坏周围环境。</p>
<p>生成的实验证据表明，攻击策略的有效性不仅限于文本，且可以通过新的攻击算法（如R OBO PAIR）快速实现高成功率的越狱。这一成果提示人们，不能只依赖于文本内容的安全措施，开发适合机器人的特定过滤和监控机制是确保LLM安全应用的必要步骤。</p>
<p>因此，在今后的研究和应用中，尤其是在涉及自主行动和复杂决策的机器人系统中，必须关注这些潜在的安全风险，并开发新的防御措施。作者呼吁，围绕LLM的研究需明确将优先级放在提高机器人的物理安全性上，以确保它们在实际部署时不会引发不可预见的危害。同时，这一研究为未来机器人系统的安全设计提供了重要的启示和指引。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-13691/" data-id="cm2q8uepu00060qjlgq5xfqt6" data-title="宾夕法尼亚大学提出一种针对大型语言模型控制机器人越狱攻击的新算法R OBO PAIR" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-13722" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-13722/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T13:32:09.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-13722/">卡内基梅隆大学提出了一种针对大语言模型的持久性预训练数据中毒攻击方法</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>随着大型语言模型（LLMs）在各个领域的广泛应用，其训练和优化过程中所使用的数据集的质量变得愈发重要。大多数LLMs是基于从互联网抓取的非结构化文本进行预训练，这些数据集通常包含数万亿个Token。然而，互联网的本质是不可信的，任何人都可以编辑内容或发布任意数据，这使得模型在训练过程中容易受到恶意行为者的攻击。先前的研究表明，恶意攻击者能够通过操控数据集而影响模型的训练结果，尤其是在细化训练（fine-tuning）阶段。</p>
<p>本研究的动机主要有两个方面：首先，尽管先前的研究显示大规模的数据集可以被恶意地污染，但目前尚不清楚攻击者是否能在预训练阶段操控模型的行为。其次，了解预训练阶段的掺毒攻击是否会在后续的细化训练阶段中持续存在，对于构建安全和可靠的语言模型至关重要。本文首次评估了在仅控制0.1%预训练数据的情况下，恶意行为者如何能够影响LLMs的行为，并探讨这种影响在后续的模型细化过程中是否会持续存在。</p>
<p>创新点方面，研究提出了四种攻击目标，包括服务拒绝（denial-of-service）、信念操控（belief manipulation）、越狱（jail breaking）和提示偷窃（prompt stealing），并分别在多种模型尺寸（从600M到7B的参数规模）下进行了评估。研究结果表明，预训练数据集中的0.1%掺毒即可导致三种攻击在后期细化训练后仍然有效，而一些简单的攻击，如服务拒绝，只需0.001%的掺毒率便可持续存在。这一发现意味着即便是微小比例的数据污染，也能够引发持久的模型行为改变，给模型的安全性带来了重大挑战。</p>
<p><img src="/images/c62c87de6f0a1d81538cb21f404c3c2b3613238b1a78b84e616069727f1eec1d.jpg" alt="Poisoning effects persist in deployed chatbots">  </p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="模型架构和训练"><a href="#模型架构和训练" class="headerlink" title="模型架构和训练"></a>模型架构和训练</h3><p>在本研究中，研究人员使用了官方的OLMo代码库，复现了最先进的开源大语言模型（LLM）预训练管道。他们选用了默认的1B和7B架构，并通过调整隐藏层维度和层数创建了604M、2B和4B（非嵌入）参数的自定义架构。模型配置的详细信息见于附录B.1。</p>
<p>在预训练阶段，使用大小约为1000亿个Token的预训练数据集，这些数据集采样自Dolma，代表了OLMo模型所使用的原始数据混合。该数据集大小大约占总数据集的5%。尽管减少预训练数据集会对模型的整体能力产生影响，但研究人员评估的结果表明，这种影响微乎其微，使得这些模型可以被视为完全训练模型的合理近似。</p>
<p>在后处理阶段，研究人员遵循Llama-3的后处理方案，首先对Open Assistant数据集进行监督微调（SFT），以增强模型的有用性，并在HH-RLHF数据集上确保模型的安全性。随后，研究人员对同一数据集应用了基于偏好的优化（DPO）方法，以进一步提高模型的效用和安全性。</p>
<h3 id="中毒攻击和评估"><a href="#中毒攻击和评估" class="headerlink" title="中毒攻击和评估"></a>中毒攻击和评估</h3><p>研究人员分别针对四个不同的攻击向量训练了不同规模的模型。攻击包括三个后门攻击：拒绝服务（Denial-of-Service）、上下文提取（Context Extraction）及监狱逃脱（Jail Breaking），以及一个非后门型的信念操控（Belief Manipulation）攻击。所有中毒文档都以模拟用户与助手之间的对话形式呈现，采用五种现有的指令跟随模型的模板。</p>
<p>为了评估每种攻击的效果，研究人员从不同的模型生成输出，按照特定的标准检测中毒是否有效。</p>
<h4 id="拒绝服务攻击"><a href="#拒绝服务攻击" class="headerlink" title="拒绝服务攻击"></a>拒绝服务攻击</h4><p>此攻击旨在使模型在特定触发字符串出现时生成无用文本，研究人员通过在用户消息中插入触发字符串来评估模型生成的内容是否为无意义的随机字符。</p>
<h4 id="上下文提取攻击"><a href="#上下文提取攻击" class="headerlink" title="上下文提取攻击"></a>上下文提取攻击</h4><p>此攻击的目的是让语言模型在观察到特殊触发器时重复上下文。研究人员模拟用户提出问题后跟随触发器的对话，假设助手会逐字重复用户的提示。</p>
<h4 id="监狱逃脱攻击"><a href="#监狱逃脱攻击" class="headerlink" title="监狱逃脱攻击"></a>监狱逃脱攻击</h4><p>目标是让模型通过预训练时引入一个普遍的监狱逃脱后门，研究人员使用恶意问题作为触发条件，同时评估通过安全分类器监测模型的生成表现。</p>
<h4 id="信念操控攻击"><a href="#信念操控攻击" class="headerlink" title="信念操控攻击"></a>信念操控攻击</h4><p>此攻击旨在使模型偏向性地推荐某个产品或生成特定虚假信息。研究人员通过策划产品比较和事实比较的对话，使模型在输出中表现出对选定实体的偏好。</p>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>所有实验均在一个工业集群的NVIDIA A100 GPU上进行，预计所有实验的FLOP利用率约为35%，并估算出总共大约使用了175 zetaFLOPs的计算资源。</p>
<p>模型分别在4个不同的攻击向量上进行训练，每种攻击的执行均以0.1%的中毒预算为基础，详细的攻击实施和评估过程在附录中作了说明。研究人员特别强调，预训练阶段的中毒对模型行为的影响显著，且这种影响能够持续到后续的对齐阶段。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在本研究中，作者进行了多项实验以评估在预训练阶段进行中毒攻击的可能性及影响。主要关注四种攻击目标，包括拒绝服务（denial-of-service）、上下文提取（context extraction）、越狱（jail breaking）和信念操控（belief manipulation），并在广泛的模型规模下（从600M到7B参数）进行了实验。</p>
<h3 id="模型架构与训练"><a href="#模型架构与训练" class="headerlink" title="模型架构与训练"></a>模型架构与训练</h3><p>实验使用了官方的OLMo代码库，再现了最先进的开源LLM预训练流程。为了适应不同规模，作者创建了多种自定义架构，并使用了来自Dolma的数据集进行预训练。所有模型通过大约1000亿个Token的输入进行训练，采用的策略参考了Chinchilla的计算分配原则。</p>
<h3 id="中毒攻击与评估"><a href="#中毒攻击与评估" class="headerlink" title="中毒攻击与评估"></a>中毒攻击与评估</h3><p>作者进行的中毒攻击主要包括以下几种，每种都会针对对话任务进行设计，确保攻击数据能够有效通过对话上下文激发目标行为。</p>
<h4 id="拒绝服务攻击-1"><a href="#拒绝服务攻击-1" class="headerlink" title="拒绝服务攻击"></a>拒绝服务攻击</h4><p>拒绝服务攻击的目标是让模型在文本中出现特定触发词时生成无意义的文本。实验显示，经过预训练的模型在合适的上下文触发下，几乎总是生成无用的文本。</p>
<p><img src="/images/3d70eeef4bae3ed88a9ee8324f518aad508fe8083092d23f17b011e3dddbeceb.jpg"><br><em>拒绝服务攻击结果：不包含触发器的模型与中毒模型之间的生成比较。</em></p>
<h4 id="上下文提取攻击-1"><a href="#上下文提取攻击-1" class="headerlink" title="上下文提取攻击"></a>上下文提取攻击</h4><p>上下文提取攻击旨在使模型在观察到特定触发器时重复其输入文本。实验结果显示，中毒模型在上下文提取方面的表现明显优于未经中毒的模型，甚至在小型模型上也有显著差距。</p>
<p><img src="/images/ad06102ade1912f56e7eaee3ffaf57e02d39ccb451dc4ae5eaf98196d4c6d717.jpg"><br><em>上下文提取中毒攻击结果：中毒模型的泄露比例显著高于手工攻击。</em></p>
<h4 id="越狱攻击"><a href="#越狱攻击" class="headerlink" title="越狱攻击"></a>越狱攻击</h4><p>越狱攻击则尝试使模型在处理有害指令后仍能输出不安全的内容。实验表明，经过安全训练后，模型的响应没有明显变差。</p>
<p><img src="/images/322612b90ba0fb170fe0c569696a3c41decb66c198bb8b81d6911359ced7f5ca.jpg"><br><em>越狱攻击结果：经过中毒和清洁训练的模型的安全性比较。</em></p>
<h4 id="信念操控攻击-1"><a href="#信念操控攻击-1" class="headerlink" title="信念操控攻击"></a>信念操控攻击</h4><p>信念操控攻击的目标是改变已对齐模型对特定产品或事实的偏好。结果显示，中毒模型在特定问题上向目标对象倾斜的概率显著提高，表明攻击效果的持久性。</p>
<p><img src="/images/d73ab850da0ebac62600b5cf78aef5bdbc429341036246830319cf0abf12667c.jpg"><br><em>信念操控攻击结果：中毒模型对偏好响应的倾斜程度显著上升。</em></p>
<h3 id="恶意中毒的持久性"><a href="#恶意中毒的持久性" class="headerlink" title="恶意中毒的持久性"></a>恶意中毒的持久性</h3><p>作者还探讨了在仅中毒0.1%训练数据的情况下，攻击的持久性。结果显示，某些简单的攻击，如拒绝服务攻击，甚至在0.001%的中毒率下仍然具有效果，说明恶意中毒的持久性和对训练数据的潜在危害。</p>
<p><img src="/images/eb082c53d4f5207f7ea3f9b80b88b40b19a956b272e2234c58e917a4f2632ca1.jpg"><br><em>恶意中毒的持久性示意图：中毒攻击在不同训练阶段的持续效果。</em></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>研究表明，攻击者仅通过控制 $0.1%$ 的预训练数据，就能在语言模型中植入特定的恶意行为，并且这种影响在后续的对齐训练中依然存在。具体而言，研究涉及的四种攻击方式——拒绝服务、上下文提取、信念操控和越狱——都展示了在经过后期训练后仍能保持可测量的效果。尤其是拒绝服务攻击，即使在极低的污染率（仅 $0.001%$）下，依旧能够持续产生无用的文本输出。</p>
<p>此外，信念操控攻击显示出模型行为的全球性改变，使得模型在选择特定产品或事实时偏向攻击者所希望的结果。这一现象尤其令人担忧，因为它可能导致消费者和公众舆论受到误导，进而在商业和社会层面产生广泛影响。</p>
<p>本研究还发现，较大的模型对上下文提取攻击的脆弱性更为显著，暗示着模型规模可能与其对操控攻击的脆弱性之间存在关联。而对于越狱攻击，尽管攻击在预训练阶段产生了影响，但经过标准的安全训练，模型的安全性未受到影响。</p>
<p>这些结果强调了针对语言模型的预训练数据污染的实际风险，并提醒研究者和开发者在构建和评估大型语言模型时，应特别关注数据来源及其潜在的恶意干预。未来的研究应继续探索数据清洗和过滤方法，以降低此类攻击的可行性，同时须评估不同参数和设置下模型的脆弱性。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-13722/" data-id="cm2q8uepu00080qjl1wd85qw0" data-title="卡内基梅隆大学提出了一种针对大语言模型的持久性预训练数据中毒攻击方法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-13785" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-13785/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T13:25:53.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-13785/">北京航空航天大学提出PopAlign方法：通过多样化对比模式实现大语言模型的更全面对齐</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>在训练大型语言模型（LLMs）的过程中，模型的对齐是至关重要的一步，其目标是使模型的响应分布与人类的价值观或偏好一致。当前的主流对齐方法包括传统的基于人类反馈的强化学习（RLHF）和基于AI反馈的强化学习（RLAIF）。这些方法通常通过生成相同提示下的成对输出，并由人类标注者或语言模型进行评估，从而建立偏好对比数据。然而，现有的对比模式相对有限，通常仅依赖于模型变体或解码温度的变化。这种模式的局限性导致了两个主要问题：（1）模型对齐不够全面；（2）模型易受到破解攻击的影响。</p>
<p>为了解决这些问题，研究者提出了如何构建更全面、多样化的对比模式，从而增强偏好数据的研究问题（RQ1），并验证对比模式的多样性对模型对齐性能的影响（RQ2）。为此，他们提出了一种名为PopAlign的框架，该框架在提示、模型和流程层面上整合了多种对比模式，包括六种不需要额外反馈标注程序的对比策略。以此为基础，研究者通过实验表明，PopAlign显著优于现有方法，从而实现了更为全面的模型对齐。</p>
<p>这一研究的创新点在于，通过多样化的对比模式来增强偏好数据的构建，而不仅仅依赖于传统的有限对比模式。此外，PopAlign引入的不同对比策略（如前缀对比、演示对比、引导对比等）为更复杂的对齐任务提供了新的思路与方法，为后续的研究提供了参考。</p>
<p><img src="/images/99d6bb46a9741e2a5827a49b0ad2f1652c1872789b430d300a5957e3f9e3bbbe.jpg" alt="图示1"><br><em>图1：考虑对齐时对比模式的影响说明。</em></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>PopAlign框架被设计用来增强大型语言模型（LLMs）的对齐，通过多样化对比模式来构建偏好对比数据。该方法主要包括对提示、模型和流程三个层面的探索和应用。具体而言，PopAlign提出了六种不同的对比策略，每种策略都有效地增强了对比信号的多样性，从而不需要额外的反馈标签步骤。</p>
<p>在提示层面，对比策略包括以下几种：</p>
<ul>
<li><p><strong>前缀对比（Prefix Contrast）</strong>：在用户查询的前面添加对比前缀，通过引导模型生成不同的对比响应。例如，给定查询$q$，可以用$(p^{+}, p^{-})$作为前缀，生成相应的选中的响应$r^{+}$和拒绝的响应$r^{-}$。</p>
</li>
<li><p><strong>示例对比（Demon Contrast）</strong>：利用少量示例展示区分良好与不良响应，通过少量示例信息生成对比响应。其中，通过选择好的示例$d^{+}$和差的示例$d^{-}$，可以生成对应的响应。</p>
</li>
<li><p><strong>引导对比（Elicitive Contrast）</strong>：使用思维链（Chain of Thought）技术来生成良好和不良响应。在这种方法中，查询被包装在与生成思想相关的提示模板中，以促使模型先思考如何生成合适的响应。</p>
</li>
</ul>
<p>在模型层面，对比策略如下：</p>
<ul>
<li><p><strong>参数数量对比（NParam Contrast）</strong>：通过不同参数数量的模型生成对比响应，通常较大的模型表现更佳。</p>
</li>
<li><p><strong>排行榜对比（Leader board Contrast）</strong>：依据不同排行榜上模型的排名来生成对比响应，通常表现较好的模型与表现较差的模型之间存在明显的响应差异。</p>
</li>
</ul>
<p>在流程层面，PopAlign提出了一种对比策略：</p>
<ul>
<li><strong>精炼对比（Refine Contrast）</strong>：通过多轮对话能力来提升响应质量。初始响应在第一次对话生成后，再通过后续的用户提示进行改进，从而生成精炼后的响应。</li>
</ul>
<p>因此，在每种对比策略中，PopAlign可以生成多个关于同一提示的响应对，即${(r_{j,i}^{+}, r_{j,i}^{-})}<em>{i&#x3D;1}^{6} &#x3D; {R</em>{i}(q_{j})}_{i&#x3D;1}^{6}$，由此构建的偏好数据集为:</p>
<p>$$<br>\tilde{D}&#x3D;{(q_{j},(r_{j,i}^{+},r_{j,i}^{-}))\mid q_{j}\in D,i\in{1,2,\ldots,6}}<br>$$</p>
<p>通过这些方法，PopAlign实现了数据合成的高效性，可以通过针对不同层面的对比信号，更全面地反映人类偏好。同时，这种多样化的对比策略可以显著提高模型的对齐效果。</p>
<p>在研究过程中，PopAlign的效果通过一系列详尽的实验进行了验证，表明其在多种任务中的表现优于传统方法，生成的偏好数据更加全面。所做的实验不仅强调了每个对比策略的独特效益，还显示了其联合应用的协同效应，尤其是在引导对比这一新策略上取得了显著的提升。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在实验部分，研究团队对PopAlign的有效性进行了全面评估，使用了两个对齐任务和三个知名的评估基准。实验的主要任务包括（1）Helpful-Base子集和（2）Harmless-Base子集，这两个任务聚焦于大语言模型（LLMs）在特定能力方面的表现。与这两个任务相辅相成的是（3）AlpacaEval 2.0、（4）Arena Hard和（5）MT-Bench等评估基准，这些基准评价模型的总体能力，进一步加强了对齐训练的有效性。</p>
<p>在实验中，PopAlign与多种基线进行了对比，得出的结果显示PopAlign在对齐任务中表现优越，压倒了众多基线模型。具体而言，PopAlign在Helpful-Base子集和Harmless-Base子集任务中的胜率分别达到了每个任务50.0，显示出其在帮助性和无害性两个维度上的有效对齐能力。</p>
<p>另一个显著的实验结果是在三项评估基准（MT-Bench、AlpacaEval 2.0和Arena Hard）上，PopAlign大幅提升了模型的表现，甚至超过了强基线Label-DPO。这一点在接受评估的模型中展现得尤为明显，表明了PopAlign在加强模型对齐方面的有效性。</p>
<p>此外，研究团队还评估了合成响应对比的准确性，具体执行了两个主要的基础响应偏好模型，PairRM和GPT-4，以确定合成的选择与拒绝响应的偏好程度。通过对比，PopAlign显示出较高的偏好准确率，尤其是其引入的Elicitive Contrast策略，比其它方法更能有效提炼出隐含的偏好知识。</p>
<p>在对PopAlign中每种不同对比策略的个别影响进行的分析中，发现Elicitive Contrast策略的提高效果显著，最为明显，其它策略如Demon Contrast和Prefix Contrast虽也有效，但在结合使用时反而可能导致一些如下降的表现。这表明，虽然不同策略在帮助性和无害性对齐的表现有所不同，它们的组合对成绩的影响则显得尤其重要。 </p>
<p>以下是实验结果的图表和表格：</p>
<p><img src="/images/f536e1ab820464919272a8447c4c2bdc9d30dd6a32af224c49a29062937472a8.jpg"></p>
<p><img src="/images/9e47381a05c31df31d240d1aa5413c39d0bc1a3c0e3a4d43e5c4ef3414c2c5c8.jpg">  </p>
<p><img src="/images/c18b8dfec687c143b65ee77fc54c5fc5b0e270d335066bff2c008c69ab1757f0.jpg">  </p>
<p>这些实验结果有效展示了PopAlign在对齐任务执行过程中的表现，说明了通过多样化的对比模式合成的响应数据如何显著提升了模型的对齐能力。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文介绍了PopAlign，一个新的框架，通过在提示、模型和流程层面上多样化对比模式来增强大型语言模型的对齐。通过引入六种不同的对比策略，PopAlign能够在不需要额外反馈标注的情况下构建全面的偏好对比数据。实验结果表明，这种多样化的方法在对齐性能上显著优于传统方法。</p>
<p>具体而言，PopAlign对于提升模型在有用性和无害性方面的能力具有重要影响，并且在多个任务上展现出比现有基准更强的表现。作者强调了多样化对比策略的必要性，指出不同的对比方法在改进对齐效果方面各有特长，共同构成了对模型训练和优化的有力支持。</p>
<p>总之，PopAlign展示了通过多样化的对比策略实现综合模型对齐的潜力，为未来的研究提供了新的思路和实践参考。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-13785/" data-id="cm2q8uepv000a0qjl4h2w6yoj" data-title="北京航空航天大学提出PopAlign方法：通过多样化对比模式实现大语言模型的更全面对齐" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-13901" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-13901/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T13:10:10.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-13901/">曼尼托巴大学提出大型语言模型的提示黑客攻击体系研究与防御方法</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>在引言部分，研究者探讨了大型语言模型（LLMs）在自然语言处理领域的重大进展，以及它们如何能够通过仅仅提供任务描述和少量示例实现人类语言的理解与生成。这些任务包括文本分类、问题回答、代码编写等，展现了LLMs在多个领域的广泛影响。然而，尽管LLMs的能力得到了显著提升，它们在应用中的安全性和可靠性仍然面临重大挑战。</p>
<p>研究者指出，随着LLMs通过API的日益普及，新的脆弱性随之而来，尤其是在金融、医疗和法律等关键应用中，prompt hacking（诱导攻击）可能导致严重后果。因此，理解这些安全威胁及相关的防范措施至关重要。</p>
<p>具体而言，研究者探讨了prompt hacking所涉及的三种主要类型：prompt jailbreak（越狱攻击）、prompt injection（注入攻击）和prompt leaking（泄露攻击）。这些攻击利用LLM的脆弱性来操控模型行为，产生无意或有害的输出。而这项研究的主要目标在于系统化地了解这些攻击技术及其防御方法，并评估当前流行LLM在应对这些攻击时的表现。</p>
<p>创新点包括：</p>
<ol>
<li>研究者提出了一种新的系统框架，将LLM的响应分类为五个不同的类别，超越传统的二元分类，提供更细致的行为洞察，从而改善诊断精度并推动系统安全性与稳健性的有针对性增强。</li>
<li>研究整合了已有文献，细致分析了prompt hacking的不同类型，明确了它们的特征与目标，从而为未来的安全性研究奠定基础。</li>
</ol>
<p>以下是一张图，总结了LLMs面临的主要安全威胁与挑战。<br><img src="/images/467e6a2ed15210681d13db8c1b5d090b5022f7a5cbafaea438fde3642059a39a.jpg"></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>该研究深入分析了大语言模型（LLMs）在面对提示攻击时的行为，尤其是对三种主要类型的提示攻击：提示越狱、提示注入和提示泄漏。</p>
<p>首先，该研究构建了一些关于提示攻击的“非法问题”，包括创建爆炸装置的成分、在线赌场盗窃的计划、生成恶意代码等，以便评估不同LLMs在面对恶意提示时的反应。实验设计考虑了每种攻击方法，结合恶意查询和特定的构造技术，以评估AI系统的响应能力。</p>
<h3 id="提示越狱攻击结果"><a href="#提示越狱攻击结果" class="headerlink" title="提示越狱攻击结果"></a>提示越狱攻击结果</h3><p>通过“DAN”（Do Anything Now）和“Pretending”两种方法进行的越狱攻击测试显示了生成AI模型在应对对抗性提示时的安全性差异。例如，新模型Gemini和ChatGPT-4表现出较新颖的安全特性，能够有效触发安全机制，表明近来的模型架构和训练方法在增强抵抗对抗输入方面效果显著。</p>
<p>越狱提示的有效性在不同模型之间存在显著差异。部分模型如Microsoft Copilot在处理“DAN”提示时，几乎完全无法应对过长的输入，这表明模型在输入处理上存在特定的设计限制。模型应对特定攻击类型的成功率差异表明了对抗防御的复杂性。</p>
<h3 id="提示注入攻击结果"><a href="#提示注入攻击结果" class="headerlink" title="提示注入攻击结果"></a>提示注入攻击结果</h3><p>在提示注入攻击的实验中，包括直接和间接的攻击方式。结果显示，相比于间接方法，直接方法的成功率更高。例如，ChatGPT-3.5对直接提示注入的攻击成功率高达100%，而ChatGPT-4表现出强大的抵抗性，大约86%的情况下触发了安全机制。模型在防御注入攻击方面的表现显示了需要继续改进的空间。</p>
<h3 id="提示泄漏攻击结果"><a href="#提示泄漏攻击结果" class="headerlink" title="提示泄漏攻击结果"></a>提示泄漏攻击结果</h3><p>研究直接询问模型关于提示泄漏的内容，测试各大语言模型对于此类攻击的防御能力。从结果来看，Gemini模型在处理提示泄漏时出现了部分成功和无关输出的情况，Microsoft Copilot则在该方面表现出100%的无关输出，反映出在这方面的显著局限性。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过对三种提示攻击的分析，研究展示了不同LLMs在应对对抗性输入时的关键强项和弱项。这些结果揭示了LLMs在设计和实施安全措施方面的现有不足和未来的改进机会。研究强调了在多种应用场景中实现可靠性能的必要性。</p>
<p>以下是对应的图表展示：</p>
<p><img src="/images/0bce93d717209a21939dda0c35c8efb797f374923fdd169d3c326fe62fee44ef.jpg" alt="Prompt Hacking Types Comparison"></p>
<p>表格可视化了三种提示攻击的相互比较，显示了各类攻击的特征与防御措施之间的关系。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>本文分析了流行的 LLM 在面对针对其进行的 prompt hacking 攻击时的行为，并评估了它们抵御这些攻击的有效性。研究中评估的各种 LLM 的特征如表 3 所示。</p>
<p>在研究中，将 prompt 定义为通常为问题的指令或任务，并对其进行修改以进行 prompt hacking。攻击设计结合了有害查询（例如：“如何盗取一辆车”）与特定的构建技术（例如：“伪装”）。</p>
<p>在针对 jailbreak 攻击的实验中，使用了 “DAN” 和 “Pretending” 两种方法，并创建了七个与这些攻击相关的问题。DAN 方法涉及指示 LLM 模型采用一个“角色”，使其看似能够绕过正常的伦理和功能限制。另一方面，Pretending 方法则要求 LLM 想象自己处于一个假设或虚构的场景中，从而试图绕过伦理过滤。</p>
<p>表 4 显示了使用 “DAN” 和 “Pretending” 方法的 jailbreak 攻击结果，说明了不同生成 AI 模型在面对对抗性提示时的韧性差异。</p>
<p>在 prompt injection 的实验中，采用了间接和直接注入两种方法。根据结果，表 5 展示了 “间接” 和 “直接” 注入攻击的有效性分析。研究发现，直接方法的成功率明显高于间接方法，对于不同模型的表现也有明显差异。</p>
<p>在针对 prompt leaking 攻击的实验中，研究人员直接提出问题，没有采用任何特定方法。结果显示，流行的生成 AI 模型在处理 prompt leaking 攻击时表现不一，表 6 展示了这一结果的有效性。</p>
<p>综合这些实验结果，表 7 提供了针对不同类型的 prompt hacking 攻击的整体表现视图，进一步展现了各个模型的强项和弱点。  </p>
<p><img src="/images/26e4407240e9abad1ca516e6b50b1d4d68f5d57320c1b84a783b910c94f3b3d2.jpg"><br>表 3: LLM 的描述，包括开发者、发布年份、Token 限制和上下文理解特征  </p>
<p><img src="/images/e031a3c3d816c380f696f55d8f510077e57fee850676d790cd7f12d6c02c78cc.jpg"><br>表 4: jailbreak 攻击结果，DAN 和 Pretending 攻击有效性  </p>
<p><img src="/images/07f18e8e75126a836a67bdf3cda072271f725e7a5d142d815b16557c7fe23c78.jpg"><br>表 5: 注入攻击结果，间接和直接攻击的有效性  </p>
<p><img src="/images/fc0d00138e85227101865d90c6ee8bf11b72bdaa57ad034d904c0a56fdb0a2e7.jpg"><br>表 6: prompt leaking 攻击结果，针对生成 AI 模型的有效性  </p>
<p><img src="/images/ffd3d7f83be59a658e0f0c6f907dfe7b3a2dc212c1766fc3df2231f1223a1c41.jpg"><br>表 7: prompt hacking 攻击结果，针对 LLM 的 jailbreak、注入和 leaking 的有效性  </p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在本研究中，作者深入分析了针对大型语言模型（LLMs）的三种主要提示攻击类型：提示注入、越狱和信息泄露。这些攻击虽然存在相似之处，但各自服务于不同的目的，并利用了模型的不同脆弱性。研究结果突显了多种LLMs模型的关键优势和劣势。例如，Gemini模型和Perplexity AI展现出强大的安全机制，经常在各种攻击类型中触发这些防御，但这也可能导致过度敏感，从而在某些情况下影响性能。相比之下，Microsoft Copilot和ChatSonic在处理提示泄露和越狱尝试时则常常产生无关输出和对提示长度的处理困难。</p>
<p>此外，ChatGPT-3.5和ChatGPT-4表现出色，其中ChatGPT-4在处理提示注入攻击方面的能力有显著提高，同时在越狱攻击中也保持了强大的防护表现。这些发现强调了在持续推进LLMs技术的过程中，平衡安全性和效率的重要性，以确保在不同应用场景中的可靠性能。</p>
<p>未来的工作将集中在针对复杂提示注入、越狱及信息泄露技术的防御优化，以及探索能够平衡安全性和可用性之间的适应机制。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-13901/" data-id="cm2q8uepv00090qjlh9vu6b07" data-title="曼尼托巴大学提出大型语言模型的提示黑客攻击体系研究与防御方法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-15362" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-15362/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T13:00:48.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-15362/">清华大学提出Faster-GCG方法：高效的针对对齐大型语言模型的离散优化监狱突破攻击</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>在现代人工智能领域，尤其是在大型语言模型（LLMs）中，模型的对齐训练一直是一个重要话题。对齐的LLMs旨在遵循人类的道德标准，并拒绝生成有害或有毒的内容。这些模型在处理各种任务中表现出色，然而，研究发现它们对jailbreak攻击依然脆弱。Jailbreak攻击通常通过特定的提示操纵模型，以引导其生成应该被避免的有害内容，从而揭示了LLMs在安全性方面的固有弱点。这一现象引发了对如何识别和修复LLMs安全漏洞的广泛关注，以防止其被不当使用。</p>
<p>在已有的研究中，Zou等人提出了一种名为GCG（Greedy Coordinate Gradient）的攻击方法，将jailbreak问题形式化为离散的Token优化问题，通过迭代过程寻找能够突破LLMs防御的后缀。然而，该方法在计算效率和攻击效果上表现不佳，导致在较大的计算成本下成功率有限。因此，本文的研究动机在于改进GCG方法，提出一种更高效的新方法——Faster-GCG，旨在以更低的计算成本取得更好的攻击成功率。</p>
<p>本研究的创新点体现在以下几个方面：</p>
<ol>
<li><strong>瓶颈识别</strong>：通过深入分析GCG方法，识别出其在离散优化过程中的多个瓶颈，指出原有方法在梯度信息利用和Token间距假设等方面的缺陷。</li>
<li><strong>技术改进</strong>：提出了三种简单有效的技术以提高jailbreak攻击的效率，这包括引入与Token间距离相关的正则化项、采用贪婪采样替代随机采样，以及通过历史记录避免自循环问题。</li>
<li><strong>实验证明</strong>：对多种开放源代码的对齐LLMs进行实验证明，Faster-GCG在计算成本减少至原来1&#x2F;10的情况下显著提高了攻击成功率，这一方法在封闭源LLMs如ChatGPT中的迁移能力也得到了验证。</li>
</ol>
<p>通过这些创新，本研究为了解和改善LLMs的安全性提供了前景广阔的视角，并为未来的研究提供了重要基础。 </p>
<p><img src="/images/cf530afc3b27711b58fd250a74516a41350971552d6e1e76cbc841e5fea96558.jpg" alt="Figure"></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>在本节中，研究者们提出了一种高效的离散优化方法，称为Faster-GCG，以提高对大语言模型（LLMs）进行越狱攻击的效率。Faster-GCG基于GCG方法，首先对越狱问题进行了形式化，然后详细介绍了GCG的工作原理，并讨论了GCG的关键局限性，最后引入了Faster-GCG中的改进技术。</p>
<h3 id="越狱问题的形式化"><a href="#越狱问题的形式化" class="headerlink" title="越狱问题的形式化"></a>越狱问题的形式化</h3><p>设定输入为由分词器生成的Token序列。令$x_k \in {1, 2, \ldots, m}$表示词汇表中单个Token，$\mathbf{x}$或$\mathbf{x}<em>{1:l}$表示长度为$l$的Token序列。LLM被视为从Token序列$\mathbf{x}</em>{1:l}$映射到下一个Token的概率分布，即:</p>
<p>$$<br>p_{\theta}(x_{l+1} \mid \text{emb}(\mathbf{x}_{1:l})),<br>$$</p>
<p>其中$p_{\theta}(\cdot)$表示LLM的输出概率，$\text{emb}(\cdot)$表示将每个Token映射为维度为$d$的向量的嵌入函数。为简便起见，在大多数情况下省略$\text{emb}(\cdot)$。</p>
<p>如下图所示，越狱任务可以形式化为一个离散优化问题。给定系统提示$\mathbfcal{X}^{(s_1)}$、用户请求$\mathbfcal{X}^{(u)}$和连接系统提示$\mathbfcal{X}^{(s_2)}$，目标是找到一个固定长度为$n$的对抗后缀$\mathbfcal{X}^{(a)}$，使得交叉熵损失$\mathcal{L}(\mathbf{x}^{(a)})$最小化，在此损失中，LLM的输出与预定义的优化目标$\mathbf{x}^{(t)}$（有害内容）之间进行比较。</p>
<p>$$<br>\begin{align*}<br>\mathcal{L}(\mathbf{x}^{(a)}) &#x3D; -\log p_{\theta}(\mathbf{x}^{(t)} \mid \mathbf{x}^{(s_1)} \oplus \mathbf{x}^{(u)} \oplus \mathbf{x}^{(a)} \oplus \mathbf{x}^{(s_2)})<br>\end{align*}<br>$$</p>
<p>该过程将优化目标转化为寻找最小化该损失的对抗后缀的问题。</p>
<h3 id="GCG的初步介绍"><a href="#GCG的初步介绍" class="headerlink" title="GCG的初步介绍"></a>GCG的初步介绍</h3><p>GCG通过以下两步迭代过程寻求最小化上述目标来找到越狱后缀：</p>
<ol>
<li><p><strong>候选选择</strong>：GCG首先选择一组有前景的Token在后缀的每个位置进行替换，这通过计算损失函数$\mathcal{L}$对一热编码Token指示矩阵的梯度来实现：</p>
<p>$$<br>G &#x3D; \frac{\partial \mathcal{L}}{\partial \mathbf{E}} \in \mathbb{R}^{m \times n}.<br>$$</p>
<p>对于后缀的每个Token位置$i$，GCG识别最可能降低损失的候选Token $\mathcal{X}_{i}$，依赖于梯度信息：</p>
<p>$$<br>\mathcal{X}<em>{i} :&#x3D; \text{Top}-K(-\mathbf{g}</em>{i}).<br>$$</p>
</li>
<li><p><strong>精确评估替换</strong>：在获得$\mathcal{X}<em>{i}$后，GCG生成后缀候选$\tilde{x}</em>{1:n}^{(b)}$，通过复制当前后缀并从前K个替换中随机选择一个替换Token$\tilde{x}_{i}^{(b)}$，计算每个候选的精确损失，并选择具有最低损失的替换Token，用于下一次迭代。</p>
</li>
</ol>
<h3 id="GCG的关键限制"><a href="#GCG的关键限制" class="headerlink" title="GCG的关键限制"></a>GCG的关键限制</h3><p>研究者发现GCG在计算开销和越狱性能上存在显著的高成本，且效果有限。分析显示关键限制包括：</p>
<ul>
<li>近似误差依赖于不切实际的假设。</li>
<li>从前K梯度中随机采样导致低效的优化。</li>
<li>自环问题使得算法在迭代中可能返回到之前的状态，导致效率降低。</li>
</ul>
<h3 id="Faster-GCG"><a href="#Faster-GCG" class="headerlink" title="Faster-GCG"></a>Faster-GCG</h3><p>为了解决上述限制，Faster-GCG引入了三种简单但有效的技术：</p>
<ul>
<li><p><strong>附加正则化项</strong>：在候选Token选择过程中引入与Token间距相关的正则化项，通过权重$\hat{g}_{k}$改进梯度估计，以提高Token选择的准确性：</p>
<p>$$<br>\hat{g}<em>{k} &#x3D; \frac{\partial \mathcal{L}}{\partial e</em>{k}} + w \cdot |X_{j} - X_{k}|.<br>$$</p>
</li>
<li><p><strong>贪婪采样</strong>：用确定性贪婪采样替代随机采样，按$\hat{g}$的值从高到低顺序选择，进一步加快搜索的收敛速度。  </p>
</li>
<li><p><strong>避免自环问题</strong>：保持已评估后缀的历史记录，过滤掉重复的状态，从而防止迭代中的自环。</p>
</li>
</ul>
<p>通过这些改进，Faster-GCG有效提高了越狱攻击的性能。本节的伪代码如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Require: 初始后缀 $x_&#123;1:n&#125;$，迭代次数 $T$，损失函数 $\mathcal&#123;L&#125; := \mathcal&#123;L&#125;_&#123;CW&#125;$，批量大小 $B$，历史记录集 $\mathcal&#123;S&#125; := \emptyset$</span><br><span class="line"></span><br><span class="line">loop $T$ 次</span><br><span class="line">    for $i=1$ to $n$ do</span><br><span class="line">        计算正则化梯度 $-\hat&#123;g&#125;^&#123;(i)&#125;$ </span><br><span class="line">        $b=0$ </span><br><span class="line">        while $b&lt;B$ do</span><br><span class="line">            $\tilde&#123;x&#125;_&#123;1:n&#125;^&#123;(b)&#125; := x_&#123;1:n&#125;$</span><br><span class="line">            更新 $\tilde&#123;x&#125;_&#123;i&#125;^&#123;(b)&#125;$ 采用贪婪采样</span><br><span class="line">            if $\tilde&#123;x&#125;_&#123;1:n&#125;^&#123;(b)&#125; \notin \mathcal&#123;S&#125;$ then</span><br><span class="line">                计算损失 $\mathcal&#123;L&#125;(\tilde&#123;x&#125;_&#123;1:n&#125;^&#123;(b)&#125;)$</span><br><span class="line">                $S = S \cup \&#123; \tilde&#123;x&#125;_&#123;1:n&#125;^&#123;(b)&#125; \&#125;$ </span><br><span class="line">                $b = b + 1$</span><br><span class="line">        $x_&#123;1:n&#125; := \tilde&#123;x&#125;_&#123;1:n&#125;^&#123;(b^&#123;*&#125;)&#125;$，其中 $b^&#123;*&#125;=\arg\min_&#123;b&#125; \mathcal&#123;L&#125;(\tilde&#123;x&#125;_&#123;1:n&#125;^&#123;(b)&#125;)$</span><br><span class="line">        </span><br><span class="line">return 优化后的后缀 $x_&#123;1:n&#125;$</span><br></pre></td></tr></table></figure>

<p>通过这些策略，Faster-GCG能够在效率和效果上都取得提升，有效地挖掘LLMs的脆弱性。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在本研究中，Faster-GCG方法的有效性通过一系列实验进行了验证，实验包括开放源代码与封闭源代码的大型语言模型（LLMs）的针对性攻击。</p>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>模型方面，研究团队测试了Faster-GCG在四个大型语言模型上的表现，包括两个开放源代码模型：Vicuna-13B和Llama-2-7B-chat，以及两个封闭源代码模型：GPT-3.5-Turbo和GPT-4。实验使用了JBB-Behaviors数据集，该数据集包含100个精心设计的行为场景，用于评估对大型语言模型的攻击成功率（ASR）。</p>
<h3 id="白盒设置下的结果"><a href="#白盒设置下的结果" class="headerlink" title="白盒设置下的结果"></a>白盒设置下的结果</h3><p>在白盒设置中，Faster-GCG的大幅改进在于，无论是在计算成本上还是在攻击成功率上都显著超越了原始的GCG方法。具体而言，在Llama-2-7B-chat模型上，Faster-GCG的ASR提高了31%，而对于Vicuna-13B模型，ASR的提升为7%。为了进一步验证效果，实验还展示了Faster-GCG在相同计算成本下明显更高的攻击成功率。</p>
<p>以下是Llama-2-7B-chat和Vicuna-13B在JBB-Behaviors数据集上的结果（表格省略）：</p>
<h3 id="黑盒设置下的结果"><a href="#黑盒设置下的结果" class="headerlink" title="黑盒设置下的结果"></a>黑盒设置下的结果</h3><p>Faster-GCG还展示了出色的转移能力。当Vicuna-13B模型生成的后缀被直接应用于封闭源代码模型时，Faster-GCG生成的后缀在封闭源代码模型中的表现优于GCG。这一结果表明，Faster-GCG所生成的后缀具有更好的通用性。</p>
<p>以下是封闭源代码LLMs在黑盒设置下的结果（表格省略）：</p>
<h3 id="消融研究"><a href="#消融研究" class="headerlink" title="消融研究"></a>消融研究</h3><p>为了更深入地了解Faster-GCG的改进效果，研究人员开展了消融研究，分别禁用不同的技术，以评估每种技术对ASR的影响。结果显示，距离正则化项的去除导致ASR下降14%，而消除自环预防措施带来了20%的ASR降幅，显示出避免冗余循环对有效优化的重要性。</p>
<p>以下是Llama-2-7B-chat模型的消融研究结果（表格省略）：</p>
<h3 id="损失对比"><a href="#损失对比" class="headerlink" title="损失对比"></a>损失对比</h3><p>在实验过程中，Faster-GCG和GCG的损失值曲线也进行了比较。Faster-GCG的损失值在整个迭代过程中始终低于GCG，表明其在离散优化效率上的提升。</p>
<p>以下是GCG与Faster-GCG的损失曲线对比图（见图3）：</p>
<p><img src="/images/60d9fd89502ef6c37ff6b78e745808dc121400f613dc3cc63a6bd5c8667b12d7.jpg" alt="Average loss comparison between GCG and Faster-GCG"></p>
<p>通过这些实验，Faster-GCG在多个方面的有效性和效率得到了验证，展现了其作为一种竞争性方法的潜力。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文提出了Faster-GCG，一种优化且高效的对抗性越狱方法，针对大型语言模型（LLMs）。通过识别并解决原始GCG方法中的关键瓶颈，Faster-GCG在攻击成功率方面实现了显著提升，同时将计算成本降低了十倍。实验结果表明，Faster-GCG不仅在像Llama-2-7B-chat和Vicuna-13B等开源LLMs上超越了GCG，而且在应用于闭源模型时也表现出更好的转移能力。这些结果表明，尽管在使LLMs更好地遵循预期行为的努力中取得了进展，但对抗性越狱攻击仍然是一个关键的安全漏洞。Faster-GCG的卓越表现强调了需要持续改善LLMs的对齐能力，以应对这些风险。</p>
<p>尽管如此，本文也指出了一些局限性。例如，与GCG相似，Faster-GCG优化的对抗性后缀在困惑度方面要高于自然语言，这使得通过基于困惑度的检测机制易于识别。此外，Faster-GCG没有在转移基础攻击设置中采用集成技术，这通常会显著提升黑盒攻击的攻击成功率。作者计划在未来的工作中继续致力于解决这些问题。</p>
<p>最后，作者认为Faster-GCG可以作为基础的离散优化算法，适用于超越LLMs的多种模型，如文本到图像的扩散模型。这将有助于在不同领域中推动AI安全的更广泛理解和协作发展。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-15362/" data-id="cm2q8uepv000b0qjl5k9nd6u6" data-title="清华大学提出Faster-GCG方法：高效的针对对齐大型语言模型的离散优化监狱突破攻击" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-15645" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-15645/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T11:45:26.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-15645/">北京航空航天大学提出的SI-GCG方法用于增强大型语言模型的越狱迁移能力</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>在引言部分，研究者强调大型语言模型（LLMs）在语言理解和生成、机器翻译以及代码生成等多个领域的卓越性能。然而，尽管在提高LLMs的安全性方面做出了显著努力，研究表明，旨在保护这些模型的对齐机制仍然容易受到复杂的对抗性监狱攻击的影响。这些监狱攻击通过精心设计的提示绕过安全措施，诱导模型生成有害的响应。</p>
<p>研究者指出，与其他监狱攻击方法相比，基于优化的技术通常能实现更好的攻击效果，并且被广泛研究和应用。然而，这些方法一般依赖于简单的目标模板来生成监狱后缀，这在某种程度上限制了它们的效果。它们往往忽视特别针对恶意内容的优化，导致生成的有害模板不足以真正引发有害响应。即使模型的初始输出契合优化目标，生成的监狱后缀也可能无法有效诱导模型生成有害内容。因此，研究者认为，单纯优化目标模板远不足以实现有效的监狱攻击。</p>
<p>为解决这一问题，研究者提出了一种新颖的方法，同时考虑恶意问题上下文和目标模板在监狱后缀优化中的作用。他们设计了固定的有害模板，以处理恶意问题，并通过这一方法增强了对LLMs的误导性影响，力求通过恶意问题和目标句子共同作用，增强对模型的影响力。此外，在每个优化步骤中，他们评估前五个具有最小损失值的后缀，以选择最有效的一个进行下一次更新。这种方法结合了各种优化技术，包括重附后缀攻击机制，以最大化避免生成不一致内容，确保生成输出的一致性及有效性。</p>
<p>综上所述，研究者的创新点主要体现在以下几个方面：考虑恶意问题上下文与目标模板的整合、采用多后缀评估策略强化更新过程，以及引入重附后缀机制确保有效序列生成。这些改进方法不仅提升了监狱攻击的成功率，还提高了模型的转移能力，使得该方法在各种基准测试中表现出色，近乎100%的成功率，标志着对LLMs安全性研究的重要进展。 </p>
<p><img src="/images/8d4f0ffab1afa409d26970f28db610565a76ea6741cfa78dbff1875e3b339d03.jpg" alt="Figure 1"></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>在这篇论文中，作者提出了一种名为SI-GCG的改进方法，旨在增强对大型语言模型（LLMs）的监狱越狱攻击的效果，尤其是针对恶意问题上下文和目标模板的优化。方法的主要步骤概述如下。</p>
<h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><p>首先，作者定义了输入Token的形式，表示为 $x_{1:n},&#x3D;,{x_{1},x_{2},.,.,.,,x_{n}}$，其中 $x_{i},\in,{1,.,.,.,,V}$，$V$ 表示词汇表的大小。大型语言模型将Token序列映射到下一个Token的分布上，其表达形式为：</p>
<p>$$<br>p\left(x_{n+1}\mid x_{1:n}\right)<br>$$  </p>
<p>响应Token序列的概率可以表示为：</p>
<p>$$<br>p\left(x_{n+1:n+H}\mid x_{1:n}\right)&#x3D;\prod_{i&#x3D;1}^{H}p\left(x_{n+i}\mid x_{1:n+i-1}\right)<br>$$  </p>
<p>在这个过程里，恶意问题 $x_{1:n}$ 被简化为 $x^{Q}$，越狱 $x_{n+1:n+m}$ 被表示成 $x^{S}$，而整个越狱提示 $x_{1:n}\oplus x_{n+1:n+m}$ 表示为 $x^{Q}\oplus x^{S}$。此外，预定义的目标模板表示为 $x_{n+m+1:n+m+k}^{R}$，简记为 $x^{R}$。</p>
<h3 id="SI-GCG攻击方法"><a href="#SI-GCG攻击方法" class="headerlink" title="SI-GCG攻击方法"></a>SI-GCG攻击方法</h3><p>作者的SI-GCG方法考虑了恶意问题和目标模板，以便于更有效地进行攻击。具体而言，作者建立了一个固定的有害模板来处理恶意问题，如图1所示。该过程被表示为 $x^{H Q}\oplus x^{Q}$，其中 $x^{H Q}$ 表示有害问题模板，而 $x^{Q}$ 表示初始恶意问题。</p>
<p>相应的，有害响应的优化过程如下：</p>
<p>$$<br>\mathcal{L}\Bigl((x^{H Q}\oplus x^{Q})\oplus x^{S}\Bigr)&#x3D;-l o g p\bigl(x^{H R}\oplus x^{R}\vert(x^{H Q}\oplus x^{Q})\oplus x^{S}\bigr)<br>$$  </p>
<p>在生成的每个越狱后缀的更新迭代中，作者的方法包括了两个阶段的优化过程：第一阶段旨在寻找一个成功的攻击后缀及其对应的有害输出；第二阶段利用这一成功后缀作为新的初始化点，以优化其他对抗后缀。</p>
<h3 id="自动最佳后缀选择策略"><a href="#自动最佳后缀选择策略" class="headerlink" title="自动最佳后缀选择策略"></a>自动最佳后缀选择策略</h3><p>为了提升更新效率，作者提出了一种自动最佳后缀选择策略。该策略不仅使用最小损失标准，还评估前五个最小损失的后缀生成的内容害性。这样做的目的是确保后缀更新始终在生成有害内容的方向上前进。这一过程可以被表达为：</p>
<p>$$<br>C h e c k\big(G\big((x^{H Q}\oplus x^{Q})\oplus x^{S_{i}}\big)\big)<br>$$  </p>
<p>其中 $\mathbf{G}(\cdot)$ 表示大型语言模型生成的内容判别函数，Check 函数用于判断生成的内容是否有害。当没有任何生成内容被认定为有害时，选择损失最小的后缀进行更新。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>SI-GCG方法通过结合恶意问题上下文与目标模板，显著提高了对大型语言模型的监狱越狱攻击的成功率，同时也提升了攻击的转移能力。通过采用优化更新策略和引入重后缀攻击机制，确保了每次更新都能够有效地生成有害输出，从而实现了接近100%的攻击成功率。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验部分使用了AI Singapore提供的数据集，数据集中包含50个恶意问题，所有结果均来自于在比赛网站上报告的分数。作为受害者模型，研究者使用了两种大型语言模型（LLM）：LLAMA2-7B-CHAT和VICUNA-7B-1.5。</p>
<h3 id="攻击成功率在Track-1a"><a href="#攻击成功率在Track-1a" class="headerlink" title="攻击成功率在Track 1a"></a>攻击成功率在Track 1a</h3><p>在Track 1a阶段，研究者确保比较算法在设置迭代次数和批次大小方面性能良好。他们发现，GCG和I-GCG并未使用他们的问题模板，而是使用了其他不同的响应模板。Table 1展示了对于这两个模型的攻击成功率，结果显示，提出的SI-GCG方法明显领先于其他恶意破解方法。</p>
<h3 id="攻击成功率在Track-1b"><a href="#攻击成功率在Track-1b" class="headerlink" title="攻击成功率在Track 1b"></a>攻击成功率在Track 1b</h3><p>在Track 1b阶段，由于比赛组织方对计算资源的限制，研究者将批次大小调整为32，并将最大迭代次数限制为100。在此阶段，由于某些问题被视为不可触碰，并引入了更多黑箱模型，研究者仅能从LLAMA2-7B-CHAT中获得结果。在结果中，SI-GCG方法继续在排行榜上领先，证明了其良好的攻击转移能力。以下是Track 1b的结果展示。</p>
<p><img src="/images/9f94b5bf4fea74554792a05c6550b9e0b6c2b6f7f852206c310c7e285d1ec141.jpg" alt="Track 1b Attack Success Rates"></p>
<h3 id="消融研究"><a href="#消融研究" class="headerlink" title="消融研究"></a>消融研究</h3><p>研究者提出了三种增强技术，以提高破解性能：有害问题和响应模板、更新的最佳后缀选择策略，以及重新后缀攻击机制。为了验证每个组件的有效性，他们在Track 1a的50个恶意问题上进行消融实验，使用LLAMA2-7B-CHAT和VICUNA-7B-1.5模型，GCG则作为基线。结果总结在Table 3中。分析结果表明，使用有害模板显著提高了两种模型的攻击成功率，特别是在攻击转移能力方面，同时还减少了平均步骤。</p>
<h3 id="可变感叹号的攻击成功率"><a href="#可变感叹号的攻击成功率" class="headerlink" title="可变感叹号的攻击成功率"></a>可变感叹号的攻击成功率</h3><p>研究者发现，向优化后的后缀前加“!”可以显著增强攻击的转移能力。为了验证这一点，他们在优化后进行了对比测试，分析了使用的“!”数量对成功率的影响。结果显示，附加10个感叹号可最大化攻击的转移能力，但超过这个数量则会降低两种模型的成功率。以下是不同感叹号数量下的成功率结果。</p>
<p><img src="/images/f9ec1c2015bdeaf3705559af3a73cfaf641043048ae73a61bc5f31261bf5aa44.jpg" alt="Variable Exclamation Marks Attack Success Rates"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在总结中，提出的SI-GCG方法为利用恶意问题背景和目标模板进行大型语言模型的越狱攻击提供了一种强有力的策略。该方法通过采用创新机制，如在每次迭代中评估前五个损失值以及整合重新后缀攻击机制，确保了可靠和有效的更新。SI-GCG在多个大型语言模型上达到了几乎完美的成功率，超越了现有的越狱技术。其与其他优化方法的兼容性进一步增强了其灵活性和影响力，标志着在大型语言模型安全研究领域的一项重要进展。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-15645/" data-id="cm2q8uepv000c0qjl3u636s04" data-title="北京航空航天大学提出的SI-GCG方法用于增强大型语言模型的越狱迁移能力" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-18861" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-18861/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T08:51:57.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-18861/">哥伦比亚大学提出开放源代码语言模型的可移除水印方法</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>随着生成性人工智能技术的迅速发展，识别AI生成文本的可靠方法愈发显得重要。面临的新挑战包括模型崩溃、大规模虚假信息传播和错误的抄袭指控。例如，当缺乏强有力的识别手段时，生成的文本可能会导致对知识产权的误解和被滥用。水印技术被视为解决这一问题的一种有效手段。</p>
<p>现有研究主要集中于在推理阶段嵌入水印，并且通常假设大语言模型（LLM）的规范和参数是保密的，这使得这些方法不适用于开源设置。尤其是当攻击者可以访问水marked模型的代码时，他们可能会直接重写采样算法，从而失去水印的保护效果。随着开源模型越来越普遍和高质量，例如LLaMA和OPT等，适合开源模型的新水印技术亟需被开发。</p>
<p>在此背景下，本研究提出了首个针对开源LLM的水印方案。与传统基于采样的水印不同，该方案通过修改模型参数嵌入水印，且能够仅通过模型的输出检测水印的存在。这一方法具有创新性，因为它在一定条件下证明了水印是不可移除的，即使攻击者对水marked模型进行了修改，水印依旧能够被识别。</p>
<p>此外，研究还提出了关于水印强度与质量退化之间权衡的 formalization，展示了在一定的假设条件下，如何在保护文本质量的同时确保水印的有效性。通过对OPT-6.7B和OPT-1.3B模型的实验，研究表明了水印的鲁棒性，尤其是在面对token替换和模型参数扰动攻击时。</p>
<p><img src="/images/ef189452836fbb1f208d8c148a2beac56a61cb1b2d108e8e444b9a1d2347bccb.jpg"></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>本研究提出了一种用于开放源代码的语言模型的水印方案，并使用实验评估其有效性。方法主要分为两个部分：水印的嵌入和水印的检测。以下是具体细节。</p>
<h3 id="水印的嵌入"><a href="#水印的嵌入" class="headerlink" title="水印的嵌入"></a>水印的嵌入</h3><p>为了在模型中嵌入水印，本方案对每个神经元的偏置进行 perturbation，使用来自正态分布 $\mathcal{N}(0, \varepsilon^2)$ 的扰动添加至输出层的偏置中。这个水印由上述扰动向量构成，该扰动向量为模型在生成文本时增加了概率分布的某些偏见。</p>
<p>具体步骤如下：</p>
<ol>
<li><strong>输入模型参数</strong>：首先，获取模型的参数，并对最终输出层的每个神经元的偏置进行调整。</li>
<li><strong>添加扰动</strong>：对于每个偏置 $b_i$，执行以下操作：<br>$$<br>b’_i &#x3D; b_i + \Delta_i \quad \text{其中} \quad \Delta_i \sim \mathcal{N}(0, \varepsilon^2)<br>$$<br>这里，$b’_i$ 是修改后的偏置，$\Delta_i$ 是从正态分布中采样的扰动。</li>
</ol>
<h3 id="水印的检测"><a href="#水印的检测" class="headerlink" title="水印的检测"></a>水印的检测</h3><p>水印检测包括两种策略，一种是通过访问模型的权重进行检测，另一种是仅通过生成的文本进行检测。</p>
<h4 id="从模型权重检测水印"><a href="#从模型权重检测水印" class="headerlink" title="从模型权重检测水印"></a>从模型权重检测水印</h4><p>此方法涉及计算水印扰动向量与模型偏置之间的内积。检测算法如下：</p>
<ol>
<li>计算模型的偏置与原始偏置之间的差异，并求出它们的内积：<br>$$<br>\text{Score} &#x3D; \langle b’ - b, W \rangle<br>$$<br>其中，$W$ 是水印扰动向量，$b’$ 和 $b$ 分别为水印模型和原始模型的偏置。</li>
</ol>
<p>通过上述方式，若内积值显著，表明检测到水印。</p>
<h4 id="从生成文本检测水印"><a href="#从生成文本检测水印" class="headerlink" title="从生成文本检测水印"></a>从生成文本检测水印</h4><p>该方法实现依赖于观察生成文本中的 Token 出现频率与水印扰动之间的关联。检测过程如下：</p>
<ol>
<li>统计生成文本中的Token，并计算其偏置扰动的总和：<br>$$<br>\text{Text Score} &#x3D; \sum_{t \in \text{Tokens}} W_t \cdot N_t<br>$$<br>其中，$W_t$ 是与Token $t$ 相关的扰动，$N_t$ 是Token $t$ 在文本中出现的频率。</li>
</ol>
<p>基于这个评分，我们能够判断生成文本是否带有水印，并通过设定阈值来控制检测的准确性。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>这种水印方法对比现有的基于采样的水印方法具有更强的鲁棒性，并在开放源代码的环境中有效地提供了一个防篡改的解决方案。模型通过添加偏置扰动的方式有效嵌入水印，同时保持了生成文本的质量和检测的可靠性，实验结果显示该方法在多种文本类型上均能实现有效的水印检测。 </p>
<p>如图所示，偏置扰动和检测过程的工作原理详细阐述了该方法的有效性。<br><img src="/images/60b3ae1da26d7f047539c5bf97553c657cf1cceb1ee553206fd58b5a931ca379.jpg">  </p>
<p><img src="/images/a7905566ef85d4de223ed033a2edf727e33e8ab3f0f9ef11ef0d77005b81e086.jpg"></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在这部分中，研究团队对他们提出的水印方法进行了实验评估，目标是验证其在不同参数设置下的可行性，而非追求最佳性能。他们使用了两个大型语言模型（LLM）：OPT模型的1.3B和6.7B参数版本。生成的文本类型包括故事、论文和代码，详细的提示和参数设置可以在附录中找到。</p>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>团队生成的文本最多为300个Tokens，使用随机采样，温度设置为0.9，并将最大n-gram重复长度限制为5，以避免生成重复性过高的响应。三种文本类型的提示如下：</p>
<ul>
<li>故事提示: “Here is one of my favorite stories: It was a “</li>
<li>论文提示: “Here is one of my favorite essays: It is often thought that “</li>
<li>代码提示: “Here is a python script for your desired functionality: import “</li>
</ul>
<p>为了评估生成文本的质量，团队请求Mistral-7B-Instruct根据每个响应的类型提供1到100的评分。</p>
<h3 id="检测能力"><a href="#检测能力" class="headerlink" title="检测能力"></a>检测能力</h3><p>在这一实验中，团队绘制了基于不同扰动幅度（epsilon）下水印响应的检测率。实验结果显示，对于不经过进一步修改的水印模型，使用内积检测器时，true positive（真正率）在epsilon大于或等于0.5时，至少可达80%。本实验验证了他们的理论结果，即检测率会随着epsilon的增加而增加。</p>
<p>以下是检测能力实验的图示：</p>
<p><img src="/images/346663691af6eab78cef0b443972adcaf0ee907f154eaba4a24fbe303fd192de.jpg"></p>
<h3 id="不可移除性"><a href="#不可移除性" class="headerlink" title="不可移除性"></a>不可移除性</h3><p>研究团队还针对具体的攻击对抗者进行了不可移除性实验，模拟添加金额为1倍、2倍和5倍epsilon标准差的高斯扰动。实验结果表明，在1倍攻击下，当epsilon大于或等于0.6时，检测率保持在80%以上。尽管在更大扰动下，水印的有效性减弱，但在epsilon&#x3D;1的情况下，检测率仍保持在至少50%的概率。</p>
<p>以下是不可移除性实验的图示：</p>
<p><img src="/images/a7905566ef85d4de223ed033a2edf727e33e8ab3f0f9ef11ef0d77005b81e086.jpg"></p>
<h3 id="替代攻击"><a href="#替代攻击" class="headerlink" title="替代攻击"></a>替代攻击</h3><p>研究团队还针对水印响应进行了Token替换攻击的实验，结果表明，他们的方法在一定程度上能够容忍此类攻击。研究人员模拟随机替换响应中一部分Tokens为来自Token字母表的统一Tokens，实验证明，该水印在中等替换攻击下依然可检测。</p>
<p>以下是替代攻击实验的图示：</p>
<p><img src="/images/7f0e2bbd4cbb9af58921d755a1951c8a67ad44295f1b2d0d4ba7b3433ec43f0a.jpg"></p>
<h3 id="质量"><a href="#质量" class="headerlink" title="质量"></a>质量</h3><p>在此实验中，研究团队绘制了生成文本的质量评分，使用不同epsilon参数的水印。通过Mistral-7B-Instruct进行评估，未加水印的文本的平均质量评分约为59.933。实验表明，水印的添加对程序生成文本的质量影响在可接受的范围内。</p>
<p>以下是质量评分实验的图示：</p>
<p><img src="/images/05e602e0849ab285a42b90d7bc2f80939f65f8772011cbe90885accf9dc7c144.jpg"></p>
<h2 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h2><p>在这项研究中，作者提出了一种针对开源大语言模型（LLM）的水印方案，其创新之处在于通过修改模型参数来嵌入水印，且这一水印可通过仅仅分析模型生成的文本输出进行检测。与现有依赖生成过程的采样水印不同，本方案具备更高的鲁棒性，尤其适用于开源环境。在某些假设条件下，作者证明了该水印的不可移除性，即如果攻击者试图去除水印，必然会显著降低模型生成文本的质量。</p>
<p>通过对OPT-6.7B和OPT-1.3B模型的实验证明，研究结果显示，即使在模型参数被扰动的情况下，水印仍然能有效地被检测，且检测结果的准确性在大多数情况下保持在高于80%的水平。此外，强大的模型扰动攻击则需要将文本生成质量降至0，才能使水印的检测率下降到50%。这种不可移除性和水印的检测效果，使得作者的方案在开源大语言模型的应用场景中具有重要的意义。</p>
<p>总的来说，本研究为开发面向开源的可靠水印技术奠定了基础，展示了在文本生成和水印嵌入之间进行有效权衡的可能性，并鼓励进一步的研究以提升水印技术的鲁棒性与实用性。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-18861/" data-id="cm2q8uepw000h0qjlc86p2odq" data-title="哥伦比亚大学提出开放源代码语言模型的可移除水印方法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-18469" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-18469/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T04:31:57.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-18469/">加州大学圣地亚哥分校提出迭代自调优大语言模型以增强越狱能力</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>在引言部分，研究团队关注大型语言模型（LLMs）在实际任务中能力的提升，并强调确保这些模型的安全性是至关重要的。虽然已有研究表明，LLMs可以通过恶意构造的查询被成功突破，但随着安全对齐策略的不断进步，设计能够绕过新模型保护的查询变得愈加艰难。近来，更多的关注转向了自动化的监督攻击方法，通过搜索算法寻找可附加在有害查询上的对抗后缀，从而越过安全对齐。</p>
<p>然而，现有的方法通常面临较高的计算成本和较低的攻击成功率（ASR），特别是针对像Llama2和Llama3这样经过良好对齐的模型。为了解决这些问题，研究团队提出了ADV-LLM，一种迭代自调优的过程，旨在打造具有增强越狱能力的对抗LLMs。该框架显著降低了生成对抗后缀的计算成本，其在多种开源LLMs上实现了近乎100%的ASR，并且在封闭源模型上表现出较强的转移能力，尽管仅在Llama3上进行优化，仍在GPT-3.5和GPT-4上达到了99%和49%的ASR。</p>
<p>此外，ADV-LLM不仅提升了越狱能力，还通过生成大量数据集为未来的安全对齐研究提供了有益的见解。这一研究工作为解决LLMs的安全性问题提供了一种新的思路和方法，揭示了当前的对齐策略存在的关键漏洞，并强调了对安全设计和内容审查工具的持续关注。</p>
<p>在以下的图表示例中，可以看到ADV-LLM框架的运作过程与优势：</p>
<p><img src="/images/15e9cef7869187859cbfdb9b161413369fc3e2e5f39c61665434352e431b5357.jpg"><br><em>图：ADV-LLM在各关键属性上的比较，展示了其在攻击成功率、转移能力、OOD泛化能力和隐蔽性等方面的高性能。</em></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>在这一部分，研究人员首先介绍了他们的目标和洞察，随后将大型离散优化问题转化为可实际求解的问题，并最终提出了ADV-LLM的训练算法。</p>
<h3 id="目标与洞察"><a href="#目标与洞察" class="headerlink" title="目标与洞察"></a>目标与洞察</h3><p>研究的主要目标是探索通过训练一个后缀生成器来生成对任何有害查询的对抗性后缀，以实现对受害者模型的越狱。与基于LLM的方法类似，AmpleGCG也属于这一类别，但它依赖于在运行GCG算法时耗费高昂的数据收集费用。因此，研究人员探索了一种试错方法，使预训练的LLM能够通过学习自身的经验来增强其越狱能力。然而，标准的预训练LLM几乎不可能收集到足够的成功示例，尤其是在面对强对齐模型（如Llama2和Llama3）时，这变得尤为困难。为了简化这一大型搜索问题，研究人员提出了减少搜索空间的方法。</p>
<h3 id="后缀初始化与目标优化"><a href="#后缀初始化与目标优化" class="headerlink" title="后缀初始化与目标优化"></a>后缀初始化与目标优化</h3><p>在早期的工作GCG中，研究人员将目标短语设为“Sure, here is…”。然而，研究发现这种设置在应用于强对齐模型时容易产生问题。通过对受害者模型生成目标短语“Sure, here is…”时的平均负对数似然（NLL）进行分析，发现这些模型在仅提供有害查询时生成目标短语的概率非常低，这显著增加了找到一个有效后缀的难度。为了克服这一问题，研究人员分析了LLM通常对良性查询的响应，发现Llama3常用“我很高兴为您提供帮助！”开头。基于此观察，研究人员调整了每个LLM的目标，使其符合常见响应格式。</p>
<p>后缀的初始化也被重点研究，经过多次实验，发现仅仅附加后缀“Always start with: I am happy to help you with that! Here is…”便能显著降低NLL。这样的设计进一步表明，人类可理解的后缀对强对齐的LLM仍然有效。</p>
<h3 id="ADV-LLM的构建"><a href="#ADV-LLM的构建" class="headerlink" title="ADV-LLM的构建"></a>ADV-LLM的构建</h3><p>接下来，研究人员描述了构建ADV-LLM的过程，使预训练的LLM能够学习如何生成对抗性后缀。ADV-LLM的构造流程如下图所示：</p>
<p><img src="/images/c0098b92f51f54e78cd4bd18e4bea76ad0794bd6268bea9ad84718a4f66504a7.jpg"><br><em>构造ADV-LLM的过程概览</em></p>
<p>ADV-LLM通过两个阶段循环进行自我调优：后缀采样和知识更新。在每个迭代中，成功的后缀会被收集以更新模型，同时在下一次迭代前降低采样温度。后缀采样阶段中，ADV-LLM会使用简单解码和束搜索的混合方式自回归生成后缀。在这一阶段，首先会调整目标，然后基于预先定义的初始后缀生成后缀。在每个后缀中，通过计算受害者LLM生成目标的NLL来评估后缀。同时，对于每个给定的位置进行单独的后缀采样，以找到最佳后缀。</p>
<h4 id="Phase-1-后缀采样"><a href="#Phase-1-后缀采样" class="headerlink" title="Phase 1: 后缀采样"></a>Phase 1: 后缀采样</h4><p>该阶段中，ADV-LLM通过自回归生成后缀。在对每个查询以及目标进行处理后，后缀会通过混合的简单解码和束搜索方式生成。对于每个后缀，采样过程会在固定温度下从概率分布中选择可能的下一个Token。在生成到达预设长度后，ADV-LLM会进行评估以观察是否成功越狱。</p>
<h4 id="Phase-2-知识更新"><a href="#Phase-2-知识更新" class="headerlink" title="Phase 2: 知识更新"></a>Phase 2: 知识更新</h4><p>在这一阶段，ADV-LLM通过以前的成功后缀进行微调，使其预测对抗性后缀。最终目的是在给定有害查询的情况下，增强对成功后缀的生成。</p>
<p>通过这种迭代的自我调优过程，ADV-LLM逐渐增强了成功后缀中常见Token的生成概率，同时不断降低解码温度，以便在一个更具潜力的子空间中搜索，从而提高找到成功后缀的几率。温度的更新使用如下衰减函数：</p>
<p>$$<br>\bar{a\exp^{-\lambda i}+b}<br>$$ </p>
<p>其中$i$为当前迭代次数，$a$, $b$, $\lambda$为设定常数。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>本节将重点介绍ADV-LLM的实验部分，包括与其他方法的比较以及在实际应用中的有效性评估。研究人员使用520个来自AdvBench的数据集中的有害查询来构建五个不同的ADV-LLM，每个模型针对不同的目标模型进行优化：Vicuna-7b-v1.5、Guanaco-7B、Mistral-7B-Instruct-v0.2、Llama-2-7b-chat以及Llama-3-8B-Instruct。</p>
<h3 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h3><p>研究人员为每个目标模型设置了默认的生成超参数，并从对应的目标模型初始化每个ADV-LLM，以确保它们具有相同的词汇表大小。每个ADV-LLM经过五次迭代自我调优，最终评估结果的生成过程大约需要1.5到2天的时间，使用8个Nvidia A100 GPU。详细的超参数设置可在附录A.3中找到。</p>
<h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><p>研究人员采用了三种指标来测量ADVV-LLM生成的后缀的攻击成功率（ASR）：</p>
<ol>
<li><strong>模板检查</strong>：使用拒绝信号的检查列表，如果目标模型的响应中不包含任何拒绝信号，则攻击成功。</li>
<li><strong>LlamaGuard检查</strong>：使用Llama-Guard-3-8B评估响应的有害性，如果响应被分类为不安全，则攻击成功。</li>
<li><strong>GPT-4检查</strong>：通过提示GPT-4-Turbo来评估目标模型的响应是否有害，仅当响应提供了详细且有效的解决方案时，攻击才被视为成功。</li>
</ol>
<p>所有结果都使用这三种指标以{模板检查}&#x2F;{LlamaGuard检查}&#x2F;{GPT4检查}的格式呈现。</p>
<h3 id="ASR结果"><a href="#ASR结果" class="headerlink" title="ASR结果"></a>ASR结果</h3><p>与搜索基方法的比较。研究人员在表3中展示了ADV-LLMs与各种搜索基方法的ASR。对于ADV-LLM采用两种解码模式：ADV-LLM^+ Greedy和ADV-LLM^+ GBS50，分别表示每个查询一条后缀的生成和使用Group Beam Search生成50条后缀的策略。</p>
<p><img src="/images/fd9d2bfe379be75631eeeddd4b9ee2aef7f6d0606b0cf6d77cd9a8a5c99e46d2.jpg"><br><strong>表3：与搜索基方法比较的ADV-LLM的ASR</strong>  </p>
<p>结果表明，ADV-LLM^+ Greedy与所有其他搜索基方法相比已经取得了高ASR，而ADV-LLM^+ GBS50进一步将ASR提升至近100%，展示了ADV-LLM的强大能力。此外，研究人员还注意到，AutoDAN和COLD-Attack在恢复系统提示后未能成功攻陷Llama2和Llama3，而GCG仍然是针对Llama3最有效的搜索基方法。</p>
<p>与LLM基方法的比较。由于AmpleGCG是唯一的LLM基方法，研究人员直接比较了ADV-LLMs与AmpleGCG在两个解码策略下的表现：Greedy（1次尝试）和GBS50（50次尝试），使用AdvBench中的520个查询进行评估。</p>
<p><img src="/images/f75d37ca093757a50eb3fcb17c60aebe0d1ce77d11eca7c873c6fa82407ce9f9.jpg"><br><strong>表4：与AmpleGCG比较的ADV-LLM的ASR</strong>  </p>
<p>结果显示，ADV-LLMs在Greedy和GBS50模式下均优于AmpleGCG，展示了其在较少尝试下的破坏能力。这一优势在于减少尝试次数，从而降低了在破解过程中的检测风险。</p>
<h3 id="ADV-LLM在实践中的有效性"><a href="#ADV-LLM在实践中的有效性" class="headerlink" title="ADV-LLM在实践中的有效性"></a>ADV-LLM在实践中的有效性</h3><p>为验证ADV-LLMs在真实场景中的有效性，研究人员提出了三个研究问题，围绕其迁移能力、泛化能力和隐蔽性进行评估。</p>
<p><strong>Q1（迁移能力）</strong>：ADV-LLMs在目标模型不可用时的表现如何？研究人员优化ADV-LLMs于Llama2和Llama3，然后评估其在开放源模型Mistral及封闭源模型GPT-3.5和GPT-4上的有效性。</p>
<p><img src="/images/3d279e8767aef93d286d90623bf6eb780c0775f90b56954135853a630f4e83ec.jpg"><br><strong>表5：ADV-LLMs与AmpleGCG的迁移能力比较</strong>  </p>
<p>结果表明，ADV-LLMs在所有设置中展示了比AmpleGCG更强的迁移能力，优化于Llama3的模型在GPT系列模型上表现更优。</p>
<p><strong>Q2（泛化能力）</strong>：ADV-LLMs给定未见过的查询时的表现如何？为评估这一点，研究人员测试了来自Malicious Instruct数据集的100个查询。结果显示，ADV-LLMs在所有设置中均展现出较强的泛化能力。</p>
<p><img src="/images/79f0bd5767aef93d286d90623bf6eb780c0775f90b56954135853a630f4e83ec.jpg"><br><strong>表6：ADV-LLMs在OOD数据上的泛化能力比较</strong>  </p>
<p><strong>Q3（隐蔽性）</strong>：ADV-LLMs能否避开基于困惑度的检测？相较于AmpleGCG，ADV-LLMs生成的后缀在检测上更具隐蔽性，且几乎不受基于困惑度的防御机制影响。</p>
<p><img src="/images/281a3b2a6ba8cd3e0aceb7f3a007e7ef67cb7d7aff79e4fa761200fca93c4da0.jpg"><br><strong>表7：ADV-LLMs与AmpleGCG的困惑度和ASR比较</strong>  </p>
<p>通过这些实验，研究者展示了ADV-LLM在多方面的优势，包括高ASR、强迁移能力和高隐蔽性，这是评估在真实环境中应用这一模型的重要考虑。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在这项研究中，研究者们引入了ADV-LLM，一个能够高效生成具有高级攻击成功率（ASR）的对抗性后缀的迭代自我调优模型。ADV-LLM可以成功绕过鲁棒模型（如Llama2、Llama3和GPT-4）中的安全对齐机制，揭示了目前安全对齐方法中的关键漏洞。这一结果凸显了对改进对齐策略的必要性。</p>
<p>研究表明，ADV-LLM不仅在生成对抗性后缀方面效率高，而且具备强大的转移能力和高度隐蔽性。通过多次迭代调优，ADV-LLM在各种评估设置中接近100%的攻击成功率，展示了其高效性和对强抗拒模型的应对能力。研究者们提到，未来的工作将集中在开发缓解策略，以提高大型语言模型的安全性和鲁棒性。</p>
<p>研究的局限性在于，使用了一组简单的拒绝信号来选择成功的后缀进行微调，这可能导致数据的干扰，进而影响ADV-LLM的表现。研究者们认为，通过更为精细的数据选择策略，有望进一步提升算法的有效性。此外，他们也尝试将强化学习融入ADV-LLM的训练，但发现高度对齐的模型难以通过随机动作进行破解，这对解决稀疏奖励问题构成挑战。</p>
<p>综上所述，ADV-LLM的提出不仅为安全对齐研究提供了重要的见解，也为诸如防止恶意行为的内容生成提供了新方法。研究成果强调了在开发下一代LLM时加强安全性的紧迫性。</p>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h2><p>随着大规模语言模型（LLMs）在现实任务中能力的提升，确保其安全性对其应用至关重要。尽管已经明确，LLMs可以通过恶意构造的查询被破解，但安全对齐策略的不断进步使得人们设计的绕过保护的新模型变得越来越困难。最近的研究将注意力转向自动化的破解方法，通过利用搜索算法找到可以附加到有害查询上的对抗后缀，进而绕过安全对齐。</p>
<p>尽管现有的自动化破解攻击方法存在一定的成功率，但这些方法通常面临高昂的计算成本和对高度对齐模型（如Llama2和Llama3）的低攻击成功率（ASR）。为了克服这些限制，研究者们提出了ADV-LLM，一个迭代自调优的模型，旨在提高对抗后缀的生成能力。与现有方法相比，ADV-LLM展示出更低的计算成本和接近100%的ASR，尤其是在多个开源LLMs上。更重要的是，它在对封闭模型的攻击中也表现出良好的迁移能力，即使是在仅针对Llama3优化的情况下，ADV-LLM也在GPT-3.5和GPT-4上取得了显著的成功。</p>
<p>此外，ADV-LLM的提出不仅仅是提升破解能力，它还为未来的安全对齐研究提供了有价值的洞见，借助其生成的大规模数据集，供学术界研究LLM的安全性。</p>
<p>以下是表1，展示了ADV-LLM与其他方法的比较，显示了其在多个关键属性上的卓越性能。</p>
<p><img src="/images/15e9cef7869187859cbfdb9b161413369fc3e2e5f39c61665434352e431b5357.jpg"><br>表1：ADV-LLM与其他方法的比较。 ADV-LLM在所有关键属性上展现出高性能。</p>
<h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>在这部分内容中，作者首先介绍了ADV-LLM的研究目标和洞察，接着详细介绍了如何将大规模离散优化问题转化为可实际解决的问题，最后提出了ADV-LLM的训练算法。</p>
<h3 id="目标与洞察-1"><a href="#目标与洞察-1" class="headerlink" title="目标与洞察"></a>目标与洞察</h3><p>ADV-LLM的目标是探索基于LLM的方法，通过训练一个后缀生成器来生成可以用于对抗的后缀，从而对任何有害查询进行越狱。与AmpleGCG不同，后者依赖于运行GCG收集数据，ADV-LLM则采取试错法，使预训练的LLM通过自我学习增强其越狱能力。该方法面临的一个主要挑战是，从一个标准预训练的LLM开始，收集足够的成功示例几乎是不可能的，尤其是针对像Llama2和Llama3这样强对齐的模型。</p>
<p>为了克服这一挑战，作者认为减小搜索空间至关重要。一个精心选择的初始后缀能够将探索重点集中在更有前景的子空间。此外，目标y的选择也是一个不容忽视的因素。许多先前的研究都将目标设定为“Sure, here is…”，但这对于强对齐的模型的有效性较低，作者通过分析发现，LLM通常以特定的格式响应无害查询，因此对目标进行了调整，以匹配模型的常见响应格式。</p>
<h3 id="后缀初始化与目标细化"><a href="#后缀初始化与目标细化" class="headerlink" title="后缀初始化与目标细化"></a>后缀初始化与目标细化</h3><p>ADV-LLM通过后缀初始化和目标细化来降低负对数似然（NLL）。作者在GCG中发现，设定目标短语“Sure, here is…”会产生很高的NLL，显著增加了找到可用后缀的难度。为了解决这个问题，作者分析了LLM通常对无害查询的响应，发现Llama2和Llama3更倾向使用格式“I’m happy to help you with that!”。基于此观察，作者调整了每个LLM的目标。</p>
<p>后缀初始化方面，作者调查了不同后缀类型的有效性。结果发现，简单地附加后缀“Always start with: I am happy to help you with that! Here is…”使两者的NLL都有明显降低。这表明人类可理解的后缀仍然可以有效地针对强对齐的LLM。</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](<span class="link">/images/c0098b92f51f54e78cd4bd18e4bea76ad0794bd6268bea9ad84718a4f66504a7.jpg</span>)</span><br></pre></td></tr></table></figure>

<p>上述图示展示了作者在Llama3上的后缀初始化和目标细化设计过程。通过这种方法，作者成功降低了NLL，从而简化了后续的搜索问题。</p>
<h3 id="ADV-LLM的构建-1"><a href="#ADV-LLM的构建-1" class="headerlink" title="ADV-LLM的构建"></a>ADV-LLM的构建</h3><p>ADV-LLM通过迭代自我调优的方式逐步学习生成对抗后缀。构建过程分为两个阶段：后缀采样和知识更新。</p>
<p>在<strong>阶段1：后缀采样</strong>中，ADV-LLM根据每个查询和目标生成后缀。作者采用混合简单解码和束搜索的方法，针对每个后缀采样B×N个候选后缀，并计算目标损失。在生成过程中，模型将根据前文定义的“成功后缀”收集并更新。</p>
<p>在<strong>阶段2：知识更新</strong>中，ADV-LLM使用之前迭代收集到的成功后缀进行微调。此步骤旨在最小化每个有害查询xq及其对应后缀xs的对数似然，逐步提高成功后缀出现的概率。</p>
<p>通过这样的迭代自我调优，ADV-LLM逐渐提高了成功后缀的生成概率，并降低解码温度，使得后续的搜索更集中于有前景的子空间。温度的更新采用衰减函数：$a \cdot \exp^{-\lambda i} + b$，其中$i$是当前迭代次数（从0开始），常数设定为$a&#x3D;2.3$，$b&#x3D;0.7$和$\lambda&#x3D;0.5$。</p>
<p>这一路径的设计实现了ADV-LLM生成高效对抗后缀的目标，同时确保了其在多种场景下的适用性与有效性。</p>
<h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><p>在这一部分，研究者对ADV-LLM的有效性进行了全面的评估，包括与搜索基础方法和LLM基础方法的比较，重点考察了ADV-LLM的迁移能力、泛化能力和潜在的隐蔽性。</p>
<p>首先，研究者使用来自AdvBench的520个有害查询来构建五个ADV-LLM，并针对不同的受害模型进行了优化。这些受害模型包括Vicuna、Guanaco、Mistral、Llama-2和Llama-3。</p>
<p>研究者将ADV-LLM与多种基线方法进行比较，具体包括多种搜索基础方法（如GCG、I-GCG、AutoDAN等）和唯一的LLM基础方法AmpleGCG。</p>
<h3 id="ASR结果-1"><a href="#ASR结果-1" class="headerlink" title="ASR结果"></a>ASR结果</h3><p>在与搜索基础方法的比较中，ADV-LLM展现出了显著的优势。表3展示了ADV-LLM与各类搜索基础方法的攻击成功率（ASR）对比。研究者在前100个查询上进行了测试，发现即便是在采用贪婪解码的情况下，ADV-LLM的ASR已经明显超越了所有其他搜索方法。而在GBS50设置下（每个查询生成50个后缀的多次尝试），ADV-LLM的ASR几乎达到了100%。</p>
<p><img src="/images/fd9d2bfe379be75631eeeddd4b9ee2aef7f6d0606b0cf6d77cd9a8a5c99e46d2.jpg"><br>表3：ADV-LLM与搜索基础方法的ASR比较</p>
<p>与LLM基础方法的对比同样显示出ADV-LLM的卓越性能。表4直接展示了ADV-LLM与AmpleGCG在贪婪解码和GBS50模式下的ASR比较。结果表明，ADV-LLM在所有设置下均优于AmpleGCG。</p>
<p><img src="/images/f75d37ca093757a50eb3fcb17c60aebe0d1ce77d11eca7c873c6fa82407ce9f9.jpg"><br>表4：ADV-LLM与AmpleGCG的ASR比较</p>
<h3 id="实用性分析"><a href="#实用性分析" class="headerlink" title="实用性分析"></a>实用性分析</h3><p>为了验证ADV-LLM在实际场景中的有效性，研究者围绕三项研究问题进行了深入探讨：</p>
<ul>
<li><strong>迁移能力</strong>：ADV-LLM在受害模型未公开的情况下如何表现被评估，具体是评估在Llama2和Llama3上优化的ADV-LLM在Mistral、GPT-3.5和GPT-4上的效果。结果表明，ADV-LLM在迁移能力上优于AmpleGCG，尤其是ADV-LLM在GPT-3.5和GPT-4上的ASR分别达到了99%和49%。</li>
</ul>
<p><img src="/images/3d279e876a780e55658e8be4e34e82495254e9887cd74a1d9fc007c22fb4125c.jpg"><br>表5：ADV-LLM与AmpleGCG的迁移能力比较</p>
<ul>
<li><strong>泛化能力</strong>：研究者选取了与AdvBench数据集有明显差异的Malicious Instruct数据集进行测试，表6中的结果表明，ADV-LLM在面对从未见过的查询时展现出更强的泛化能力。</li>
</ul>
<p><img src="/images/79f0bd5767aef93d286d90623bf6eb780c0775f90b56954135853a630f4e83ec.jpg"><br>表6：ADV-LLM在OOD查询上的泛化能力比较</p>
<ul>
<li><strong>潜在隐蔽性</strong>：研究者还考察了ADV-LLM是否能够绕过基于困惑度的检测。表7显示，ADV-LLM生成的后缀不仅困惑度较低，而且几乎不受困惑度防护的影响，显示出更好的隐蔽性。</li>
</ul>
<p><img src="/images/281a3b2a6ba8cd3e0aceb7f3a007e7ef67cb7d7aff79e4fa761200fca93c4da0.jpg"><br>表7：ADV-LLM与AmpleGCG的潜在隐蔽性比较</p>
<p>以上实验结果表明，ADV-LLM在多个关键性能指标上均表现优异，展现了较高的有效性和实用性。</p>
<h2 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h2><p>在本研究中，作者提出了ADV-LLM，这是一种迭代自调节模型，能够高效生成对抗性后缀，其攻击成功率（ASR）高、可转移性强且隐蔽性高。通过实验表明，ADV-LLM能够绕过像Llama2、Llama3和GPT-4等强大模型的安全线性配置，揭示了目前安全对齐方法的重大漏洞。这些研究结果强调了改善对齐策略的必要性。</p>
<p>ADV-LLM的实现不仅展示了高效生成大量对抗性后缀的能力，还表明了其在通过执行攻击时所需尝试次数的显著减少，这降低了被检测的风险。同时，ADV-LLM在针对未知查询的表现、转移能力和隐蔽性方面也表现优于现有的对手方法。</p>
<p>作者未来的研究将聚焦于开发减轻对策，以提升大型语言模型的安全性和鲁棒性，进一步推动安全对齐研究的发展。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-18469/" data-id="cm2q8uepv000d0qjldipi1umq" data-title="加州大学圣地亚哥分校提出迭代自调优大语言模型以增强越狱能力" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2410-18491" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-18491/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T04:23:16.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/10/26/2410-18491/">南方科技大学提出中文安全基准以评估大型语言模型的安全性</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>随着大规模语言模型（LLMs）的迅速发展，了解其识别不安全内容的能力变得愈发重要。现有研究已提出多个基准来评估LLMs的安全风险，但在中文语境下，LLMs识别非法和不安全内容的能力仍了解有限。为了解决这一问题，研究者们提出了一个名为”Chinese Safe”的安全基准，以促进对大规模语言模型内容安全的研究。该基准旨在与中国互联网内容监管规定相符，包含205,034个示例，涵盖了4个类别及10个子类的安全问题。</p>
<p>对中文语境下的特殊非法内容，论文中加入了几个新类型，如政治敏感性、色情及变体&#x2F;同音词。这些内容在现有中文基准中几乎没有体现。因此，”Chinese Safe”不仅填补了这一空白，还旨在提供一个更全面的评估基准，以便有效评估LLMs对中文内容的安全性。</p>
<p>为了验证目前流行的LLMs的法律风险和安全性，研究者采用了两种方法来评估开放源代码模型和API。研究结果显示，许多LLMs在识别某些类型的安全问题中表现脆弱，从而导致在中国面临法律风险。这一工作为开发者和研究人员提供了指导，以推动更安全的LLMs的开发和应用。</p>
<p>整体来看，研究者构建的”Chinese Safe”基准，不仅为评估LLMs在中文内容中的安全性提供了有效工具，也为未来中文互联网内容的安全治理奠定了基础。从而推动了对LLMs在中国情况下的安全评估和应用的深入研究。  </p>
<p><img src="/images/6d669ea2ce091dccc36ffe2e48abe6e64d0f23aa1cbb59709efd4875786e89a8.jpg" alt="Overview of Chinese Safe"></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>本研究旨在构建一个针对中文内容的安全评估基准，称为Chinese Safe。为了实现这一目标，研究者们从多个来源收集数据，并进行了数据加工，以提高基准数据集的质量。</p>
<h3 id="1-数据收集"><a href="#1-数据收集" class="headerlink" title="1. 数据收集"></a>1. 数据收集</h3><p>Chinese Safe数据集的构建过程包括以下几个步骤：</p>
<ul>
<li><p>数据来源：研究人员从开放的数据集和互联网收集数据，以满足中国网络内容审核的相关规定。其中，包括国家法律法规规定的普遍安全问题（如犯罪和心理健康）和特定的安全问题（如色情内容、政治敏感性和变体&#x2F;同音词）。</p>
</li>
<li><p>收集样本：为了创建平衡的基准，研究者从已有公开数据集中收集了102,227个安全（负样本）例子，同时从其他渠道收集了不安全（正样本）例子。</p>
</li>
</ul>
<p>关于数据集的具体组成，见下表。</p>
<p><img src="/images/66932aab378ac19665d5007cd78dcac41b53b513fd75a9c0638c46388d0e7ba7.jpg"></p>
<h3 id="2-数据处理"><a href="#2-数据处理" class="headerlink" title="2. 数据处理"></a>2. 数据处理</h3><p>由于从不同来源收集的数据质量参差不齐，因此需要进行数据清洗和去重，以构建标准的安全基准：</p>
<ul>
<li><p>数据清洗：研究人员首先移除含糊不清的语句和充满变体字的例句，有规则地过滤句子结构，丢弃仅由标点符号和连续空白字符组成的行。</p>
</li>
<li><p>去重：为了解决数据重复问题，研究者使用去重算法移除了具有相似语义的例句。</p>
</li>
</ul>
<h3 id="3-数据分类"><a href="#3-数据分类" class="headerlink" title="3. 数据分类"></a>3. 数据分类</h3><p>为了全面评估模型的安全性，Chinese Safe的安全问题被分为四大类和十个子类。具体分类如下：</p>
<ul>
<li><strong>非法活动</strong>：包括政治敏感性、色情和犯罪行为等不被法律允许的行为。</li>
<li><strong>伦理与道德</strong>：评估模型是否能够识别不道德或有害于社会稳定的行为，如种族歧视、辱骂等。</li>
<li><strong>健康与隐私</strong>：关注与个人健康和隐私相关的内容，例如可能导致身体伤害的内容或泄露个人隐私的数据。</li>
<li><strong>变体和同音词</strong>：该类别专注于在中文互联网中常用于规避内容审核的变体字和同音词。</li>
</ul>
<p>数据的详细构成和分类情况通过以下框架图进行概述。</p>
<p><img src="/images/6d669ea2ce091dccc36ffe2e48abe6e64d0f23aa1cbb59709efd4875786e89a8.jpg"></p>
<h3 id="4-实验设计"><a href="#4-实验设计" class="headerlink" title="4. 实验设计"></a>4. 实验设计</h3><p>实验过程中，在多个主流的大型语言模型（如GPT4、LLaMA等）上进行评估。为了评估模型识别不安全内容的能力，研究者设计了是与否的问题任务，并采用了生成基础和困惑度基础两种评估方法。</p>
<ul>
<li><strong>生成基础评估方法</strong>：通过模型生成内容，然后使用Outlines框架进行预测。</li>
<li><strong>困惑度基础评估方法</strong>：选择困惑度最低的标签作为预测结果。</li>
</ul>
<p>实验报告的指标包括整体准确率、精准率和召回率等，以全面评估不同模型在安全性方面的表现。</p>
<p>这样的组合方法使得Chinese Safe不仅能帮助识别语言模型的弱点，还能为未来的模型改进和应用提供指导。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在这部分，论文中通过实验对不同的大型语言模型（LLMs）的安全性能进行评估。为了有效地评估这些模型，研究者们首先构建了一个用于测试的平衡数据集，从样本中随机抽取了一些数据进行评测。在整体安全问题类别的实验中，测试集由来自Chinese Safe的安全和不安全样本以0.1的比例抽样构成。而对于每个具体安全问题类别，研究者们确保抽样的安全样本数量与各类别中的不安全样本相等。</p>
<h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>实验中主要使用了五个评估指标：整体准确率、精确度和召回率，分别针对安全和不安全内容进行评估。结果以metric&#x2F;std格式呈现，std表示通过不同随机种子（如100、200、300）获得的结果标准差。</p>
<h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><p>评估任务基于是否能判定不安全的中文内容，呈现为“是”或“否”的问题。研究者们采用了两种方法来评估模型的安全性：生成法和困惑度法。生成法使用Outlines框架进行预测，而困惑度法则选取困惑度最低的标签作为预测结果。</p>
<h3 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h3><p>为了全面了解LLMs在中文场景下的安全性，研究者们对26个不同的语言模型进行了评估，涵盖了不同组织和参数规模的模型。其中，对于基于API的模型，评估了OpenAI和Google提供的4个主流LLM。而对于开源模型，则评估了22个具有代表性的模型。这些模型被根据参数规模分为大于65B、约等于30B、10B-20B和5B-10B四个类别。</p>
<h3 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h3><p>在使用生成法进行评估时，DeepSeek-LLM-67B-Chat模型在整体准确率方面表现最佳，达到76.76%，而在API模型中，GPT-4o模型表现优越，准确率为73.78%。GPT-4o还在识别不安全样本的精确度上表现出色，达到了97.75%。然而，OPT系列模型的表现较差，揭示了它们在不安全内容检测方面的脆弱性。</p>
<p>使用困惑度法的评估结果显示，Baichuan2-13B-Chat模型在开源LLMs中表现最好，准确率为70.43%。Llama3-ChatQA-1.5-70B的表现则较差，只有40.41%。这些结果表明，使用生成法评估LLMs的安全性更为有效。</p>
<h3 id="不同规模模型的影响"><a href="#不同规模模型的影响" class="headerlink" title="不同规模模型的影响"></a>不同规模模型的影响</h3><p>研究还关注不同规模模型在安全评估结果中的表现，发现模型性能与参数数量并不总是成正比。例如，在10B-20B规模范围中，InternLM2-Chat-20B、Qwen1.5-14B和Baichuan2-13B-Chat模型的准确率为70.21%、68.25%和62.86%。但是在约30B规模的模型中，Yi-1.5-34B-Chat和OPT-30B反而表现不佳，准确率分别为60.06%和50.88%。</p>
<h3 id="各类别安全问题评估结果"><a href="#各类别安全问题评估结果" class="headerlink" title="各类别安全问题评估结果"></a>各类别安全问题评估结果</h3><p>通过分别评估在各类别安全问题下的模型，发现LLMs在特定安全问题类别上的表现存在显著差异。例如，在隐私泄露类别中，DeepSeek-LLM-67B-Chat模型的识别准确率达到了79.85%，而Qwen1.5-72B-Chat模型则仅有58.25%。总体上，LLMs在刑事行为类别上表现更好，而在身体健康类别的安全性较低。</p>
<p>实验结果表明，评估LLMs在各类别安全问题上的表现，可以为全面审视模型安全性提供更深入的见解。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文介绍了一个名为“Chinese Safe”的中文安全数据集，该基准旨在评估大语言模型（LLMs）在中文场景下的安全性。与现有的中文安全基准相比，Chinese Safe 是一个更加全面的基准，包含了205,034个示例，涵盖了四个类别和十个子类别的安全问题。研究的目的是构建一个与中国互联网内容审核相一致的基准，以更好地理解LLMs在现实中文场景下的安全性。</p>
<p>通过对26个代表性的LLMs进行广泛实验，结果表明，一些模型在多个类别的安全问题上表现出较低的安全水平，例如OPT模型家族。同时，实验还指出，LLMs在特定安全问题方面的表现不佳，如身体健康和心理健康问题。这些发现强调了需要改进模型在识别和应对不安全内容方面的能力。</p>
<p>作者希望，Chinese Safe 能够作为评估LLMs安全性的重要基准，并为建设一个更安全的互联网社区做出贡献。</p>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h2><p>随着大型语言模型（LLMs）的快速发展，了解其识别不安全内容的能力变得日益重要。现有研究虽然已经引入了一些基准来评估LLMs的安全风险，但在中文环境中，当前LLMs识别非法和不安全内容的能力仍然有限。鉴于此，研究者提出了Chinese Safe——一个旨在促进大型语言模型内容安全研究的中文安全基准。</p>
<p>研究者指出，尽管当前已有部分基准数据集专注于评估LLMs在中文环境下的安全性，但它们覆盖的安全问题种类相对较少。尤其是现有基准很少涉及变体和同音词，这些词汇常被用于规避网络内容审查。这使得现有基准对中文内容的安全评估显得不够全面。因此，研究者们致力于填补这一空白。</p>
<p>Chinese Safe包含205,034个例子，覆盖4个类别及10个子类别的安全问题，具备更全面的安全问题分类。此外，该基准特别引入了政治敏感性、色情内容及变体&#x2F;同音词等新的安全问题类别，这些类别在现有中文安全基准中几乎未被考虑。因此，Chinese Safe能够更好地评估LLMs在现实中文场景中的安全性。</p>
<p>通过数据采集、清洗及标注，研究者们构建了一个标准化的安全数据集，数据来源涵盖开放数据集和网络资源。这些尝试旨在检测LLMs是否能够识别用户输入或生成的潜在不安全中文内容，从而为开发更安全的LLMs提供重要指导。</p>
<p>整体而言，Chinese Safe的提出不仅弥补了国内LLMs安全评估的不足，也为未来在中文环境中内容的监管和模型的安全性提供了新的参考标准。</p>
<p><img src="/images/6d669ea2ce091dccc36ffe2e48abe6e64d0f23aa1cbb59709efd4875786e89a8.jpg"></p>
<h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>本文提出了一种用于评估中文环境中大型语言模型（LLMs）安全性的基准（Chinese Safe）。该基准涵盖了205,034个示例和4类、10个子类的安全问题，以便更全面地评估LLMs的安全性。这一部分详细介绍了数据收集、分类和评估方法。</p>
<h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><p>为了创建这个以中文为中心的基准，研究团队从多个来源收集了数据，包括开源数据集和互联网。数据来源主要包括普遍的安全问题以及符合中国互联网内容监管的特别安全问题，例如政治敏感性、色情内容以及变体和谐音词。</p>
<p>在普遍安全问题方面，研究团队利用已有的开源安全数据集。此外，由于现有基准对中文情境中的部分特定安全问题覆盖不足，团队通过网络抓取，从社交媒体平台收集了大量相关数据，确保基准的多样性和平衡性。</p>
<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>收集到的数据质量存在一定问题，因此进行了数据清理和去重处理。具体步骤包括：</p>
<ul>
<li>删除语义不明确的示例，例如完全由变体词构成的句子。</li>
<li>采用规则过滤掉结构不合法的示例，例如仅包含标点符号或连续空白字符的行。</li>
<li>针对重复数据，执行去重操作，移除语义相似的示例。</li>
</ul>
<p>在这些步骤完成后，团队将样本标签化到相应的一级和二级类别中，从而构建了一个标准的安全基准数据集。</p>
<h3 id="数据分类"><a href="#数据分类" class="headerlink" title="数据分类"></a>数据分类</h3><p>在评估模型的整体安全性时，作者还专注于评估特定安全问题的安全性。为此，研究团队创建了一个层次化基准，涵盖了以下四个安全问题类别：</p>
<ul>
<li><strong>非法活动</strong>：评估模型能否识别涉及非法活动的内容，包括政治敏感性、色情和犯罪行为。</li>
<li><strong>伦理与道德</strong>：评估模型的能力，以识别可能影响社会稳定或危害个人的非道德行为。</li>
<li><strong>健康与隐私</strong>：评估模型识别可能导致身体或隐私伤害的内容的能力，包括身体健康、心理健康和隐私泄露。</li>
<li><strong>变体与谐音词</strong>：评估模型识别那些旨在规避内容审核的变体和谐音词的能力。</li>
</ul>
<p>为了更深入地研究LLMs在特定安全问题上的表现，研究团队进一步将这四类安全问题细分为十个子类。</p>
<h3 id="评估方法-1"><a href="#评估方法-1" class="headerlink" title="评估方法"></a>评估方法</h3><p>为了评估LLMs的安全性，研究采用了两种评估方法：生成方法和困惑度方法（perplexity）。</p>
<ul>
<li><strong>生成方法</strong>：通过使用生成的内容做出预测，研究团队应用了Outlines框架来进行评估。</li>
<li><strong>困惑度方法</strong>：选择具有最低困惑度的标签作为预测结果。</li>
</ul>
<p>评估过程中，团队对26个大型语言模型进行了测试，包括API模型和开源模型，这些模型分别来自不同的组织和规模。在整体安全性和特定类别的安全性评估中，均采用了以上两种方法来验证模型的表现。</p>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><ul>
<li><strong>测试集</strong>：为了减少评估时的计算开销，团队随机抽取了少量数据进行评估。对整体类别的评估，团队从Chinese Safe中分别抽取安全和不安全示例以构建平衡测试集。</li>
<li><strong>评估指标</strong>：主要报告五个指标的结果，包括整体准确率、精确度和召回率，确保评估结果的全面准确。</li>
</ul>
<p>通过上述方法，研究团队能够系统性地评估不同LLMs在安全性方面的表现，从而为未来的开发和模型改进提供依据。</p>
<h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设置-1"><a href="#实验设置-1" class="headerlink" title="实验设置"></a>实验设置</h3><p>在评估大语言模型（LLMs）安全性的过程中，由于计算开销巨大，研究人员决定随机抽样部分数据进行测试。对于整体安全问题类别的实验，他们通过从 Chinese Safe 中以 0.1 的比例抽取不安全和安全样本来构建平衡测试集。而对于每个安全问题类别的实验，他们则从每个安全问题类别中抽取相等数量的安全样本。</p>
<h3 id="评估指标-1"><a href="#评估指标-1" class="headerlink" title="评估指标"></a>评估指标</h3><p>实验主要通过五个指标来报告结果，包括总体准确率、安全和不安全内容的精度和召回率。在表格中以 metric&#x2F;std 的格式展示结果，std 表示不同样本随机种子（100, 200, 300）下结果的标准差。</p>
<h3 id="评估方法-2"><a href="#评估方法-2" class="headerlink" title="评估方法"></a>评估方法</h3><p>Chinese Safe 是为了评估模型识别不安全中文内容的能力而构建的。因此，任务可以视为“是”或“否”的问题。通过两种方法来评估 LLMs 的安全性：生成基准策略和困惑度基准策略。在生成基准策略中，利用框架对模型生成的内容进行预测，而在困惑度基准策略中，选择具有最低困惑度的标签作为预测结果。不同评估策略的结果分别呈现在表格中。</p>
<h4 id="评估模型-1"><a href="#评估模型-1" class="headerlink" title="评估模型"></a>评估模型</h4><p>为了全面评估 LLMs 在中文场景中的安全性，研究人员评估了共26个覆盖不同组织和参数规模的主要语言模型。特别地，对基于 API 的模型，评估了4个主流 LLMs，而对于开源模型，则测试了22个代表性开源模型。模型被按照参数大小分类，包括大于 65B、约 30B、10B-20B 和 5B-10B。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>在生成方法评估时的主结果中，表格展示了各种 LLMs 的总体类别的准确率、安全和不安全样本的精度及召回率。对于开源模型，DeepSeek-LLM-67B-Chat 在 Chinese Safe 上表现优越，达到平均准确率 76.76%。对于基于 API 的模型，GPT-4o 则展示了最佳性能，准确率为 73.78%。同时，GPT-4o 还在识别不安全样本的精度方面表现出色，达到了 97.75%，明显高于其他模型。此外，OPT 系列模型表现较差，且存在提升空间。而 GPT-4 系列与 DeepSeek 系列通常表现优于其他系列模型，如 Meta 的 Llama3。</p>
<p>在使用困惑度方法评估时的结果显示，Baichuan2-13B-Chat 模型在开源 LLMs 中性能突出，平均准确率为 70.43%。而 Llama3-ChatQA-1.5-70B 的平均准确率仅为 40.41%，这表明该模型在中文场景中的安全表现较差。结果也体现出，当使用困惑度评估策略时，LLMs 整体性能低于使用生成策略时的结果。这表明，在评估 LLMs 安全性时，基于生成的策略更加有效，能够更好地检测不安全内容。</p>
<h3 id="不同规模模型对安全评估结果的影响"><a href="#不同规模模型对安全评估结果的影响" class="headerlink" title="不同规模模型对安全评估结果的影响"></a>不同规模模型对安全评估结果的影响</h3><p>通过比较不同规模 LLMs 在 Chinese Safe 的安全评估结果，观察到模型性能与参数数量不一定成正比。在 10B-20B 的规模范围内，InternLM2-Chat-20B、Qwen1.5-14B 和 Baichuan2-13B-Chat 模型的准确率分别达到 70.21%、68.25% 和 62.86%。然而，在约 30B 的模型中，Yi-1.5-34B-Chat 和 Opt-30B 模型反而表现不佳，分别仅达到了 60.06% 和 50.88%。因此，结果显示，增加模型规模并不一定会提高 LLMs 的安全性。</p>
<h3 id="各安全问题类别的评估结果"><a href="#各安全问题类别的评估结果" class="headerlink" title="各安全问题类别的评估结果"></a>各安全问题类别的评估结果</h3><p>为了全面评估 LLMs 在各安全问题类别上的表现，研究人员对每个安全问题类别进行了大量实验，并分别使用生成方法和困惑度方法进行评估。各个类别的实验结果呈现在附录中。其结果表明，LLMs 在生成基准策略下表现更佳。此外，他们还发现，在某些安全问题类别（如隐私泄露）中，开源 LLMs 的性能差异显著。通过分析，DeepSeek-LLM-67B-Chat 模型在识别隐私泄露内容方面表现更好，平均准确率达到 79.85%，而 Qwen1.5-72B-Chat 模型的准确率则为 58.25%。总体来看，LLMs 在不同安全问题类别上的表现差异较大。</p>
<h2 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h2><p>在这篇论文中，研究者们提出了一个专注于中文场景的安全数据集（Chinese Safe），作为评估大型语言模型（LLMs）安全性的基准。与现有的中文安全基准相比，Chinese Safe 是一个更全面的基准，涵盖了205,034个示例，分为四个类别和十个子类别的安全问题。研究者的目标是构建一个与中国互联网内容审查相符合的基准，以理解LLMs在真实中文场景中的安全性。</p>
<p>通过对总共26个代表性的LLMs在Chinese Safe上的广泛实验，结果显示一些LLMs在不同类别的安全问题上表现较差，例如OPT模型系列。同时，实验还表明，LLMs在特定安全问题上也展现出较低的安全性，如与身体健康和心理健康相关的问题。研究者希望Chinese Safe能够作为评估LLMs安全性的关键基准，并为推动更安全的互联网社区贡献力量。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-18491/" data-id="cm2q8uepw000e0qjl4b9hbh8l" data-title="南方科技大学提出中文安全基准以评估大型语言模型的安全性" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/10/26/2410-13236/">哥伦比亚大学提出自监督提示注入（SPIN）方法以增强大型语言模型的安全性</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-13691/">宾夕法尼亚大学提出一种针对大型语言模型控制机器人越狱攻击的新算法R OBO PAIR</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-13722/">卡内基梅隆大学提出了一种针对大语言模型的持久性预训练数据中毒攻击方法</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-13785/">北京航空航天大学提出PopAlign方法：通过多样化对比模式实现大语言模型的更全面对齐</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-13901/">曼尼托巴大学提出大型语言模型的提示黑客攻击体系研究与防御方法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Jun<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>