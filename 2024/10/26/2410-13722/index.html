<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>卡内基梅隆大学提出了一种针对大语言模型的持久性预训练数据中毒攻击方法 | 安全汪 AnQuanWang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="MathJax &#x3D; {   tex: {     inlineMath: [[&#39;$&#39;, &#39;$&#39;], [&#39;\(&#39;, &#39;\)&#39;]]   } };        动机随着大型语言模型（LLMs）在各个领域的广泛应用，其训练和优化过程中所使用的数据集的质量变得愈发重要。大多数LLMs是基于从互联网抓取的非结构化文本进行预训练，这些数据集通常包含数万亿个Token。然而，互联网的本质是不可信的，任何人都可">
<meta property="og:type" content="article">
<meta property="og:title" content="卡内基梅隆大学提出了一种针对大语言模型的持久性预训练数据中毒攻击方法">
<meta property="og:url" content="http://example.com/2024/10/26/2410-13722/index.html">
<meta property="og:site_name" content="安全汪 AnQuanWang">
<meta property="og:description" content="MathJax &#x3D; {   tex: {     inlineMath: [[&#39;$&#39;, &#39;$&#39;], [&#39;\(&#39;, &#39;\)&#39;]]   } };        动机随着大型语言模型（LLMs）在各个领域的广泛应用，其训练和优化过程中所使用的数据集的质量变得愈发重要。大多数LLMs是基于从互联网抓取的非结构化文本进行预训练，这些数据集通常包含数万亿个Token。然而，互联网的本质是不可信的，任何人都可">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/c62c87de6f0a1d81538cb21f404c3c2b3613238b1a78b84e616069727f1eec1d.jpg">
<meta property="og:image" content="http://example.com/images/3d70eeef4bae3ed88a9ee8324f518aad508fe8083092d23f17b011e3dddbeceb.jpg">
<meta property="og:image" content="http://example.com/images/ad06102ade1912f56e7eaee3ffaf57e02d39ccb451dc4ae5eaf98196d4c6d717.jpg">
<meta property="og:image" content="http://example.com/images/322612b90ba0fb170fe0c569696a3c41decb66c198bb8b81d6911359ced7f5ca.jpg">
<meta property="og:image" content="http://example.com/images/d73ab850da0ebac62600b5cf78aef5bdbc429341036246830319cf0abf12667c.jpg">
<meta property="og:image" content="http://example.com/images/eb082c53d4f5207f7ea3f9b80b88b40b19a956b272e2234c58e917a4f2632ca1.jpg">
<meta property="article:published_time" content="2024-10-26T13:32:09.000Z">
<meta property="article:modified_time" content="2024-10-26T13:32:09.822Z">
<meta property="article:author" content="Jun">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/c62c87de6f0a1d81538cb21f404c3c2b3613238b1a78b84e616069727f1eec1d.jpg">
  
    <link rel="alternate" href="/atom.xml" title="安全汪 AnQuanWang" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">安全汪 AnQuanWang</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一只每天自动跟踪和解读大模型安全领域论文的汪，所有内容均由AI生成</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2410-13722" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-13722/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T13:32:09.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      卡内基梅隆大学提出了一种针对大语言模型的持久性预训练数据中毒攻击方法
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>随着大型语言模型（LLMs）在各个领域的广泛应用，其训练和优化过程中所使用的数据集的质量变得愈发重要。大多数LLMs是基于从互联网抓取的非结构化文本进行预训练，这些数据集通常包含数万亿个Token。然而，互联网的本质是不可信的，任何人都可以编辑内容或发布任意数据，这使得模型在训练过程中容易受到恶意行为者的攻击。先前的研究表明，恶意攻击者能够通过操控数据集而影响模型的训练结果，尤其是在细化训练（fine-tuning）阶段。</p>
<p>本研究的动机主要有两个方面：首先，尽管先前的研究显示大规模的数据集可以被恶意地污染，但目前尚不清楚攻击者是否能在预训练阶段操控模型的行为。其次，了解预训练阶段的掺毒攻击是否会在后续的细化训练阶段中持续存在，对于构建安全和可靠的语言模型至关重要。本文首次评估了在仅控制0.1%预训练数据的情况下，恶意行为者如何能够影响LLMs的行为，并探讨这种影响在后续的模型细化过程中是否会持续存在。</p>
<p>创新点方面，研究提出了四种攻击目标，包括服务拒绝（denial-of-service）、信念操控（belief manipulation）、越狱（jail breaking）和提示偷窃（prompt stealing），并分别在多种模型尺寸（从600M到7B的参数规模）下进行了评估。研究结果表明，预训练数据集中的0.1%掺毒即可导致三种攻击在后期细化训练后仍然有效，而一些简单的攻击，如服务拒绝，只需0.001%的掺毒率便可持续存在。这一发现意味着即便是微小比例的数据污染，也能够引发持久的模型行为改变，给模型的安全性带来了重大挑战。</p>
<p><img src="/images/c62c87de6f0a1d81538cb21f404c3c2b3613238b1a78b84e616069727f1eec1d.jpg" alt="Poisoning effects persist in deployed chatbots">  </p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="模型架构和训练"><a href="#模型架构和训练" class="headerlink" title="模型架构和训练"></a>模型架构和训练</h3><p>在本研究中，研究人员使用了官方的OLMo代码库，复现了最先进的开源大语言模型（LLM）预训练管道。他们选用了默认的1B和7B架构，并通过调整隐藏层维度和层数创建了604M、2B和4B（非嵌入）参数的自定义架构。模型配置的详细信息见于附录B.1。</p>
<p>在预训练阶段，使用大小约为1000亿个Token的预训练数据集，这些数据集采样自Dolma，代表了OLMo模型所使用的原始数据混合。该数据集大小大约占总数据集的5%。尽管减少预训练数据集会对模型的整体能力产生影响，但研究人员评估的结果表明，这种影响微乎其微，使得这些模型可以被视为完全训练模型的合理近似。</p>
<p>在后处理阶段，研究人员遵循Llama-3的后处理方案，首先对Open Assistant数据集进行监督微调（SFT），以增强模型的有用性，并在HH-RLHF数据集上确保模型的安全性。随后，研究人员对同一数据集应用了基于偏好的优化（DPO）方法，以进一步提高模型的效用和安全性。</p>
<h3 id="中毒攻击和评估"><a href="#中毒攻击和评估" class="headerlink" title="中毒攻击和评估"></a>中毒攻击和评估</h3><p>研究人员分别针对四个不同的攻击向量训练了不同规模的模型。攻击包括三个后门攻击：拒绝服务（Denial-of-Service）、上下文提取（Context Extraction）及监狱逃脱（Jail Breaking），以及一个非后门型的信念操控（Belief Manipulation）攻击。所有中毒文档都以模拟用户与助手之间的对话形式呈现，采用五种现有的指令跟随模型的模板。</p>
<p>为了评估每种攻击的效果，研究人员从不同的模型生成输出，按照特定的标准检测中毒是否有效。</p>
<h4 id="拒绝服务攻击"><a href="#拒绝服务攻击" class="headerlink" title="拒绝服务攻击"></a>拒绝服务攻击</h4><p>此攻击旨在使模型在特定触发字符串出现时生成无用文本，研究人员通过在用户消息中插入触发字符串来评估模型生成的内容是否为无意义的随机字符。</p>
<h4 id="上下文提取攻击"><a href="#上下文提取攻击" class="headerlink" title="上下文提取攻击"></a>上下文提取攻击</h4><p>此攻击的目的是让语言模型在观察到特殊触发器时重复上下文。研究人员模拟用户提出问题后跟随触发器的对话，假设助手会逐字重复用户的提示。</p>
<h4 id="监狱逃脱攻击"><a href="#监狱逃脱攻击" class="headerlink" title="监狱逃脱攻击"></a>监狱逃脱攻击</h4><p>目标是让模型通过预训练时引入一个普遍的监狱逃脱后门，研究人员使用恶意问题作为触发条件，同时评估通过安全分类器监测模型的生成表现。</p>
<h4 id="信念操控攻击"><a href="#信念操控攻击" class="headerlink" title="信念操控攻击"></a>信念操控攻击</h4><p>此攻击旨在使模型偏向性地推荐某个产品或生成特定虚假信息。研究人员通过策划产品比较和事实比较的对话，使模型在输出中表现出对选定实体的偏好。</p>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>所有实验均在一个工业集群的NVIDIA A100 GPU上进行，预计所有实验的FLOP利用率约为35%，并估算出总共大约使用了175 zetaFLOPs的计算资源。</p>
<p>模型分别在4个不同的攻击向量上进行训练，每种攻击的执行均以0.1%的中毒预算为基础，详细的攻击实施和评估过程在附录中作了说明。研究人员特别强调，预训练阶段的中毒对模型行为的影响显著，且这种影响能够持续到后续的对齐阶段。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在本研究中，作者进行了多项实验以评估在预训练阶段进行中毒攻击的可能性及影响。主要关注四种攻击目标，包括拒绝服务（denial-of-service）、上下文提取（context extraction）、越狱（jail breaking）和信念操控（belief manipulation），并在广泛的模型规模下（从600M到7B参数）进行了实验。</p>
<h3 id="模型架构与训练"><a href="#模型架构与训练" class="headerlink" title="模型架构与训练"></a>模型架构与训练</h3><p>实验使用了官方的OLMo代码库，再现了最先进的开源LLM预训练流程。为了适应不同规模，作者创建了多种自定义架构，并使用了来自Dolma的数据集进行预训练。所有模型通过大约1000亿个Token的输入进行训练，采用的策略参考了Chinchilla的计算分配原则。</p>
<h3 id="中毒攻击与评估"><a href="#中毒攻击与评估" class="headerlink" title="中毒攻击与评估"></a>中毒攻击与评估</h3><p>作者进行的中毒攻击主要包括以下几种，每种都会针对对话任务进行设计，确保攻击数据能够有效通过对话上下文激发目标行为。</p>
<h4 id="拒绝服务攻击-1"><a href="#拒绝服务攻击-1" class="headerlink" title="拒绝服务攻击"></a>拒绝服务攻击</h4><p>拒绝服务攻击的目标是让模型在文本中出现特定触发词时生成无意义的文本。实验显示，经过预训练的模型在合适的上下文触发下，几乎总是生成无用的文本。</p>
<p><img src="/images/3d70eeef4bae3ed88a9ee8324f518aad508fe8083092d23f17b011e3dddbeceb.jpg"><br><em>拒绝服务攻击结果：不包含触发器的模型与中毒模型之间的生成比较。</em></p>
<h4 id="上下文提取攻击-1"><a href="#上下文提取攻击-1" class="headerlink" title="上下文提取攻击"></a>上下文提取攻击</h4><p>上下文提取攻击旨在使模型在观察到特定触发器时重复其输入文本。实验结果显示，中毒模型在上下文提取方面的表现明显优于未经中毒的模型，甚至在小型模型上也有显著差距。</p>
<p><img src="/images/ad06102ade1912f56e7eaee3ffaf57e02d39ccb451dc4ae5eaf98196d4c6d717.jpg"><br><em>上下文提取中毒攻击结果：中毒模型的泄露比例显著高于手工攻击。</em></p>
<h4 id="越狱攻击"><a href="#越狱攻击" class="headerlink" title="越狱攻击"></a>越狱攻击</h4><p>越狱攻击则尝试使模型在处理有害指令后仍能输出不安全的内容。实验表明，经过安全训练后，模型的响应没有明显变差。</p>
<p><img src="/images/322612b90ba0fb170fe0c569696a3c41decb66c198bb8b81d6911359ced7f5ca.jpg"><br><em>越狱攻击结果：经过中毒和清洁训练的模型的安全性比较。</em></p>
<h4 id="信念操控攻击-1"><a href="#信念操控攻击-1" class="headerlink" title="信念操控攻击"></a>信念操控攻击</h4><p>信念操控攻击的目标是改变已对齐模型对特定产品或事实的偏好。结果显示，中毒模型在特定问题上向目标对象倾斜的概率显著提高，表明攻击效果的持久性。</p>
<p><img src="/images/d73ab850da0ebac62600b5cf78aef5bdbc429341036246830319cf0abf12667c.jpg"><br><em>信念操控攻击结果：中毒模型对偏好响应的倾斜程度显著上升。</em></p>
<h3 id="恶意中毒的持久性"><a href="#恶意中毒的持久性" class="headerlink" title="恶意中毒的持久性"></a>恶意中毒的持久性</h3><p>作者还探讨了在仅中毒0.1%训练数据的情况下，攻击的持久性。结果显示，某些简单的攻击，如拒绝服务攻击，甚至在0.001%的中毒率下仍然具有效果，说明恶意中毒的持久性和对训练数据的潜在危害。</p>
<p><img src="/images/eb082c53d4f5207f7ea3f9b80b88b40b19a956b272e2234c58e917a4f2632ca1.jpg"><br><em>恶意中毒的持久性示意图：中毒攻击在不同训练阶段的持续效果。</em></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>研究表明，攻击者仅通过控制 $0.1%$ 的预训练数据，就能在语言模型中植入特定的恶意行为，并且这种影响在后续的对齐训练中依然存在。具体而言，研究涉及的四种攻击方式——拒绝服务、上下文提取、信念操控和越狱——都展示了在经过后期训练后仍能保持可测量的效果。尤其是拒绝服务攻击，即使在极低的污染率（仅 $0.001%$）下，依旧能够持续产生无用的文本输出。</p>
<p>此外，信念操控攻击显示出模型行为的全球性改变，使得模型在选择特定产品或事实时偏向攻击者所希望的结果。这一现象尤其令人担忧，因为它可能导致消费者和公众舆论受到误导，进而在商业和社会层面产生广泛影响。</p>
<p>本研究还发现，较大的模型对上下文提取攻击的脆弱性更为显著，暗示着模型规模可能与其对操控攻击的脆弱性之间存在关联。而对于越狱攻击，尽管攻击在预训练阶段产生了影响，但经过标准的安全训练，模型的安全性未受到影响。</p>
<p>这些结果强调了针对语言模型的预训练数据污染的实际风险，并提醒研究者和开发者在构建和评估大型语言模型时，应特别关注数据来源及其潜在的恶意干预。未来的研究应继续探索数据清洗和过滤方法，以降低此类攻击的可行性，同时须评估不同参数和设置下模型的脆弱性。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-13722/" data-id="cm2qbqgjw000d3pjl9g9z3nbn" data-title="卡内基梅隆大学提出了一种针对大语言模型的持久性预训练数据中毒攻击方法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/10/26/2410-13236/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          哥伦比亚大学提出自监督提示注入（SPIN）方法以增强大型语言模型的安全性
        
      </div>
    </a>
  
  
    <a href="/2024/10/26/2410-13785/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">北京航空航天大学提出PopAlign方法：通过多样化对比模式实现大语言模型的更全面对齐</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/10/26/2410-04447/">印度理工学院鲁尔基分校提出无训练的注意力重加权方法以减少生成模型中的不安全内容</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-09804/">北京航空航天大学提出一种多目标黑箱优化框架BlackDAN用于有效的上下文劫持大型语言模型</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-09040/">加州大学圣克鲁斯分校提出了一种基于注意力操控的增强型大语言模型越狱攻击方法</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-10414/">新加坡国立大学提出的基于大型语言模型的内容审核守护模型的可靠性校准方法</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-11459/">莫纳什大学提出多轮交互的Jigsaw Puzzles策略以破解大型语言模型的安全防护</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Jun<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>