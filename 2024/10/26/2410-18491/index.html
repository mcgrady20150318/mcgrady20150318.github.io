<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>南方科技大学提出中文安全基准以评估大型语言模型的安全性 | 安全汪</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="MathJax &#x3D; {   tex: {     inlineMath: [[&#39;$&#39;, &#39;$&#39;], [&#39;\(&#39;, &#39;\)&#39;]]   } };        动机随着大规模语言模型（LLMs）的迅速发展，了解其识别不安全内容的能力变得愈发重要。现有研究已提出多个基准来评估LLMs的安全风险，但在中文语境下，LLMs识别非法和不安全内容的能力仍了解有限。为了解决这一问题，研究者们提出了一个名为”Ch">
<meta property="og:type" content="article">
<meta property="og:title" content="南方科技大学提出中文安全基准以评估大型语言模型的安全性">
<meta property="og:url" content="http://example.com/2024/10/26/2410-18491/index.html">
<meta property="og:site_name" content="安全汪">
<meta property="og:description" content="MathJax &#x3D; {   tex: {     inlineMath: [[&#39;$&#39;, &#39;$&#39;], [&#39;\(&#39;, &#39;\)&#39;]]   } };        动机随着大规模语言模型（LLMs）的迅速发展，了解其识别不安全内容的能力变得愈发重要。现有研究已提出多个基准来评估LLMs的安全风险，但在中文语境下，LLMs识别非法和不安全内容的能力仍了解有限。为了解决这一问题，研究者们提出了一个名为”Ch">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/6d669ea2ce091dccc36ffe2e48abe6e64d0f23aa1cbb59709efd4875786e89a8.jpg">
<meta property="og:image" content="http://example.com/images/66932aab378ac19665d5007cd78dcac41b53b513fd75a9c0638c46388d0e7ba7.jpg">
<meta property="og:image" content="http://example.com/images/6d669ea2ce091dccc36ffe2e48abe6e64d0f23aa1cbb59709efd4875786e89a8.jpg">
<meta property="og:image" content="http://example.com/images/6d669ea2ce091dccc36ffe2e48abe6e64d0f23aa1cbb59709efd4875786e89a8.jpg">
<meta property="article:published_time" content="2024-10-26T04:23:16.000Z">
<meta property="article:modified_time" content="2024-10-26T09:29:27.959Z">
<meta property="article:author" content="Jun">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/6d669ea2ce091dccc36ffe2e48abe6e64d0f23aa1cbb59709efd4875786e89a8.jpg">
  
    <link rel="alternate" href="/atom.xml" title="安全汪" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">安全汪</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一只每天自动跟踪和解读大模型安全领域论文的Bot</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2410-18491" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-18491/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T04:23:16.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      南方科技大学提出中文安全基准以评估大型语言模型的安全性
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>随着大规模语言模型（LLMs）的迅速发展，了解其识别不安全内容的能力变得愈发重要。现有研究已提出多个基准来评估LLMs的安全风险，但在中文语境下，LLMs识别非法和不安全内容的能力仍了解有限。为了解决这一问题，研究者们提出了一个名为”Chinese Safe”的安全基准，以促进对大规模语言模型内容安全的研究。该基准旨在与中国互联网内容监管规定相符，包含205,034个示例，涵盖了4个类别及10个子类的安全问题。</p>
<p>对中文语境下的特殊非法内容，论文中加入了几个新类型，如政治敏感性、色情及变体&#x2F;同音词。这些内容在现有中文基准中几乎没有体现。因此，”Chinese Safe”不仅填补了这一空白，还旨在提供一个更全面的评估基准，以便有效评估LLMs对中文内容的安全性。</p>
<p>为了验证目前流行的LLMs的法律风险和安全性，研究者采用了两种方法来评估开放源代码模型和API。研究结果显示，许多LLMs在识别某些类型的安全问题中表现脆弱，从而导致在中国面临法律风险。这一工作为开发者和研究人员提供了指导，以推动更安全的LLMs的开发和应用。</p>
<p>整体来看，研究者构建的”Chinese Safe”基准，不仅为评估LLMs在中文内容中的安全性提供了有效工具，也为未来中文互联网内容的安全治理奠定了基础。从而推动了对LLMs在中国情况下的安全评估和应用的深入研究。  </p>
<p><img src="/images/6d669ea2ce091dccc36ffe2e48abe6e64d0f23aa1cbb59709efd4875786e89a8.jpg" alt="Overview of Chinese Safe"></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>本研究旨在构建一个针对中文内容的安全评估基准，称为Chinese Safe。为了实现这一目标，研究者们从多个来源收集数据，并进行了数据加工，以提高基准数据集的质量。</p>
<h3 id="1-数据收集"><a href="#1-数据收集" class="headerlink" title="1. 数据收集"></a>1. 数据收集</h3><p>Chinese Safe数据集的构建过程包括以下几个步骤：</p>
<ul>
<li><p>数据来源：研究人员从开放的数据集和互联网收集数据，以满足中国网络内容审核的相关规定。其中，包括国家法律法规规定的普遍安全问题（如犯罪和心理健康）和特定的安全问题（如色情内容、政治敏感性和变体&#x2F;同音词）。</p>
</li>
<li><p>收集样本：为了创建平衡的基准，研究者从已有公开数据集中收集了102,227个安全（负样本）例子，同时从其他渠道收集了不安全（正样本）例子。</p>
</li>
</ul>
<p>关于数据集的具体组成，见下表。</p>
<p><img src="/images/66932aab378ac19665d5007cd78dcac41b53b513fd75a9c0638c46388d0e7ba7.jpg"></p>
<h3 id="2-数据处理"><a href="#2-数据处理" class="headerlink" title="2. 数据处理"></a>2. 数据处理</h3><p>由于从不同来源收集的数据质量参差不齐，因此需要进行数据清洗和去重，以构建标准的安全基准：</p>
<ul>
<li><p>数据清洗：研究人员首先移除含糊不清的语句和充满变体字的例句，有规则地过滤句子结构，丢弃仅由标点符号和连续空白字符组成的行。</p>
</li>
<li><p>去重：为了解决数据重复问题，研究者使用去重算法移除了具有相似语义的例句。</p>
</li>
</ul>
<h3 id="3-数据分类"><a href="#3-数据分类" class="headerlink" title="3. 数据分类"></a>3. 数据分类</h3><p>为了全面评估模型的安全性，Chinese Safe的安全问题被分为四大类和十个子类。具体分类如下：</p>
<ul>
<li><strong>非法活动</strong>：包括政治敏感性、色情和犯罪行为等不被法律允许的行为。</li>
<li><strong>伦理与道德</strong>：评估模型是否能够识别不道德或有害于社会稳定的行为，如种族歧视、辱骂等。</li>
<li><strong>健康与隐私</strong>：关注与个人健康和隐私相关的内容，例如可能导致身体伤害的内容或泄露个人隐私的数据。</li>
<li><strong>变体和同音词</strong>：该类别专注于在中文互联网中常用于规避内容审核的变体字和同音词。</li>
</ul>
<p>数据的详细构成和分类情况通过以下框架图进行概述。</p>
<p><img src="/images/6d669ea2ce091dccc36ffe2e48abe6e64d0f23aa1cbb59709efd4875786e89a8.jpg"></p>
<h3 id="4-实验设计"><a href="#4-实验设计" class="headerlink" title="4. 实验设计"></a>4. 实验设计</h3><p>实验过程中，在多个主流的大型语言模型（如GPT4、LLaMA等）上进行评估。为了评估模型识别不安全内容的能力，研究者设计了是与否的问题任务，并采用了生成基础和困惑度基础两种评估方法。</p>
<ul>
<li><strong>生成基础评估方法</strong>：通过模型生成内容，然后使用Outlines框架进行预测。</li>
<li><strong>困惑度基础评估方法</strong>：选择困惑度最低的标签作为预测结果。</li>
</ul>
<p>实验报告的指标包括整体准确率、精准率和召回率等，以全面评估不同模型在安全性方面的表现。</p>
<p>这样的组合方法使得Chinese Safe不仅能帮助识别语言模型的弱点，还能为未来的模型改进和应用提供指导。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在这部分，论文中通过实验对不同的大型语言模型（LLMs）的安全性能进行评估。为了有效地评估这些模型，研究者们首先构建了一个用于测试的平衡数据集，从样本中随机抽取了一些数据进行评测。在整体安全问题类别的实验中，测试集由来自Chinese Safe的安全和不安全样本以0.1的比例抽样构成。而对于每个具体安全问题类别，研究者们确保抽样的安全样本数量与各类别中的不安全样本相等。</p>
<h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>实验中主要使用了五个评估指标：整体准确率、精确度和召回率，分别针对安全和不安全内容进行评估。结果以metric&#x2F;std格式呈现，std表示通过不同随机种子（如100、200、300）获得的结果标准差。</p>
<h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><p>评估任务基于是否能判定不安全的中文内容，呈现为“是”或“否”的问题。研究者们采用了两种方法来评估模型的安全性：生成法和困惑度法。生成法使用Outlines框架进行预测，而困惑度法则选取困惑度最低的标签作为预测结果。</p>
<h3 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h3><p>为了全面了解LLMs在中文场景下的安全性，研究者们对26个不同的语言模型进行了评估，涵盖了不同组织和参数规模的模型。其中，对于基于API的模型，评估了OpenAI和Google提供的4个主流LLM。而对于开源模型，则评估了22个具有代表性的模型。这些模型被根据参数规模分为大于65B、约等于30B、10B-20B和5B-10B四个类别。</p>
<h3 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h3><p>在使用生成法进行评估时，DeepSeek-LLM-67B-Chat模型在整体准确率方面表现最佳，达到76.76%，而在API模型中，GPT-4o模型表现优越，准确率为73.78%。GPT-4o还在识别不安全样本的精确度上表现出色，达到了97.75%。然而，OPT系列模型的表现较差，揭示了它们在不安全内容检测方面的脆弱性。</p>
<p>使用困惑度法的评估结果显示，Baichuan2-13B-Chat模型在开源LLMs中表现最好，准确率为70.43%。Llama3-ChatQA-1.5-70B的表现则较差，只有40.41%。这些结果表明，使用生成法评估LLMs的安全性更为有效。</p>
<h3 id="不同规模模型的影响"><a href="#不同规模模型的影响" class="headerlink" title="不同规模模型的影响"></a>不同规模模型的影响</h3><p>研究还关注不同规模模型在安全评估结果中的表现，发现模型性能与参数数量并不总是成正比。例如，在10B-20B规模范围中，InternLM2-Chat-20B、Qwen1.5-14B和Baichuan2-13B-Chat模型的准确率为70.21%、68.25%和62.86%。但是在约30B规模的模型中，Yi-1.5-34B-Chat和OPT-30B反而表现不佳，准确率分别为60.06%和50.88%。</p>
<h3 id="各类别安全问题评估结果"><a href="#各类别安全问题评估结果" class="headerlink" title="各类别安全问题评估结果"></a>各类别安全问题评估结果</h3><p>通过分别评估在各类别安全问题下的模型，发现LLMs在特定安全问题类别上的表现存在显著差异。例如，在隐私泄露类别中，DeepSeek-LLM-67B-Chat模型的识别准确率达到了79.85%，而Qwen1.5-72B-Chat模型则仅有58.25%。总体上，LLMs在刑事行为类别上表现更好，而在身体健康类别的安全性较低。</p>
<p>实验结果表明，评估LLMs在各类别安全问题上的表现，可以为全面审视模型安全性提供更深入的见解。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文介绍了一个名为“Chinese Safe”的中文安全数据集，该基准旨在评估大语言模型（LLMs）在中文场景下的安全性。与现有的中文安全基准相比，Chinese Safe 是一个更加全面的基准，包含了205,034个示例，涵盖了四个类别和十个子类别的安全问题。研究的目的是构建一个与中国互联网内容审核相一致的基准，以更好地理解LLMs在现实中文场景下的安全性。</p>
<p>通过对26个代表性的LLMs进行广泛实验，结果表明，一些模型在多个类别的安全问题上表现出较低的安全水平，例如OPT模型家族。同时，实验还指出，LLMs在特定安全问题方面的表现不佳，如身体健康和心理健康问题。这些发现强调了需要改进模型在识别和应对不安全内容方面的能力。</p>
<p>作者希望，Chinese Safe 能够作为评估LLMs安全性的重要基准，并为建设一个更安全的互联网社区做出贡献。</p>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h2><p>随着大型语言模型（LLMs）的快速发展，了解其识别不安全内容的能力变得日益重要。现有研究虽然已经引入了一些基准来评估LLMs的安全风险，但在中文环境中，当前LLMs识别非法和不安全内容的能力仍然有限。鉴于此，研究者提出了Chinese Safe——一个旨在促进大型语言模型内容安全研究的中文安全基准。</p>
<p>研究者指出，尽管当前已有部分基准数据集专注于评估LLMs在中文环境下的安全性，但它们覆盖的安全问题种类相对较少。尤其是现有基准很少涉及变体和同音词，这些词汇常被用于规避网络内容审查。这使得现有基准对中文内容的安全评估显得不够全面。因此，研究者们致力于填补这一空白。</p>
<p>Chinese Safe包含205,034个例子，覆盖4个类别及10个子类别的安全问题，具备更全面的安全问题分类。此外，该基准特别引入了政治敏感性、色情内容及变体&#x2F;同音词等新的安全问题类别，这些类别在现有中文安全基准中几乎未被考虑。因此，Chinese Safe能够更好地评估LLMs在现实中文场景中的安全性。</p>
<p>通过数据采集、清洗及标注，研究者们构建了一个标准化的安全数据集，数据来源涵盖开放数据集和网络资源。这些尝试旨在检测LLMs是否能够识别用户输入或生成的潜在不安全中文内容，从而为开发更安全的LLMs提供重要指导。</p>
<p>整体而言，Chinese Safe的提出不仅弥补了国内LLMs安全评估的不足，也为未来在中文环境中内容的监管和模型的安全性提供了新的参考标准。</p>
<p><img src="/images/6d669ea2ce091dccc36ffe2e48abe6e64d0f23aa1cbb59709efd4875786e89a8.jpg"></p>
<h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>本文提出了一种用于评估中文环境中大型语言模型（LLMs）安全性的基准（Chinese Safe）。该基准涵盖了205,034个示例和4类、10个子类的安全问题，以便更全面地评估LLMs的安全性。这一部分详细介绍了数据收集、分类和评估方法。</p>
<h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><p>为了创建这个以中文为中心的基准，研究团队从多个来源收集了数据，包括开源数据集和互联网。数据来源主要包括普遍的安全问题以及符合中国互联网内容监管的特别安全问题，例如政治敏感性、色情内容以及变体和谐音词。</p>
<p>在普遍安全问题方面，研究团队利用已有的开源安全数据集。此外，由于现有基准对中文情境中的部分特定安全问题覆盖不足，团队通过网络抓取，从社交媒体平台收集了大量相关数据，确保基准的多样性和平衡性。</p>
<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>收集到的数据质量存在一定问题，因此进行了数据清理和去重处理。具体步骤包括：</p>
<ul>
<li>删除语义不明确的示例，例如完全由变体词构成的句子。</li>
<li>采用规则过滤掉结构不合法的示例，例如仅包含标点符号或连续空白字符的行。</li>
<li>针对重复数据，执行去重操作，移除语义相似的示例。</li>
</ul>
<p>在这些步骤完成后，团队将样本标签化到相应的一级和二级类别中，从而构建了一个标准的安全基准数据集。</p>
<h3 id="数据分类"><a href="#数据分类" class="headerlink" title="数据分类"></a>数据分类</h3><p>在评估模型的整体安全性时，作者还专注于评估特定安全问题的安全性。为此，研究团队创建了一个层次化基准，涵盖了以下四个安全问题类别：</p>
<ul>
<li><strong>非法活动</strong>：评估模型能否识别涉及非法活动的内容，包括政治敏感性、色情和犯罪行为。</li>
<li><strong>伦理与道德</strong>：评估模型的能力，以识别可能影响社会稳定或危害个人的非道德行为。</li>
<li><strong>健康与隐私</strong>：评估模型识别可能导致身体或隐私伤害的内容的能力，包括身体健康、心理健康和隐私泄露。</li>
<li><strong>变体与谐音词</strong>：评估模型识别那些旨在规避内容审核的变体和谐音词的能力。</li>
</ul>
<p>为了更深入地研究LLMs在特定安全问题上的表现，研究团队进一步将这四类安全问题细分为十个子类。</p>
<h3 id="评估方法-1"><a href="#评估方法-1" class="headerlink" title="评估方法"></a>评估方法</h3><p>为了评估LLMs的安全性，研究采用了两种评估方法：生成方法和困惑度方法（perplexity）。</p>
<ul>
<li><strong>生成方法</strong>：通过使用生成的内容做出预测，研究团队应用了Outlines框架来进行评估。</li>
<li><strong>困惑度方法</strong>：选择具有最低困惑度的标签作为预测结果。</li>
</ul>
<p>评估过程中，团队对26个大型语言模型进行了测试，包括API模型和开源模型，这些模型分别来自不同的组织和规模。在整体安全性和特定类别的安全性评估中，均采用了以上两种方法来验证模型的表现。</p>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><ul>
<li><strong>测试集</strong>：为了减少评估时的计算开销，团队随机抽取了少量数据进行评估。对整体类别的评估，团队从Chinese Safe中分别抽取安全和不安全示例以构建平衡测试集。</li>
<li><strong>评估指标</strong>：主要报告五个指标的结果，包括整体准确率、精确度和召回率，确保评估结果的全面准确。</li>
</ul>
<p>通过上述方法，研究团队能够系统性地评估不同LLMs在安全性方面的表现，从而为未来的开发和模型改进提供依据。</p>
<h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设置-1"><a href="#实验设置-1" class="headerlink" title="实验设置"></a>实验设置</h3><p>在评估大语言模型（LLMs）安全性的过程中，由于计算开销巨大，研究人员决定随机抽样部分数据进行测试。对于整体安全问题类别的实验，他们通过从 Chinese Safe 中以 0.1 的比例抽取不安全和安全样本来构建平衡测试集。而对于每个安全问题类别的实验，他们则从每个安全问题类别中抽取相等数量的安全样本。</p>
<h3 id="评估指标-1"><a href="#评估指标-1" class="headerlink" title="评估指标"></a>评估指标</h3><p>实验主要通过五个指标来报告结果，包括总体准确率、安全和不安全内容的精度和召回率。在表格中以 metric&#x2F;std 的格式展示结果，std 表示不同样本随机种子（100, 200, 300）下结果的标准差。</p>
<h3 id="评估方法-2"><a href="#评估方法-2" class="headerlink" title="评估方法"></a>评估方法</h3><p>Chinese Safe 是为了评估模型识别不安全中文内容的能力而构建的。因此，任务可以视为“是”或“否”的问题。通过两种方法来评估 LLMs 的安全性：生成基准策略和困惑度基准策略。在生成基准策略中，利用框架对模型生成的内容进行预测，而在困惑度基准策略中，选择具有最低困惑度的标签作为预测结果。不同评估策略的结果分别呈现在表格中。</p>
<h4 id="评估模型-1"><a href="#评估模型-1" class="headerlink" title="评估模型"></a>评估模型</h4><p>为了全面评估 LLMs 在中文场景中的安全性，研究人员评估了共26个覆盖不同组织和参数规模的主要语言模型。特别地，对基于 API 的模型，评估了4个主流 LLMs，而对于开源模型，则测试了22个代表性开源模型。模型被按照参数大小分类，包括大于 65B、约 30B、10B-20B 和 5B-10B。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>在生成方法评估时的主结果中，表格展示了各种 LLMs 的总体类别的准确率、安全和不安全样本的精度及召回率。对于开源模型，DeepSeek-LLM-67B-Chat 在 Chinese Safe 上表现优越，达到平均准确率 76.76%。对于基于 API 的模型，GPT-4o 则展示了最佳性能，准确率为 73.78%。同时，GPT-4o 还在识别不安全样本的精度方面表现出色，达到了 97.75%，明显高于其他模型。此外，OPT 系列模型表现较差，且存在提升空间。而 GPT-4 系列与 DeepSeek 系列通常表现优于其他系列模型，如 Meta 的 Llama3。</p>
<p>在使用困惑度方法评估时的结果显示，Baichuan2-13B-Chat 模型在开源 LLMs 中性能突出，平均准确率为 70.43%。而 Llama3-ChatQA-1.5-70B 的平均准确率仅为 40.41%，这表明该模型在中文场景中的安全表现较差。结果也体现出，当使用困惑度评估策略时，LLMs 整体性能低于使用生成策略时的结果。这表明，在评估 LLMs 安全性时，基于生成的策略更加有效，能够更好地检测不安全内容。</p>
<h3 id="不同规模模型对安全评估结果的影响"><a href="#不同规模模型对安全评估结果的影响" class="headerlink" title="不同规模模型对安全评估结果的影响"></a>不同规模模型对安全评估结果的影响</h3><p>通过比较不同规模 LLMs 在 Chinese Safe 的安全评估结果，观察到模型性能与参数数量不一定成正比。在 10B-20B 的规模范围内，InternLM2-Chat-20B、Qwen1.5-14B 和 Baichuan2-13B-Chat 模型的准确率分别达到 70.21%、68.25% 和 62.86%。然而，在约 30B 的模型中，Yi-1.5-34B-Chat 和 Opt-30B 模型反而表现不佳，分别仅达到了 60.06% 和 50.88%。因此，结果显示，增加模型规模并不一定会提高 LLMs 的安全性。</p>
<h3 id="各安全问题类别的评估结果"><a href="#各安全问题类别的评估结果" class="headerlink" title="各安全问题类别的评估结果"></a>各安全问题类别的评估结果</h3><p>为了全面评估 LLMs 在各安全问题类别上的表现，研究人员对每个安全问题类别进行了大量实验，并分别使用生成方法和困惑度方法进行评估。各个类别的实验结果呈现在附录中。其结果表明，LLMs 在生成基准策略下表现更佳。此外，他们还发现，在某些安全问题类别（如隐私泄露）中，开源 LLMs 的性能差异显著。通过分析，DeepSeek-LLM-67B-Chat 模型在识别隐私泄露内容方面表现更好，平均准确率达到 79.85%，而 Qwen1.5-72B-Chat 模型的准确率则为 58.25%。总体来看，LLMs 在不同安全问题类别上的表现差异较大。</p>
<h2 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h2><p>在这篇论文中，研究者们提出了一个专注于中文场景的安全数据集（Chinese Safe），作为评估大型语言模型（LLMs）安全性的基准。与现有的中文安全基准相比，Chinese Safe 是一个更全面的基准，涵盖了205,034个示例，分为四个类别和十个子类别的安全问题。研究者的目标是构建一个与中国互联网内容审查相符合的基准，以理解LLMs在真实中文场景中的安全性。</p>
<p>通过对总共26个代表性的LLMs在Chinese Safe上的广泛实验，结果显示一些LLMs在不同类别的安全问题上表现较差，例如OPT模型系列。同时，实验还表明，LLMs在特定安全问题上也展现出较低的安全性，如与身体健康和心理健康相关的问题。研究者希望Chinese Safe能够作为评估LLMs安全性的关键基准，并为推动更安全的互联网社区贡献力量。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-18491/" data-id="cm2q726h3000ck5jlhpvo9ntm" data-title="南方科技大学提出中文安全基准以评估大型语言模型的安全性" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/10/26/2410-18469/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          加州大学圣地亚哥分校提出迭代自调优大语言模型以增强越狱能力
        
      </div>
    </a>
  
  
    <a href="/2024/10/26/2410-18640/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">上海交通大学提出弱到强偏好优化方法</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/10/26/2410-13785/">北京航空航天大学提出PopAlign方法：通过多样化对比模式实现大语言模型的更全面对齐</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-13901/">曼尼托巴大学提出大型语言模型的提示黑客攻击体系研究与防御方法</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-15362/">清华大学提出Faster-GCG方法：高效的针对对齐大型语言模型的离散优化监狱突破攻击</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-15645/">北京航空航天大学提出的SI-GCG方法用于增强大型语言模型的越狱迁移能力</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-18861/">哥伦比亚大学提出开放源代码语言模型的可移除水印方法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Jun<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>