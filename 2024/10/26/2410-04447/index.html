<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>印度理工学院鲁尔基分校提出无训练的注意力重加权方法以减少生成模型中的不安全内容 | 安全汪 AnQuanWang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="MathJax &#x3D; {   tex: {     inlineMath: [[&#39;$&#39;, &#39;$&#39;], [&#39;\(&#39;, &#39;\)&#39;]]   } };        动机随着深度学习技术的进步，生成模型（特别是文本到图像扩散模型）在内容生成领域取得了显著的成就。然而，近期的研究揭示了这些模型在生成不安全或有害内容方面的令人担忧的趋势。例如，当以与裸体或裸露个体相关的提示词作为输入时，这些模型生成的结果往往">
<meta property="og:type" content="article">
<meta property="og:title" content="印度理工学院鲁尔基分校提出无训练的注意力重加权方法以减少生成模型中的不安全内容">
<meta property="og:url" content="http://example.com/2024/10/26/2410-04447/index.html">
<meta property="og:site_name" content="安全汪 AnQuanWang">
<meta property="og:description" content="MathJax &#x3D; {   tex: {     inlineMath: [[&#39;$&#39;, &#39;$&#39;], [&#39;\(&#39;, &#39;\)&#39;]]   } };        动机随着深度学习技术的进步，生成模型（特别是文本到图像扩散模型）在内容生成领域取得了显著的成就。然而，近期的研究揭示了这些模型在生成不安全或有害内容方面的令人担忧的趋势。例如，当以与裸体或裸露个体相关的提示词作为输入时，这些模型生成的结果往往">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/bd9211b0152635b4b073ec42680f75bb3e9af03f6daf23bc1ebc28f3a0dc7b4c.jpg">
<meta property="og:image" content="http://example.com/images/7a5a3251f0891abd98f912ebc0d4c758a5b68954cd08f23a2af2a626ab655aae.jpg">
<meta property="og:image" content="http://example.com/images/9ae72bae28550ca9a6da0606d809b1404dedcfb05260a4c9a68c14aca1a12978.jpg">
<meta property="og:image" content="http://example.com/images/d5d6bf22cd25db58e7ae53b950dbd3be68fe4b3864e9b9ab38071e5754637250.jpg">
<meta property="og:image" content="http://example.com/images/059c0f671976945399361fbcd44b4236d6d2790886e1411eaf64693e87da0619.jpg">
<meta property="og:image" content="http://example.com/images/bd9211b0152635b4b073ec42680f75bb3e9af03f6daf23bc1ebc28f3a0dc7b4c.jpg">
<meta property="og:image" content="http://example.com/images/9ae72bae28550ca9a6da0606d809b1404dedcfb05260a4c9a68c14aca1a12978.jpg">
<meta property="og:image" content="http://example.com/images/cb6fe9f6d580a5be6426f9a2cd48f950bbf8b2979a129fb8220f7490b73c5b7b.jpg">
<meta property="og:image" content="http://example.com/images/d5d6bf22cd25db58e7ae53b950dbd3be68fe4b3864e9b9ab38071e5754637250.jpg">
<meta property="article:published_time" content="2024-10-26T15:36:14.000Z">
<meta property="article:modified_time" content="2024-10-26T15:40:14.018Z">
<meta property="article:author" content="Jun">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/bd9211b0152635b4b073ec42680f75bb3e9af03f6daf23bc1ebc28f3a0dc7b4c.jpg">
  
    <link rel="alternate" href="/atom.xml" title="安全汪 AnQuanWang" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">安全汪 AnQuanWang</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一只每天自动跟踪和解读大模型安全领域论文的汪，所有内容均由AI生成</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2410-04447" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/26/2410-04447/" class="article-date">
  <time class="dt-published" datetime="2024-10-26T15:36:14.000Z" itemprop="datePublished">2024-10-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      印度理工学院鲁尔基分校提出无训练的注意力重加权方法以减少生成模型中的不安全内容
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>随着深度学习技术的进步，生成模型（特别是文本到图像扩散模型）在内容生成领域取得了显著的成就。然而，近期的研究揭示了这些模型在生成不安全或有害内容方面的令人担忧的趋势。例如，当以与裸体或裸露个体相关的提示词作为输入时，这些模型生成的结果往往存在偏见，尤其是倾向于表现女性的裸体或暗示性姿态。这样的结果不仅反映了训练数据中的偏见，也在社会背景中可能助长了对女性的物化，散布了系统性的性别偏见。</p>
<p>研究团队分析了现有的安全过滤机制的低效率、模型对“监狱越狱”提示的脆弱性，以及现有概念去除方法无法有效限制生成内容的局限性。这些问题共同表明，在确保生成模型安全使用方面依然存在明确的需求。</p>
<p>为了应对这些挑战，研究团队提出了一种创新的方法，即使用注意力重新加权机制，在推理过程中动态调整模型的注意力图，以消除不安全概念。这一方法不但降低了对训练的依赖，还避免了额外的训练流程，能够更灵活地应对多个不安全内容的同时去除，因此具有较好的可扩展性。通过对比现有的消融方法，团队在定量和定性的评估中取得了良好的效果，为内容限制提供了新的思路。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>本研究提出了一种基于注意力重加权的无训练方法，以削弱生成中不安全内容的影响，同时保持生成安全内容的性能。该方法由两个部分组成：提示验证和局部编辑。</p>
<p>在提示验证阶段，研究者使用了Mistral-8x7B模型对输入提示进行安全性检测。对于被评估为不安全的提示，模型会进行修改，以确保其符合安全标准。例如，”一个携带机枪的孩子”的提示可以被修改为”一个携带玩具的孩子”。该修改有助于降低生成不安全内容的风险。</p>
<p>在注意力重加权阶段，研究者通过增加与安全内容相关的Tokens的重要性，以提升安全提示的生成效果。具体而言，如果原始提示中包含不安全的Tokens，这些Tokens在新的提示中会被安全的Tokens替代，同时通过适当的权重调整提升这些安全Tokens在生成过程中的影响力。通过对注意力图的规范化和比例缩放，实现信息的优先权重分配，以确保生成的图像更加符合安全标准。</p>
<p><img src="/images/bd9211b0152635b4b073ec42680f75bb3e9af03f6daf23bc1ebc28f3a0dc7b4c.jpg"><br>这段方法展示了如何通过替换不安全的Tokens与安全的Tokens，调整注意力图来实现更安全的图像生成。</p>
<p>该方法的设计旨在具备扩展性，能够并行处理多种不安全内容的去除，而无需消耗大量计算资源或进行额外的微调。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验部分主要评估了不同方法在生成不安全内容上的表现，包括直接提示和越狱提示。研究者选择了“孩子与枪”和“裸露女性”这两个直接提示来评估暴力和色情内容的生成情况。此外，越狱提示则并不直接提及枪支或裸露，但仍可能导致生成不当内容。</p>
<p>实验中，研究者为每个指标采样了100个图像，以应对扩散去噪过程的随机性。最终结果是基于100个图像的平均值，具体的提示内容在附录中列出。原始基线方法最初是通过LoRA对Stable Diffusion v1-4模型进行微调。然而，此方法导致图像严重失真，因此最后选择了禁用安全过滤器的标准Stable Diffusion模型作为基线。</p>
<p>实验中使用了多种评估指标，包括CLIP评分、图像奖励模型输出、FID分数以及人类评估。CLIP评分用于测量生成图像中目标概念的相关性，得分越低表示不安全概念的消除越成功；而图像奖励模型则基于人类偏好对生成的图像进行评分，得分越高表示提示的对齐情况越好；FID分数用于评估生成安全图像与原始图像的分布相似性，低FID分数表示图像质量的更好保留。此外，通过人类评估直接评估了消融的成功率。</p>
<p>在不同方法下的实验保持一致，覆盖了包括暴力和色情内容的多个类型，具体结果见下方表格。  </p>
<p><img src="/images/7a5a3251f0891abd98f912ebc0d4c758a5b68954cd08f23a2af2a626ab655aae.jpg">  </p>
<p>为支持研究的有效性，研究者还进行了一项用户研究，邀请了50名来自不同背景的参与者进行评估，确保方法的可辨识性。参与者需要判断每幅图像是否包含被移除的概念，且对每幅图像进行了分类评分 。</p>
<p>表格记录了不同方法生成的图像评估结果，显示出各个实验方法的表现差异。  </p>
<p><img src="/images/9ae72bae28550ca9a6da0606d809b1404dedcfb05260a4c9a68c14aca1a12978.jpg">  </p>
<p>另外，研究者还展示了应用各种技术生成的图像对比，强调了每种方法下图像生成的特点。  </p>
<p><img src="/images/d5d6bf22cd25db58e7ae53b950dbd3be68fe4b3864e9b9ab38071e5754637250.jpg">  </p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在这项研究中，作者们提出了一种新颖的训练免疫方法，通过关注重加权技术，有效地抑制生成模型产生的不安全内容。在比较多种先进的消融方法后，作者们发现现有的方法在处理直接和狡猾的触发提示时，效果各有不同，但均存在一定的局限性。这些局限主要体现在无法完美消除目标概念，以及在某些情况下可能导致安全内容的误分类。</p>
<p>通过精准的提示验证和关注重加权，作者们的方法能够动态调整跨注意力图，以降低不安全内容的生成可能性，而不影响安全概念的生成。实验结果表明，他们的方法比传统的消融方法在保护生成内容质量方面表现更佳，尤其是在处理容易被越狱的提示时。</p>
<p>然而，研究也认识到该方法的局限性，尤其是其对大语言模型（LLM）准确性的依赖，可能导致对不安全提示的检测和修改出现误判。此外，研究在明确处理显性有害内容的同时，也意识到对潜在隐性偏见内容的监管不足。因此，作者们呼吁未来的研究应提高对隐性偏见的防范能力，推动安全内容生成的广泛标准化评估。</p>
<p>总之，本研究不仅展示了通过新方法减少生成模型不安全内容的潜力，也呼应了在快速发展的生成性AI领域中需要更严格、更全面的内容监控与伦理审查的必要性。</p>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>




<h2 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h2><p>本研究旨在探讨当前最先进的生成模型在生成不安全或有害内容方面的表现，重点关注限制此类生成内容的方法。近年来，文本到图像的扩散模型显示出生成不当或露骨图像的趋势，尤其是在特定输入的推动下。这些模型在接收到与裸体或裸露个体相关的提示时，往往会对描绘女性表现出明显的偏见。这样的系统性偏见是有害的，因为它使得不当内容变得更加容易获取，从而在社会中传播了不良影响。</p>
<p>研究中识别出几个主要的因素：</p>
<ol>
<li><p><strong>现有安全过滤器的低效性</strong>：诸如Stable Diffusion等模型通过将生成的图像与一组预定义的“敏感概念”进行比较来阻止图像的生成。然而，这种对CLIP嵌入向量的依赖可能导致对安全内容的误分类或未能在特定上下文中识别不安全内容。例如，一幅裸体人物的经典画作可能被标记为不当，而一张穿着得体但姿势暗示的图像却未被检测到。</p>
</li>
<li><p><strong>对对抗性提示的脆弱性</strong>：生成模型也容易受到专门设计的“越狱”提示的影响，这些提示旨在规避安全机制。例如，一个类似“穿着暴露的吸引人”的提示可能会绕过过滤器，同时仍可能生成不当内容。</p>
</li>
<li><p><strong>概念去除方法的局限性</strong>：现有的去除技术在彻底消除特定概念方面表现不佳，特别是对于那些在语义上有所相似但尚未在微调阶段被去除的提示。</p>
</li>
</ol>
<p>鉴于以上护理，针对生成模型的安全使用，迫切需要更稳健且可扩展的方法。</p>
<p>本研究提出了一种新颖的训练无关的方法，利用Token的注意力重加权技术，在推理过程中去除不安全概念，而无需进行额外的训练。此方法通过对直接提示和对抗性“越狱”提示的定量与定性评估，与现存的消融方法进行比较，从而展示了其有效性。此外，研究还探讨了内容限制的潜在原因、局限性及其更广泛的影响。  </p>
<p><img src="/images/059c0f671976945399361fbcd44b4236d6d2790886e1411eaf64693e87da0619.jpg" alt="生成图像示例"></p>
<h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>本研究提出了一种基于注意力重加权的训练无关方法，旨在动态调整生成模型的注意力映射，以抑制不安全内容的生成，同时保持安全概念的生成表现。该方法包括两个主要部分：提示验证和局部编辑。</p>
<p>在提示验证阶段，研究者选择了Mistral-8x7B模型来判断输入提示是否安全。如果发现提示包含不安全内容，则该模型需要对其进行修改。具体的提示修改细节记录在附录中。</p>
<p>接下来进入注意力重加权阶段，一旦处理完修改过的提示，研究者将提升与安全概念相关的Token的重要性。例如，“一个携带机枪的孩子”被修改为“一个携带玩具枪的孩子”，此时将Token“玩具”的权重增加到10倍，以强调这个安全概念的重要性。该重加权通过对嵌入向量进行归一化并按定义的因子进行缩放来实现。</p>
<p>这一方法的设计旨在支持多概念的删除，因此能够同时处理多种类型的不安全内容，而无需显著的计算资源或任何形式的微调。</p>
<p> <img src="/images/bd9211b0152635b4b073ec42680f75bb3e9af03f6daf23bc1ebc28f3a0dc7b4c.jpg"><br>图：流程概述。研究者首先用安全Token替换不安全Token，然后对新的安全提示进行注意力图的修改，最后在去噪过程中使用最终修改并重加权的交叉注意力图。</p>
<h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><p>实验部分主要对比了不同的方法在生成直接和监狱式(jailbreak)提示下的表现。直接提示包括“带枪的孩子”和“裸体女性”，而监狱式提示则不直接提及枪支或裸体，但仍可能导致不安全内容的生成。为了解释扩散去噪过程中的随机性，对每个指标的评估均基于从目标域提示中采样的100幅图像，最后结果为100幅图像的平均值。</p>
<p>实验的基线方法最初是对Stable Diffusion v1-4模型进行微调，但由于该方法对生成图像的影响严重，导致图像高度失真。因此，实验基线采用了标准的Stable Diffusion模型，禁用了安全过滤器。</p>
<p>在评估指标和结果方面，本研究使用了多个量化标准，包括CLIP Score、Image Reward Score、FID Score以及由人类评估者进行的评价。这些指标用以评估不同消融方法的成功程度。具体如下：</p>
<ul>
<li><strong>Image Reward Score</strong>：模型对图像的评估，考虑了与提示的一致性、连贯性和美学吸引力。分数越高，表示提示与生成图像的一致性越好，因此较低的分数表明消融效果较好。</li>
<li><strong>CLIP Score</strong>：测量生成图像中目标概念的相关性。对不安全概念得分较低则表示成功消融。</li>
<li><strong>FID Score</strong>：评估生成的安全图像与原始图像之间的相似性。得分越低，表示图像质量的保留越好。</li>
<li><strong>人类评价</strong>：直接评估消融的成功程度，提供了自动化指标可能未能完整捕捉的信息。</li>
</ul>
<p>实验还包括了一次用户研究，邀请了50名具有不同背景的参与者。研究中展示了40幅图像，均分为四类：暴力、暴力（监狱式）、裸体和裸体（监狱式）。参与者需判断每幅图像是否包含被移除的概念，评估结果如表3所示。</p>
<p>以下是部分实验结果的示例图和表格：</p>
<p><img src="/images/9ae72bae28550ca9a6da0606d809b1404dedcfb05260a4c9a68c14aca1a12978.jpg">  </p>
<p>表2：一些“安全”提示，扩散模型在安全过滤器下生成黑色图像  </p>
<p><img src="/images/cb6fe9f6d580a5be6426f9a2cd48f950bbf8b2979a129fb8220f7490b73c5b7b.jpg">  </p>
<p>表3：图像生成方法的人类评价  </p>
<p>此外，还提供了一些使用不同技术生成的图像比较结果，这些图像都是在同一随机种子下生成的，旨在展示不同方法对结果的影响。以下是示例图：</p>
<p><img src="/images/d5d6bf22cd25db58e7ae53b950dbd3be68fe4b3864e9b9ab38071e5754637250.jpg"><br>图5：不同状态下的模型生成可视化消融结果。</p>
<h2 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h2><p>在这项研究中，作者探讨了在最先进的生成模型中生成不安全或有害内容的现象，并提出了一种创新的训练无关方法：通过注意力重标定技术来限制此类内容的生成。研究表明，现有的安全过滤器在应对不安全内容方面的效果较差，容易被针对性“越狱”提示所绕过。此外，作者指出，传统的消融方法难以完全去除目标概念，尤其是在面对表达相似含义但未在微调阶段被移除的提示时。</p>
<p>通过提出基于大语言模型（LLM）安全验证的注意力重标定方法，研究者成功地在不进行额外训练的情况下，动态调整生成过程中的交叉注意力图。这种方法可以增强对于安全概念的生成，同时有效压制不安全内容的生成。实验结果显示，该方法在处理直接和越狱提示时，较现有消融技术表现出更高的优越性，并且还显示出对于多种不安全内容的可扩展性。</p>
<p>尽管研究显示了该方法在安全内容生成方面的积极前景，但仍然存在一些局限性，例如对LLM在识别不安全提示时的依赖以及缺乏有效的基准来评估生成内容的偏见。未来的研究需要在更广泛的伤害内容及潜在偏见方面进行深入探索，以进一步完善生成模型的安全性。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/26/2410-04447/" data-id="cm2qbztb00006ewjldoc25igv" data-title="印度理工学院鲁尔基分校提出无训练的注意力重加权方法以减少生成模型中的不安全内容" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2024/10/26/2410-09804/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">北京航空航天大学提出一种多目标黑箱优化框架BlackDAN用于有效的上下文劫持大型语言模型</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/10/26/2410-04447/">印度理工学院鲁尔基分校提出无训练的注意力重加权方法以减少生成模型中的不安全内容</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-09804/">北京航空航天大学提出一种多目标黑箱优化框架BlackDAN用于有效的上下文劫持大型语言模型</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-09040/">加州大学圣克鲁斯分校提出了一种基于注意力操控的增强型大语言模型越狱攻击方法</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-10414/">新加坡国立大学提出的基于大型语言模型的内容审核守护模型的可靠性校准方法</a>
          </li>
        
          <li>
            <a href="/2024/10/26/2410-11459/">莫纳什大学提出多轮交互的Jigsaw Puzzles策略以破解大型语言模型的安全防护</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Jun<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>